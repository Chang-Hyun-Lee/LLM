{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967c53a9-cb65-49a5-99f4-145651f38bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/ubuntu/.local/lib/python3.10/site-packages (1.97.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx<1,>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.14.1)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Collecting langsmith>=0.1.17\n",
      "  Downloading langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.0/368.0 KB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66\n",
      "  Downloading langchain_core-0.3.70-py3-none-any.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.3/442.3 KB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (1.26.5)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.2/582.2 KB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: zstandard, tenacity, regex, python-dotenv, propcache, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.3 httpx-sse-0.4.1 jsonpatch-1.33 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.70 langchain-openai-0.3.28 langchain-text-splitters-0.3.8 langsmith-0.4.8 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 propcache-0.3.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 yarl-1.20.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install openai\n",
    "#!pip install langchain-core==0.2.17\n",
    "#!pip install langchain-openai==0.1.17\n",
    "#!pip install pydantic==1.10.8\n",
    "#!pip install pydantic_core==2.18.2\n",
    "!pip install -U openai langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020041bc-3149-4195-b28f-4d38c65cbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from google-search-results) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->google-search-results) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->google-search-results) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->google-search-results) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->google-search-results) (2025.7.14)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32016 sha256=e97e216767ffefff52630e0db626365136e10709352534c724ec5cc2f3a682d8\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/ubuntu/.local/lib/python3.10/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=22faca5f18bd6cef98615e86f02f60b7ee7992ee1bdc008f92183ec32ecbc4a0\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (4.14.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (11.3.0)\n",
      "Collecting torch>=1.11.0\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (0.33.4)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.7.0)\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 KB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.1\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.6.77\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence_transformers) (59.6.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.6)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.7.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, threadpoolctl, sympy, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence_transformers\n",
      "Successfully installed joblib-1.5.1 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.15.3 sentence_transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.2 triton-3.3.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results\n",
    "!pip install wikipedia\n",
    "!pip install faiss-cpu \n",
    "!pip install sentence_transformers \n",
    "!pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a332d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field # 예전 방식\n",
    "\n",
    "# from pydantic import BaseModel # 새로운 방식\n",
    "\n",
    "from pydantic.v1 import BaseModel # pydantic.v1 호환이 필요한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd2a51b-6740-4a14-8ad9-fb59e3ec001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/2744496630.py:15: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  aimsg = chat([sys, msg])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1990년대는 재즈 장르에서도 다양한 시도와 혁신이 있었던 시기입니다. 다음은 1990년대에 발표된 추천할 만한 재즈 곡 5곡입니다:\\n\\n1. **\"The Way Up\" - Pat Metheny Group (1996)**  \\n   이 곡은 약 1시간에 이르는 서사적인 구성의 재즈 작품으로, 패트 메신이 선보이는 독창적인 멜로디와 신디사이저 사운드가 조화를 이루고 있습니다.\\n\\n2. **\"Tusk\" - The Brian Blade Fellowship (1999)**  \\n   브라이언 블레이드의 드럼과 창의적인 편곡이 돋보이는 곡으로, 현대 재즈의 다양한 요소를 담고 있습니다.\\n\\n3. **\"Black Codes (From the Underground)\" - Wynton Marsalis (1990)**  \\n   훌륭한 트럼페터인 윈턴 마살리스의 작품으로, 전통적인 재즈 사운드와 현대적인 요소를 잘 결합한 곡입니다.\\n\\n4. **\"Soweto\" - Hugh Masekela (1992)**  \\n   남아프리카의 전설적인 재즈 트럼펫터 휴 마세켈라의 곡으로, 그의 독특한 스타일과 함께 아프리카 리듬을 잘 살린 재즈입니다.\\n\\n5. **\"Carmen\" - The Lost Album - Bill Evans (1996)**  \\n   빌 에반스의 1960년대 녹음이 1996년에 발매된 앨범으로, 그의 섬세한 피아노 연주가 인상적입니다.\\n\\n이 곡들을 통해 1990년대 재즈의 다양한 매력을 느껴보시기 바랍니다!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 음악 추천을 해주는 전문 AI입니다.\")\n",
    "msg = HumanMessage(content='1990년대 재즈 5곡만 추천해줘.')\n",
    "\n",
    "aimsg = chat([sys, msg])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e5f17c-6a25-4489-919f-b1e477368296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/1939471710.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'사파 대신 1박 2일로 가기 좋은 베트남의 여행지를 몇 곳 추천해드릴게요.\\n\\n1. **하롱베이**: 하롱베이는 UNESCO 세계유산으로 등록된 아름다운 섬과 바위들이 있는 곳입니다. 크루즈를 타고 섬을 탐험하거나 카약을 즐길 수 있습니다.\\n\\n2. **호이안**: 역사적인 매력을 간직한 호이안은 전통적인 건축물과 아름다운 야경, 맛있는 음식들로 유명합니다. 자전거를 빌려 주변을 둘러보거나, 해변에서 여유를 즐길 수 있습니다.\\n\\n3. **다낭**: 현대적인 도시와 아름다운 해변이 조화롭게 어우러진 다낭은 관광과 휴식을 동시에 즐길 수 있는 곳입니다. 미케 해변이나 바나힐즈를 방문해보세요.\\n\\n4. **하노이**: 베트남의 수도인 하노이는 문화와 역사적인 명소가 많습니다. 호안끼엠 호수, 구시가지, 그리고 다양한 음식들을 경험할 수 있습니다.\\n\\n5. **푸꾸옥 섬**: 아름다운 해변과 리조트가 있는 푸꾸옥은 편안한 휴양을 원하는 여행자에게 최적의 선택입니다. 스노클링이나 바다에서의 여유를 즐길 수 있습니다.\\n\\n각 여행지마다 다양한 매력이 있으니, 본인의 취향에 맞춰 선택하시면 좋겠습니다!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 여행을 추천해주는 전문 AI입니다.\")\n",
    "msg1 = HumanMessage(content='베트남 사파에 1박 2일 여행 가면 충분할까?')\n",
    "msg2 = AIMessage(content='사파는 1박 2일로 가기에는 너무 멉니다. 일정을 조정하거나 다른 여행지를 선택하시면 어떨까요?')\n",
    "msg3 = HumanMessage(content='음 ... 그러면 어디가 좋을까?')\n",
    "\n",
    "aimsg = chat([sys, msg1, msg2, msg3])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5f966f-2923-4af7-9e96-bab696d6c956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI 여행 추천 서비스 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate를 사용하여 각 LLM에 맞는 prompt를 지정할 수 있음\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"상품\"],\n",
    "    template=\"{상품} 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로\",\n",
    ")\n",
    "\n",
    "prompt.format(상품=\"AI 여행 추천 서비스\") # 변수가 여러 개 일 때, 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ae1232-2911-463a-838f-47eb2cbf5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/2518977364.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=prompt)\n",
      "/tmp/ipykernel_190134/2518977364.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(상품=\"학원 추천 서비스\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'여러 가지 기억에 남는 한글 이름을 제안해 드릴게요:\\n\\n1. **학원찾기**\\n2. **배움의길**\\n3. **지식나라**\\n4. **학습메이트**\\n5. **에듀링크**\\n6. **한걸음학원**\\n7. **학원연결**\\n8. **스쿨서치**\\n9. **도움학원**\\n10. **멘토링**\\n\\n이 중에서 마음에 드는 이름이 있기를 바랍니다! 추가적인 아이디어가 필요하시면 언제든지 말씀해 주세요.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "chain.run(상품=\"학원 추천 서비스\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cede08e-1a08-40de-b79a-edee95d7950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 프로그래밍을 사랑해.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 Prompt 별 template 지정도 가능\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)  \n",
    "\n",
    "template=\"You are a helpful assisstant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "chatchain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chatchain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e138db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (399 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.8/399.8 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numexpr) (2.2.6)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96ce4ed-c8cd-453a-941f-67f937037653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/2673388559.py:10: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm=chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m아마존의 창업자는 제프 베조스(Jeff Bezos)이다. 그의 나이를 알아내고, 배우자의 나이도 알아내야 한다. 제프 베조스의 나이를 확인하기 위해 위키피디아를 검색하겠다. \n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"Jeff Bezos\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Jeff Bezos\n",
      "Summary: Jeffrey Preston Bezos ( BAY-zohss; né Jorgensen; born January 12, 1964) is an American businessman best known as the founder, executive chairman, and former president and CEO of Amazon, the world's largest e-commerce and cloud computing company. According to Forbes, as of May 2025, Bezos's estimated net worth exceeded $220 billion, making him the third richest person in the world. He was the wealthiest person from 2017 to 2021, according to Forbes and the Bloomberg Billionaires Index.\n",
      "Bezos was born in Albuquerque and raised in Houston and Miami. He graduated from Princeton University in 1986 with degrees in electrical engineering and computer science. He worked on Wall Street in a variety of related fields from 1986 to early 1994. Bezos founded Amazon in mid-1994 on a road trip from New York City to Seattle. The company began as an online bookstore and has since expanded to a variety of other e-commerce products and services, including video and audio streaming, cloud computing, and artificial intelligence. It is the world's largest online sales company, the largest Internet company by revenue, and the largest provider of virtual assistants and cloud infrastructure services through its Amazon Web Services branch.\n",
      "Bezos founded the aerospace manufacturer and sub-orbital spaceflight services company Blue Origin in 2000. Blue Origin's New Shepard vehicle reached space in 2015 and afterwards successfully landed back on Earth; he flew into space on Blue Origin NS-16 in 2021. He purchased the major American newspaper The Washington Post in 2013 for $250 million and manages many other investments through his venture capital firm, Bezos Expeditions. In September 2021, Bezos co-founded Altos Labs with Mail.ru founder Yuri Milner.\n",
      "The first centibillionaire on the Forbes Real Time Billionaires Index and the second ever to have achieved the feat since Bill Gates in 1999, Bezos was named the \"richest man in modern history\" after his net worth increased to $150 billion in July 2018. In August 2020, according to Forbes, he had a net worth exceeding $200 billion. On July 5, 2021, Bezos stepped down as the CEO and president of Amazon and took over the role of executive chairman. Amazon Web Services CEO Andy Jassy succeeded Bezos as the CEO and president of Amazon.\n",
      "\n",
      "Page: Wedding of Jeff Bezos and Lauren Sánchez\n",
      "Summary: The wedding of American businessman Jeff Bezos and media personality Lauren Sánchez took place in Venice, Italy, between June 26–28, 2025. The multi-day event, held at various historic sites across the city, drew significant media attention due to its high-profile guest list, lavish celebrations, and extensive security measures. It also sparked local protests and criticism from Venetian activists over the city's commercialization and use as a venue for private luxury events.\n",
      "\n",
      "Page: Family of Jeff Bezos\n",
      "Summary: The family of Jeff Bezos, who is an American business magnate, is active in business and philanthropy. Bezos and his former wife MacKenzie Scott co-founded Amazon.\n",
      "Jeff's mother and stepfather are the co-founders of the Bezos Family Foundation.\n",
      "The family members have Danish-American, Mexican American and Cuban heritage.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found information about Jeff Bezos, including his birth date, which is January 12, 1964. To calculate his current age, I will subtract his birth year from the current year, 2023. Additionally, I need to find out the age of his spouse, Lauren Sánchez, to complete the multiplication.\n",
      "\n",
      "Next, I will search for Lauren Sánchez's birth date to determine her age.\n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"Lauren Sánchez\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Lauren Sánchez\n",
      "Summary: Lauren Wendy Sánchez Bezos (née Sánchez; born December 19, 1969) is an American philanthropist and former journalist. She is a licensed pilot and founder of Black Ops Aviation, an aerial film and production company. Sánchez has been a guest host on The View, co-host on KTTV Fox 11's Good Day L.A. and anchor on the Fox 11 News at Ten, and anchor and special correspondent on Extra. She has also been a regular contributor on shows including Larry King Live, The Joy Behar Show and Showbiz Tonight. \n",
      "In 2024, Sánchez released her debut children's book, The Fly Who Flew to Space, which became a New York Times bestseller. On April 14, 2025, Sánchez flew as a passenger in Blue Origin NS-31, an all-female spaceflight aboard the New Shepard rocket.\n",
      "In June 2025, Sánchez married Jeff Bezos, the third-richest person in the world.\n",
      "\n",
      "Page: Wedding of Jeff Bezos and Lauren Sánchez\n",
      "Summary: The wedding of American businessman Jeff Bezos and media personality Lauren Sánchez took place in Venice, Italy, between June 26–28, 2025. The multi-day event, held at various historic sites across the city, drew significant media attention due to its high-profile guest list, lavish celebrations, and extensive security measures. It also sparked local protests and criticism from Venetian activists over the city's commercialization and use as a venue for private luxury events.\n",
      "\n",
      "Page: Blue Origin NS-31\n",
      "Summary: Blue Origin NS-31 was a sub-orbital spaceflight operated by Blue Origin as part of New Shepard, the company's space tourism program. The flight took place on April 14, 2025, and lasted 10 minutes and 21 seconds. The flight carried all female passengers and was organized by journalist Lauren Sánchez, the then-fiancée of Blue Origin founder Jeff Bezos. She was joined by Aisha Bowe, Kerianne Flynn, Gayle King, Amanda Nguyen and Katy Perry.\n",
      "NS-31 received widespread media attention for its symbolic significance of being the first all-female spaceflight since Soviet cosmonaut Valentina Tereshkova became the first woman in space in 1963, making a solo flight on Vostok 6. Overall, the reception to the flight was predominantly negative, with criticism of its messaging and timing.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found Lauren Sánchez's birth date, which is December 19, 1969. Now I can calculate the ages of both Jeff Bezos and Lauren Sánchez.\n",
      "\n",
      "1. Jeff Bezos's age in 2023: 2023 - 1964 = 59 years old.\n",
      "2. Lauren Sánchez's age in 2023: 2023 - 1969 = 54 years old.\n",
      "\n",
      "Now, I will multiply their ages together.\n",
      "\n",
      "Action: Calculator  \n",
      "Action Input: 59 * 54  \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3186\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: 3186\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3186'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatOpenAI를 LLM으로 사용하는 agent\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0) \n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=chat)\n",
    "\n",
    "agent = initialize_agent(tools, llm=chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) \n",
    "\n",
    "agent.run(\"아마존의 창업자는 누구인가? 그의 나이와 배우자의 나이를 곱하면 얼마인가?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f84a01db-ce84-4ddf-be0d-9976ba3580ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/ubuntu/.local/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f30163b60b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f302d0c3ca0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f30163b60b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f302d0c3ca0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47498a86-a6b0-4e43-bbe3-e84d6e338b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Useful for when you need to answer questions about math.\n"
     ]
    }
   ],
   "source": [
    "print(agent.tools[0].description)\n",
    "print(agent.tools[1].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c1c5cf-26f1-448f-8687-784ca932ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/2149845611.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=chat, verbose=True)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미해. 이는 많은 양의 텍스트 데이터를 기반으로 학습하여 자연어 처리(NLP) 작업을 수행하는 인공지능 모델이야. LLM은 문장 생성, 번역, 요약, 질문 응답 등 다양한 언어 관련 작업을 잘 수행할 수 있도록 설계되었지. \\n\\n예를 들어, LLM은 책, 웹사이트, 기사 등에서 수집한 방대한 데이터로 학습하여 여러 주제에 대한 언어적 이해를 높이고, 사람과 비슷한 방식으로 대화를 나눌 수 있게 돼. 최근의 발전 덕분에 이러한 모델들은 문맥을 이해하고, 더 자연스럽고 유창한 응답을 생성하는 데 매우 능숙해졌어. \\n\\n혹시 LLM에 대해 더 궁금한 점이 있니?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "conversation = ConversationChain(llm=chat, verbose=True)\n",
    "conversation.predict(input=\"인공지능에서 LLM이 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bd1f46e-daa1-4098-8d3e-4c36f4ec0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI: LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미해. 이는 많은 양의 텍스트 데이터를 기반으로 학습하여 자연어 처리(NLP) 작업을 수행하는 인공지능 모델이야. LLM은 문장 생성, 번역, 요약, 질문 응답 등 다양한 언어 관련 작업을 잘 수행할 수 있도록 설계되었지. \n",
      "\n",
      "예를 들어, LLM은 책, 웹사이트, 기사 등에서 수집한 방대한 데이터로 학습하여 여러 주제에 대한 언어적 이해를 높이고, 사람과 비슷한 방식으로 대화를 나눌 수 있게 돼. 최근의 발전 덕분에 이러한 모델들은 문맥을 이해하고, 더 자연스럽고 유창한 응답을 생성하는 데 매우 능숙해졌어. \n",
      "\n",
      "혹시 LLM에 대해 더 궁금한 점이 있니?\n",
      "Human: PLM과의 차이를 설명해줘.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PLM은 \"Pre-trained Language Model\"의 약자로, 사전 훈련된 언어 모델을 의미해. LLM과 PLM은 밀접하게 관련이 있지만, 그 차이점은 주로 모델의 크기와 훈련 방식에 있어.\\n\\n1. **규모**: LLM은 \\'대규모\\' 언어 모델을 의미하며, 수억 또는 수십억 개의 매개변수를 가진 모델을 지칭해. PLM은 이러한 LLM의 일부일 수도 있지만, 반드시 대규모일 필요는 없어. PLM은 비교적 작은 모델에서도 사용될 수 있어.\\n\\n2. **훈련 과정**: PLM은 일반적으로 대량의 데이터로 사전 훈련되어, 다양한 언어적 패턴을 학습해. 그 후, 특정 작업에 맞춰 추가 훈련(tuning)을 거칠 수 있어. 반면에 LLM은 이미 대규모로 훈련이 완료된 모델이어서, 즉시 다양한 작업을 수행할 수 있는 능력을 갖추고 있어.\\n\\n3. **응용 범위**: LLM은 주로 복잡한 자연어 처리 작업에 최적화되어 있으며, 대화 생성, 고급 번역 등에서 뛰어난 성능을 보여줘. PLM은 기본적인 언어 이해나 특정한 작업에 대한 더 세밀한 조정이 가능해.\\n\\n결국 PLM은 LLM의 한 형태로 볼 수 있지만, 그 크기와 목적에 따라 다르게 사용할 수 있다는 점이 중요해. 더 궁금한 부분이 있다면 말씀해줘!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"PLM과의 차이를 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7e4767-87f5-4731-be17-7d74b9a309f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='인공지능에서 LLM이 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미해. 이는 많은 양의 텍스트 데이터를 기반으로 학습하여 자연어 처리(NLP) 작업을 수행하는 인공지능 모델이야. LLM은 문장 생성, 번역, 요약, 질문 응답 등 다양한 언어 관련 작업을 잘 수행할 수 있도록 설계되었지. \\n\\n예를 들어, LLM은 책, 웹사이트, 기사 등에서 수집한 방대한 데이터로 학습하여 여러 주제에 대한 언어적 이해를 높이고, 사람과 비슷한 방식으로 대화를 나눌 수 있게 돼. 최근의 발전 덕분에 이러한 모델들은 문맥을 이해하고, 더 자연스럽고 유창한 응답을 생성하는 데 매우 능숙해졌어. \\n\\n혹시 LLM에 대해 더 궁금한 점이 있니?', additional_kwargs={}, response_metadata={}), HumanMessage(content='PLM과의 차이를 설명해줘.', additional_kwargs={}, response_metadata={}), AIMessage(content='PLM은 \"Pre-trained Language Model\"의 약자로, 사전 훈련된 언어 모델을 의미해. LLM과 PLM은 밀접하게 관련이 있지만, 그 차이점은 주로 모델의 크기와 훈련 방식에 있어.\\n\\n1. **규모**: LLM은 \\'대규모\\' 언어 모델을 의미하며, 수억 또는 수십억 개의 매개변수를 가진 모델을 지칭해. PLM은 이러한 LLM의 일부일 수도 있지만, 반드시 대규모일 필요는 없어. PLM은 비교적 작은 모델에서도 사용될 수 있어.\\n\\n2. **훈련 과정**: PLM은 일반적으로 대량의 데이터로 사전 훈련되어, 다양한 언어적 패턴을 학습해. 그 후, 특정 작업에 맞춰 추가 훈련(tuning)을 거칠 수 있어. 반면에 LLM은 이미 대규모로 훈련이 완료된 모델이어서, 즉시 다양한 작업을 수행할 수 있는 능력을 갖추고 있어.\\n\\n3. **응용 범위**: LLM은 주로 복잡한 자연어 처리 작업에 최적화되어 있으며, 대화 생성, 고급 번역 등에서 뛰어난 성능을 보여줘. PLM은 기본적인 언어 이해나 특정한 작업에 대한 더 세밀한 조정이 가능해.\\n\\n결국 PLM은 LLM의 한 형태로 볼 수 있지만, 그 크기와 목적에 따라 다르게 사용할 수 있다는 점이 중요해. 더 궁금한 부분이 있다면 말씀해줘!', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67d6347c-2c99-4d28-b9e6-b7585d72a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Created a chunk of size 2293, which is longer than the specified 1000\n",
      "Created a chunk of size 2843, which is longer than the specified 1000\n",
      "Created a chunk of size 6664, which is longer than the specified 1000\n",
      "Created a chunk of size 2194, which is longer than the specified 1000\n",
      "Created a chunk of size 1471, which is longer than the specified 1000\n",
      "Created a chunk of size 2774, which is longer than the specified 1000\n",
      "Created a chunk of size 1873, which is longer than the specified 1000\n",
      "Created a chunk of size 3432, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 2746, which is longer than the specified 1000\n",
      "Created a chunk of size 3230, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "# https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_path=\"https://ko.wikipedia.org/wiki/IU\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "692cd1a0-48c1-48b6-977c-a2a48ea96fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='아이유 - 위키백과, 우리 모두의 백과사전\\n\\n\\n본문으로 이동\\n\\n주 메뉴\\n\\n주 메뉴\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t둘러보기\\n\\t\\n\\n\\n대문최근 바뀜요즘 화제임의의 문서로\\n\\n\\t\\t사용자 모임\\n\\t\\n\\n\\n사랑방사용자 모임관리 요청\\n\\n\\t\\t편집 안내\\n\\t\\n\\n\\n소개도움말정책과 지침질문방\\n\\n검색\\n\\n검색\\n\\n\\n보이기\\n\\n\\n기부\\n\\n계정 만들기\\n\\n로그인\\n\\n\\n개인 도구\\n\\n기부 계정 만들기 로그인\\n\\n\\n목차\\n사이드바로 이동\\n숨기기\\n\\n\\n처음 위치\\n\\n1\\n학력\\n\\n\\n2\\n활동\\n\\n\\n활동 하위섹션 토글하기\\n\\n2.1\\n2008-2010\\n\\n\\n2.2\\n2011-2013\\n\\n\\n2.3\\n2014-2016\\n\\n\\n2.4\\n2017-2018\\n\\n\\n2.5\\n2019-2020\\n\\n\\n2.6\\n2021-2022\\n\\n\\n2.7\\n2023-2024\\n\\n\\n2.8\\n2025\\n\\n\\n3\\n음반 목록\\n\\n\\n4\\n음반 외 활동 목록\\n\\n\\n5\\n수상 및 후보 목록\\n\\n\\n6\\n사건\\n\\n\\n7\\n같이 보기\\n\\n\\n8\\n각주\\n\\n\\n각주 하위섹션 토글하기\\n\\n8.1\\n내용주\\n\\n\\n8.2\\n참조주\\n\\n\\n9\\n외부 링크\\n\\n\\n목차 토글\\n\\n아이유\\n\\n63개 언어'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='العربيةمصرىتۆرکجهBasa BaliBikol CentralБългарскиCatalàCebuanoکوردیČeštinaDanskDeutschKadazandusunΕλληνικάEnglishEsperantoEspañolEestiEuskaraفارسیSuomiFrançaisहिन्दीKreyòl ayisyenMagyarՀայերենInterlinguaBahasa IndonesiaItaliano日本語JawaҚазақшаکٲشُرLatinaMāoriമലയാളംМонголBahasa Melayuမြန်မာဘာသာNederlandsNorsk bokmålPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSoomaaligaСрпски / srpskiSesothoSundaSvenskaதமிழ்ไทยTagalogSetswanaTürkçeУкраїнськаOʻzbekcha / ўзбекчаTiếng Việt中文閩南語 / Bân-lâm-gí粵語\\n\\n링크 편집\\n\\n문서토론\\n\\n한국어\\n\\n읽기원본 보기역사 보기\\n\\n도구\\n\\n도구\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t동작\\n\\t\\n\\n\\n읽기원본 보기역사 보기\\n\\n\\t\\t일반\\n\\t\\n\\n\\n여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR코드 다운로드\\n\\n\\t\\t인쇄/내보내기\\n\\t\\n\\n\\n책 만들기PDF로 다운로드인쇄용 판\\n\\n\\t\\t다른 프로젝트\\n\\t\\n\\n\\n위키미디어 공용위키데이터 항목\\n\\n보이기\\n사이드바로 이동\\n숨기기\\n\\n\\n위키백과, 우리 모두의 백과사전.'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='(IU에서 넘어옴)\\n  다른 뜻에 대해서는 아이유 (동음이의) 문서를 참고하십시오. IU는 여기로 연결됩니다. 다른 뜻에 대해서는 IU (동음이의) 문서를 참고하십시오.\\n아이유2022년 11월 25일 청룡영화상에서의 아이유기본 정보본명이지은출생1993년 5월 16일(1993-05-16)(32세)대한민국 서울특별시 성동구 송정동성별여성직업가수, 배우장르댄스, 발라드, 어쿠스틱, R&B활동 시기2008년~학력동덕여자고등학교 (졸업)레이블카카오엔터테인먼트, EMI 뮤직 재팬소속사EDAM 엔터테인먼트서명웹사이트edam-ent.com유튜브채널이지금 [IU Official]분야일상, 음악개설일2017년 2월 13일구독자 수978만\\xa0명(2024년 10월 6일 기준)총 조회수3,150,968,674회(2024년 10월 6일 기준)vte\\n\\n아이유(IU, 본명:\\xa0이지은, 李知恩[1], 1993년 5월 16일~)는 대한민국의 싱어송라이터, 작곡가, 배우이다. 2007년 로엔 엔터테인먼트(현 카카오 엔터테인먼트) 연습생으로 전속 계약을 맺고 15세의 나이에 2008년 첫 EP인 로스트 앤 파운드(Lost and Found)를 통해 가수로 데뷔했다.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36be6f88-571b-4010-bbdb-af25b75a43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190134/4030962451.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#embeddings = HuggingFaceEmbeddings() # sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    ).from_loaders([loader])\n",
    "\n",
    "# 파일로 저장\n",
    "index.vectorstore.save_local(\"faiss-nj-IU\")\n",
    "# db = FAISS.load_local(\"faiss-nj-IU\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c656357-f0e7-439b-b6c6-14927f4e7c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유의 데뷔곡은 2008년에 발매된 \"미아\"입니다.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 데뷔곡은?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aebc9b7c-64b9-4cd5-be7d-f2e525e6a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유는 1993년 5월 16일에 태어났으므로, 2024년 기준으로 32세입니다.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1917167b-5b64-4e99-9b8e-17a6047d5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유는 1993년 5월 16일에 태어났으므로, 2025년 7월 22일 기준으로 32세입니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는? (오늘은 2025년 7월 22일)\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403356b0-5ef9-45a1-b0d7-743866575946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그에 대한 정보는 제시된 문맥에 포함되어 있지 않습니다. 따라서 아이유가 지금까지 총 몇 회의 콘서트를 했는지에 대해서는 알지 못합니다.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유는 지금까지 총 콘서트를 몇회했는가?\", llm=chat, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
