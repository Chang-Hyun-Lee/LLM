# app.py
import streamlit as st
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.query_engine import RetrieverQueryEngine

st.set_page_config(page_title="PDF Q&A", layout="wide")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì¸ë±ìŠ¤ ë¡œë“œ
@st.cache_resource
def load_index():
    storage_context = StorageContext.from_defaults(persist_dir="index")
    return load_index_from_storage(storage_context)

index = load_index()
query_engine = RetrieverQueryEngine.from_args(index.as_retriever())

# UI
st.title("ğŸ“„ PDF ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì±—ë´‡")

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    st.session_state.chat_history.append(("user", user_input))

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):

        #ë©€í‹°í„´(LLMì— ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í•¨ê»˜ ì „ë‹¬ (ë¬¸ë§¥ ìœ ì§€))
        full_context = "\n".join([f"{role}: {msg}" for role, msg in st.session_state.chat_history if role == "user" or role == "bot"])
        full_context += f"\nuser: {user_input}"

        response = query_engine.query(full_context)
        st.session_state.chat_history.append(("bot", str(response)))

# ëŒ€í™” ë‚´ì—­ í‘œì‹œ
for role, msg in st.session_state.chat_history:
    with st.chat_message(role):
        st.markdown(msg)
