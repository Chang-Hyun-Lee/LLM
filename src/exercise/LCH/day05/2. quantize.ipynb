{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.41.1)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: huggingface_hub[cli] in /home/ubuntu/.local/lib/python3.10/site-packages (0.23.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.local/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers accelerate huggingface_hub[cli] bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # CUDA 장치로 설정\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU 장치로 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"42dot/42dot_LLM-SFT-1.3B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 25 12:33:53 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 531.41       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        On  | 00000000:05:00.0  On |                  N/A |\n",
      "|  0%   39C    P2              37W / 170W |   9739MiB / 12288MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        24      G   d                                         N/A      |\n",
      "|    0   N/A  N/A     48553      C   .10                                       N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<||bos||>근육이 커지기 위해서는 충분한 단백질 섭취가 필요합니다.\n",
      "단백질을 충분히 먹으면 근육을 키우는 데 도움이 됩니다<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "gen_ids = model.generate(input_ids,\n",
    "                           max_length=256,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: 3박 4일 하노이 여행을 가려는데 일정을 짜줘.\n",
      "\n",
      "<bot>: 하노이 여행 일정\n",
      "\n",
      "일정:\n",
      "\n",
      "Day 1: 하노이 도착 후 호안끼엠(Ho Chi Minh) 시장 방문\n",
      "\n",
      "Day 2: 호치민 시립 역사 박물관(Bien Hoa Museum) 방문\n",
      "\n",
      "Day 3: 호치민 시립 역사 박물관(Bien Hoa Museum) 방문\n",
      "\n",
      "Day 4: 후에(Hue) 또는 롱비엔(Long Bien) 방문\n",
      "\n",
      "Day 5: 후에(Hue) 또는 롱비엔(Long Bien) 방문\n",
      "\n",
      "Day 6: 하노이 출발\n",
      "\n",
      "이 여행 일정은 하노이에서 호안끼엠(Ho Chi Minh) 시장, 호치민 시립 역사 박물관, 후에(Hue) 또는 롱비엔(Long Bien)를 방문하는 일정을 가진다고 되어 있습니다.\n",
      "\n",
      "하노이에서 호안끼엠(Ho Chi Minh) 시장까지는 지하철, 버스 또는 택시를 이용하면 됩니다. 이 날은 시장 구경하고 맛있는 음식도 맛볼 수 있습니다.\n",
      "\n",
      "호치민 시립 역사 박물관에는 베트남 역사에 관련된 많은 유물과 작품이 전시되어 있습니다. 하노이에 있는 역사에 관심이 있다면 꼭 들러보세요.\n",
      "\n",
      "후에(Hue) 또는 롱비엔(Long Bien)를 방문하면, 그 곳의 역사와 문화를 느낄 수 있습니다.\n",
      "\n",
      "하노이에서 출발하면 일정은 마무리됩니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    "\n",
    ")\n",
    "def query(instruction, input=None):\n",
    "    if input:\n",
    "        prompt = f\"<human>: {instruction}\\n\\n<input>: {input}\\n\\n<bot>:\"\n",
    "    else:\n",
    "        prompt = f\"<human>: {instruction}\\n\\n<bot>:\"    \n",
    "    \n",
    "    print(generator(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        early_stopping=True,\n",
    "        max_length=512,\n",
    "    )[0]['generated_text'])\n",
    "\n",
    "\n",
    "query(\"3박 4일 하노이 여행을 가려는데 일정을 짜줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "def gen(device, instruction, input=''):\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature = 0.1,\n",
    "        max_new_tokens = 512,\n",
    "        exponential_decay_length_penalty = (512, 1.03),\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        repetition_penalty = 1.05,\n",
    "        do_sample = True,\n",
    "        top_p = 0.7,\n",
    "        min_length = 5,\n",
    "        use_cache = True,\n",
    "        return_dict_in_generate = True,\n",
    "    )\n",
    "    if input:\n",
    "        prompt = f\"<human>: {instruction}\\n\\n<input>: {input}\\n\\n<bot>:\"\n",
    "    else:\n",
    "        prompt = f\"<human>: {instruction}\\n\\n<bot>:\"    \n",
    "        \n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to(device),\n",
    "        generation_config=generation_config,\n",
    "        streamer=streamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<||bos||><human>: 세종대왕에 대해서 알려줘.\n",
      "\n",
      "<bot>: 세종대왕은 조선시대의 왕으로, 1418년에 태어나 1450년에 사망한 인물입니다. 그는 한글 창제와 과학 기술 발전에 큰 업적을 남겼습니다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "gen(device, \"세종대왕에 대해서 알려줘.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
