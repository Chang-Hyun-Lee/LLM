import os
import streamlit as st
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
SOURCE_FOLDER = "../data"

# ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡
def list_files_in_directory(directory):
    try:
        files = os.listdir(directory)
        return [f for f in files if os.path.isfile(os.path.join(directory, f))]
    except Exception as e:
        st.error(f"Error: {e}")
        return []

# íŒŒì¼ ì €ì¥
def save_uploaded_file(directory, file):
    if not os.path.exists(directory):
        os.makedirs(directory)
    with open(os.path.join(directory, file.name), 'wb') as f:
        f.write(file.getbuffer())

# llama_index ì¸ë±ìŠ¤ ìƒì„±
@st.cache_resource
def get_index():
    Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")
    try:
        documents = SimpleDirectoryReader(SOURCE_FOLDER).load_data()
        return VectorStoreIndex.from_documents(documents)
    except Exception as e:
        st.error(f"ë¬¸ì„œ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
@st.cache_resource
def get_memory():
    return ChatMemoryBuffer.from_defaults()

# ì±—ë´‡ ì‘ë‹µ
def chat_with_bot(user_input, index):
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        memory=get_memory(),
        system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
    )
    return chat_engine.stream_chat(user_input)

# íŒŒì¼ ì‚­ì œ
def delete_file(filename):
    file_path = os.path.join(SOURCE_FOLDER, filename)
    if os.path.exists(file_path):
        os.remove(file_path)
        get_index.clear()
        st.rerun()

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ“š RAG Chatbot with llama_index + OpenAI")

    if "file_uploader_key" not in st.session_state:
        st.session_state["file_uploader_key"] = 0

    with st.sidebar:
        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ë° ì‚­ì œ")
        upload_file = st.file_uploader("PDF ì—…ë¡œë“œ", type=['pdf'], key=st.session_state["file_uploader_key"])
        if upload_file:
            save_uploaded_file(SOURCE_FOLDER, upload_file)
            get_index.clear()
            st.session_state["file_uploader_key"] += 1
            st.rerun()

        files = list_files_in_directory(SOURCE_FOLDER)
        if files:
            selected_file = st.selectbox("ì‚­ì œí•  íŒŒì¼ ì„ íƒ", files)
            if st.button("ì„ íƒí•œ íŒŒì¼ ì‚­ì œ"):
                delete_file(selected_file)

    index = get_index()
    if index is None:
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    if user_input:
        st.chat_message("user").markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        response = chat_with_bot(user_input, index)

        with st.chat_message("assistant"):
            msg_box = st.empty()
            full_response = ""
            for chunk in response.response_gen:
                full_response += chunk
                msg_box.markdown(full_response + "â–Œ")
            msg_box.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
