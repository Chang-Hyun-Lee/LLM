import os
import streamlit as st
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter

# í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

# llama_index ì„¤ì •
Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")
Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)

st.title("ğŸ’¬ ë©€í‹°í„´ ë¬¸ì„œ ê¸°ë°˜ ì±—ë´‡")

# ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def get_chat_engine():
    documents = SimpleDirectoryReader("data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    return index.as_chat_engine(chat_mode="condense_question", verbose=True)

chat_engine = get_chat_engine()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

# ë©”ì‹œì§€ ë Œë”ë§
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì§ˆë¬¸ ì²˜ë¦¬
if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.chat_history.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ì±—ì—”ì§„ìœ¼ë¡œë¶€í„° ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ìƒì„± ì¤‘..."):
            response = chat_engine.chat(user_input)
            st.markdown(response.response)
            st.session_state.chat_history.append({"role": "assistant", "content": response.response})
