{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cce95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.51-py3-none-any.whl (7.1 kB)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2\n",
      "  Downloading llama_index_cli-0.4.4-py3-none-any.whl (28 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.10-py3-none-any.whl (16 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.51\n",
      "  Downloading llama_index_core-0.12.52-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-agent-openai<0.5,>=0.4.0\n",
      "  Downloading llama_index_agent_openai-0.4.12-py3-none-any.whl (14 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0\n",
      "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0\n",
      "  Downloading llama_index_readers_file-0.4.11-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.3-py3-none-any.whl (3.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Collecting nltk>3.8.1\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-program-openai<0.4,>=0.3.0\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.97.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (4.14.1)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (0.28.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (2.32.4)\n",
      "Collecting aiosqlite\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (0.9.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (9.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (2025.7.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (4.67.1)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (1.6.0)\n",
      "Collecting pyyaml>=6.0.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (1.17.2)\n",
      "Collecting filetype<2,>=1.2.0\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting banks<3,>=2.2.0\n",
      "  Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: dataclasses-json in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (0.6.7)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (0.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (1.2.18)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (3.12.14)\n",
      "Requirement already satisfied: platformdirs in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (2.11.7)\n",
      "Collecting setuptools>=80.9.0\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Collecting llama-index-workflows<2,>=1.0.1\n",
      "  Downloading llama_index_workflows-1.1.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (11.3.0)\n",
      "Collecting dirtyjson<2,>=1.0.8\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: numpy in /home/eons/.local/lib/python3.10/site-packages (from llama-index-core<0.13,>=0.12.51->llama-index) (2.2.6)\n",
      "Collecting llama-cloud==0.1.32\n",
      "  Downloading llama_cloud-0.1.32-py3-none-any.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 KB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2024.7.4 in /home/eons/.local/lib/python3.10/site-packages (from llama-cloud==0.1.32->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.7.14)\n",
      "Collecting defusedxml>=0.7.1\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pandas<2.3.0\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pypdf<6,>=5.1.0 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.8.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /home/eons/.local/lib/python3.10/site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.13.4)\n",
      "Collecting striprtf<0.0.27,>=0.0.26\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Collecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.51-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: joblib in /home/eons/.local/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/eons/.local/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>3.8.1->llama-index) (8.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (1.20.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (0.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (6.6.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/eons/.local/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.51->llama-index) (4.0.3)\n",
      "Collecting griffe\n",
      "  Downloading griffe-1.8.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 KB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.13,>=0.12.51->llama-index) (3.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/eons/.local/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13,>=0.12.51->llama-index) (1.0.9)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13,>=0.12.51->llama-index) (4.9.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx->llama-index-core<0.13,>=0.12.51->llama-index) (3.3)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.51->llama-index) (0.16.0)\n",
      "Collecting llama-index-instrumentation>=0.1.0\n",
      "  Downloading llama_index_instrumentation-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting llama-cloud-services>=0.6.51\n",
      "  Downloading llama_cloud_services-0.6.51-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eons/.local/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eons/.local/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eons/.local/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.51->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.51->llama-index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.51->llama-index) (0.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.51->llama-index) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/eons/.local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.51->llama-index) (3.4.2)\n",
      "Requirement already satisfied: greenlet>=1 in /home/eons/.local/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.51->llama-index) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/eons/.local/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.51->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/eons/.local/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.51->llama-index) (3.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.51->llama-index) (1.3.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /home/eons/.local/lib/python3.10/site-packages (from llama-cloud-services>=0.6.51->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Collecting click\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 KB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of jiter to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.5/352.5 KB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of idna to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting idna\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of greenlet to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "INFO: pip is looking at multiple versions of frozenlist to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "INFO: pip is looking at multiple versions of distro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "INFO: pip is looking at multiple versions of charset-normalizer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "INFO: pip is looking at multiple versions of certifi to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting certifi>=2024.7.4\n",
      "  Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of async-timeout to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "INFO: pip is looking at multiple versions of anyio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of annotated-types to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "INFO: pip is looking at multiple versions of aiosignal to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "INFO: pip is looking at multiple versions of aiohappyeyeballs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of platformdirs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting platformdirs\n",
      "  Using cached platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "INFO: pip is looking at multiple versions of joblib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "INFO: pip is looking at multiple versions of dataclasses-json to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dataclasses-json\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of click to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of aiosqlite to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosqlite\n",
      "  Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of wrapt to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "INFO: pip is looking at multiple versions of typing-inspect to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting typing-inspect>=0.8.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "INFO: pip is looking at multiple versions of typing-extensions to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tqdm<5,>=4.66.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tiktoken to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tiktoken>=0.7.0\n",
      "  Using cached tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "INFO: pip is looking at multiple versions of tenacity to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "INFO: pip is looking at multiple versions of striprtf to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sqlalchemy[asyncio] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sqlalchemy[asyncio]>=1.4.49\n",
      "  Using cached sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests>=2.31.0\n",
      "  Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "INFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=6.0.1\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 KB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pypdf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pypdf<6,>=5.1.0\n",
      "  Using cached pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic>=2.8.0\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 KB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow>=9.0.0\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas<2.3.0\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openai>=1.14.0\n",
      "  Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 KB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx>=3.0\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "INFO: pip is looking at multiple versions of nest-asyncio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting nest-asyncio<2,>=1.5.8\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "INFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.50-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.49\n",
      "  Downloading llama_cloud_services-0.6.50-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading llama_cloud_services-0.6.49-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.49-py3-none-any.whl (4.9 kB)\n",
      "  Downloading llama_parse-0.6.48-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.48\n",
      "  Downloading llama_cloud_services-0.6.48-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.47-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.47\n",
      "  Downloading llama_cloud_services-0.6.47-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.46-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.45\n",
      "  Downloading llama_cloud_services-0.6.46-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.9/47.9 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading llama_cloud_services-0.6.45-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.45-py3-none-any.whl (4.9 kB)\n",
      "  Downloading llama_parse-0.6.44-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.44\n",
      "  Downloading llama_cloud_services-0.6.44-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0\n",
      "  Downloading llama_parse-0.6.43-py3-none-any.whl (4.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.43\n",
      "  Downloading llama_cloud_services-0.6.43-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/eons/.local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.51->llama-index) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/lib/python3/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.13,>=0.12.51->llama-index) (0.4.4)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, setuptools, pyyaml, griffe, defusedxml, click, aiosqlite, pandas, nltk, llama-index-instrumentation, banks, llama-index-workflows, llama-cloud, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.1\n",
      "    Uninstalling pandas-2.3.1:\n",
      "      Successfully uninstalled pandas-2.3.1\n",
      "Successfully installed aiosqlite-0.21.0 banks-2.2.0 click-8.2.1 defusedxml-0.7.1 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.8.0 llama-cloud-0.1.32 llama-cloud-services-0.6.43 llama-index-0.12.51 llama-index-agent-openai-0.4.12 llama-index-cli-0.4.4 llama-index-core-0.12.52 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.10 llama-index-instrumentation-0.3.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.3 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.11 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.1.0 llama-parse-0.6.43 nltk-3.9.1 pandas-2.2.3 pyyaml-6.0.2 setuptools-80.9.0 striprtf-0.0.26\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deee347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9193b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30614b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개울가\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"소년과 소녀는 어디에서 처음 만났나? 한국어로 대답해줘.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e89b4f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근육이 커지려면 꾸준한 운동과 올바른 식이요법이 필요합니다. 근육을 키우기 위해서는 저항 운동을 통해 근육을 자극하고 성장시키는 것이 중요합니다. 또한 단백질 섭취를 증가시키고 충분한 수면을 취하는 것도 근육을 키우는데 도움이 됩니다. 또한 적절한 휴식과 회복을 위한 시간을 확보하는 것도 중요합니다. 이러한 요소들을 조합하여 꾸준히 운동하고 올바른 식이요법을 유지한다면 근육을 키우는데 도움이 될 것입니다.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "response = OpenAI().complete(\"근육이 커지려면\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8877ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0.2, model=\"gpt-4o-mini\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a1f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소녀는 윤 초시의 증손녀입니다.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"소녀와 윤초시는 어떤 관계인가? 한국어로 대답해줘.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2101c614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소년은 소녀에게 덕쇠 할아버지네 호두를 맛보여 주려고 했습니다.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"소년은 소녀에게 무엇을 주려고 했나? 한국어로 대답해줘.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6286af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) # logging.INFO\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f16035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG) # logging.INFO\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.core.storage.kvstore.simple_kvstore:Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/docstore.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/docstore.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/docstore.json\n",
      "DEBUG:llama_index.core.storage.kvstore.simple_kvstore:Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/index_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/index_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/index_store.json\n",
      "DEBUG:llama_index.core.graph_stores.simple:Loading llama_index.core.graph_stores.simple from ./storage/graph_store.json.\n",
      "Loading llama_index.core.graph_stores.simple from ./storage/graph_store.json.\n",
      "Loading llama_index.core.graph_stores.simple from ./storage/graph_store.json.\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/graph_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/graph_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/graph_store.json\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/property_graph_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/property_graph_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/property_graph_store.json\n",
      "DEBUG:llama_index.core.vector_stores.simple:Loading llama_index.core.vector_stores.simple from ./storage/default__vector_store.json.\n",
      "Loading llama_index.core.vector_stores.simple from ./storage/default__vector_store.json.\n",
      "Loading llama_index.core.vector_stores.simple from ./storage/default__vector_store.json.\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/default__vector_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/default__vector_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/default__vector_store.json\n",
      "DEBUG:llama_index.core.vector_stores.simple:Loading llama_index.core.vector_stores.simple from ./storage/image__vector_store.json.\n",
      "Loading llama_index.core.vector_stores.simple from ./storage/image__vector_store.json.\n",
      "Loading llama_index.core.vector_stores.simple from ./storage/image__vector_store.json.\n",
      "DEBUG:fsspec.local:open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/image__vector_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/image__vector_store.json\n",
      "open file: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/storage/image__vector_store.json\n",
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-0061910e-1b04-43aa-b86f-db28bfd28465', 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f9987e59e10>, 'json_data': {'input': ['앙기모딱.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-0061910e-1b04-43aa-b86f-db28bfd28465', 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f9987e59e10>, 'json_data': {'input': ['앙기모딱.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-0061910e-1b04-43aa-b86f-db28bfd28465', 'post_parser': <function Embeddings.create.<locals>.parser at 0x7f9987e59e10>, 'json_data': {'input': ['앙기모딱.'], 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802b310>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802b310>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802b310>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f998874e140> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f998874e140> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f998874e140> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a2c0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a2c0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a2c0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-ada-002-v2'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'43'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-8579944f5-g8tnz'), (b'x-envoy-upstream-service-time', b'48'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999997'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_441e84a2937b71dbf1979b27118fac14'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'96397149dc7b8b66-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-ada-002-v2'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'43'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-8579944f5-g8tnz'), (b'x-envoy-upstream-service-time', b'48'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999997'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_441e84a2937b71dbf1979b27118fac14'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'96397149dc7b8b66-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-ada-002-v2'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'43'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'via', b'envoy-router-8579944f5-g8tnz'), (b'x-envoy-upstream-service-time', b'48'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999997'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_441e84a2937b71dbf1979b27118fac14'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'96397149dc7b8b66-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-ada-002-v2', 'openai-organization': 'weable-inc', 'openai-processing-ms': '43', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-8579944f5-g8tnz', 'x-envoy-upstream-service-time': '48', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '4999997', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_441e84a2937b71dbf1979b27118fac14', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '96397149dc7b8b66-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-ada-002-v2', 'openai-organization': 'weable-inc', 'openai-processing-ms': '43', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-8579944f5-g8tnz', 'x-envoy-upstream-service-time': '48', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '4999997', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_441e84a2937b71dbf1979b27118fac14', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '96397149dc7b8b66-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:12 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-ada-002-v2', 'openai-organization': 'weable-inc', 'openai-processing-ms': '43', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'via': 'envoy-router-8579944f5-g8tnz', 'x-envoy-upstream-service-time': '48', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '4999997', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_441e84a2937b71dbf1979b27118fac14', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '96397149dc7b8b66-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: req_441e84a2937b71dbf1979b27118fac14\n",
      "request_id: req_441e84a2937b71dbf1979b27118fac14\n",
      "request_id: req_441e84a2937b71dbf1979b27118fac14\n",
      "DEBUG:llama_index.core.indices.utils:> Top 2 nodes:\n",
      "> [Node 926ea718-b0b7-4931-8c47-c5dddbf1441e] [Similarity score:             0.809784] - 6 -\n",
      "소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알...\n",
      "> [Node fe31bbac-4ba5-4609-8735-287b60ef35a0] [Similarity score:             0.805265] - 3 -\n",
      "돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다...\n",
      "> Top 2 nodes:\n",
      "> [Node 926ea718-b0b7-4931-8c47-c5dddbf1441e] [Similarity score:             0.809784] - 6 -\n",
      "소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알...\n",
      "> [Node fe31bbac-4ba5-4609-8735-287b60ef35a0] [Similarity score:             0.805265] - 3 -\n",
      "돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다...\n",
      "> Top 2 nodes:\n",
      "> [Node 926ea718-b0b7-4931-8c47-c5dddbf1441e] [Similarity score:             0.809784] - 6 -\n",
      "소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알...\n",
      "> [Node fe31bbac-4ba5-4609-8735-287b60ef35a0] [Similarity score:             0.805265] - 3 -\n",
      "돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다...\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3b5164a5-acb3-404a-9107-dcdd04cf6cb6', 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\npage_label: 6\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 6 -\\n소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알고 있었다. 그것이 이번에는 고향 집마저 남의 손에 넘기게 된 모양이었다.“왜 그런지 난 이사 가는 게 싫어졌다. 어른들이 하는 일이니 어쩔 수 없지만…….”전에 없이, 소녀의 까만 눈에 쓸쓸한 빛이 떠돌았다.소녀와 헤어져 돌아오는 길에, 소년은 혼잣속으로, 소녀가 이사를 간다는 말을 수없이 되뇌어 보았다. 무어 그리 안타까울 것도 서러울 것도 없었다. 그렇건만, 소년은 지금 자기가 씹고 있는 대추알의 단맛을 모르고 있었다.이 날 밤, 소년은 몰래 덕쇠 할아버지네 호두밭으로 갔다.낯에 봐 두었던 나무로 올라갔다. 그리고, 봐 두었던 가지를 향해 작대기를 내리쳤다. 호두송이 떨어지는 소리가 별나게 크게 들렸다. 가슴이 선뜩했다. 그러나 다음 순간, 굵은 호두야 많이 떨어져라, 많이 떨어져라, 저도 모를 힘에 이끌려 마구 작대기를 내리 치는 것이었다.돌아오는 길에는 열 이틀 달이 지우는 그늘만 골라 디뎠다. 그늘의 고마움을 처음 느꼈다.불룩한 주머니를 어루만졌다. 호두송이를 맨손으로 깠다가는 옴이 오르기 쉽다는 말 같은 건 아무렇지도 않았다. 그저 근동에서 제일 가는 이 덕쇠 할아버지네 호두를 어서 소녀에게 맛보여야 한다는 생각만이 앞섰다.그러다, 아차 하는 생각이 들었다. 소녀더러 병이 좀 낫거들랑 이사 가기 전에 한 번 개울가로 나와 달라는 말을 못해 둔 것이었다. 바보 같은것, 바보 같은것.이튿날, 소년이 학교에서 돌아오니, 아버지가 나들이옷으로 갈아입고 닭 한 마리를 안고 있었다.어디 가시느냐고 물었다.그 말에도 대꾸도 없이, 아버지는 안고 있는 닭의 무게를 겨냥해 보면서,“이만하면 될까?”어머니가 망태기를 내주며,“벌써 며칠째 ‘걀걀’하고 알 날 자리를 보던데요.\\n\\npage_label: 3\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 3 -\\n돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다.거기서부터 산 밑까지는 밭이었다.수숫단을 세워 놓은 밭머리를 지났다.“저게 뭐니?”“원두막.”“여기 참외, 맛있니?”“그럼, 참외 맛도 좋지만 수박 맛은 더 좋다.”“하나 먹어 봤으면.”소년이 참외 그루에 심은 무우밭으로 들어가, 무우 두 밑을 뽑아 왔다. 아직 밑이 덜 들어 있었다. 잎을 비틀어 팽개친 후, 소녀에게 한개 건넨다. 그리고는 이렇게 먹어야 한다는 듯이, 먼저 대강이를 한 입 베물어 낸 다음, 손톱으로 한 돌이 껍질을 벗겨 우쩍 깨문다.소녀도 따라 했다. 그러나, 세 입도 못 먹고,“아, 맵고 지려.”하며 집어던지고 만다.“참, 맛 없어 못 먹겠다.”소년이 더 멀리 팽개쳐 버렸다.산이 가까워졌다.단풍이 눈에 따가웠다.“야아!”소녀가 산을 향해 달려갔다. 이번은 소년이 뒤따라 달리지 않았다. 그러고도 곧 소녀보다 더 많은 꽃을 꺾었다.“이게 들국화, 이게 싸리꽃, 이게 도라지꽃,…….”“도라지꽃이 이렇게 예쁜 줄은 몰랐네. 난 보랏빛이 좋아!…… 그런데, 이 양산 같이 생긴 노란 꽃이 뭐지?”“마타리꽃.”소녀는 마타리꽃을 양산 받듯이 해 보인다. 약간 상기된 얼굴에 살포시 보조개를 떠올리며.다시 소년은 꽃 한 옴큼을 꺾어 왔다. 싱싱한 꽃가지만 골라 소녀에게 건넨다.그러나 소녀는“하나도 버리지 마라.”산마루께로 올라갔다.맞은편 골짜기에 오순도순 초가집이 몇 모여 있었다.누가 말할 것도 아닌데, 바위에 나란히 걸터앉았다. 유달리 주위가 조용해진 것 같았다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 앙기모딱.\\nAnswer: '}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.2}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3b5164a5-acb3-404a-9107-dcdd04cf6cb6', 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\npage_label: 6\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 6 -\\n소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알고 있었다. 그것이 이번에는 고향 집마저 남의 손에 넘기게 된 모양이었다.“왜 그런지 난 이사 가는 게 싫어졌다. 어른들이 하는 일이니 어쩔 수 없지만…….”전에 없이, 소녀의 까만 눈에 쓸쓸한 빛이 떠돌았다.소녀와 헤어져 돌아오는 길에, 소년은 혼잣속으로, 소녀가 이사를 간다는 말을 수없이 되뇌어 보았다. 무어 그리 안타까울 것도 서러울 것도 없었다. 그렇건만, 소년은 지금 자기가 씹고 있는 대추알의 단맛을 모르고 있었다.이 날 밤, 소년은 몰래 덕쇠 할아버지네 호두밭으로 갔다.낯에 봐 두었던 나무로 올라갔다. 그리고, 봐 두었던 가지를 향해 작대기를 내리쳤다. 호두송이 떨어지는 소리가 별나게 크게 들렸다. 가슴이 선뜩했다. 그러나 다음 순간, 굵은 호두야 많이 떨어져라, 많이 떨어져라, 저도 모를 힘에 이끌려 마구 작대기를 내리 치는 것이었다.돌아오는 길에는 열 이틀 달이 지우는 그늘만 골라 디뎠다. 그늘의 고마움을 처음 느꼈다.불룩한 주머니를 어루만졌다. 호두송이를 맨손으로 깠다가는 옴이 오르기 쉽다는 말 같은 건 아무렇지도 않았다. 그저 근동에서 제일 가는 이 덕쇠 할아버지네 호두를 어서 소녀에게 맛보여야 한다는 생각만이 앞섰다.그러다, 아차 하는 생각이 들었다. 소녀더러 병이 좀 낫거들랑 이사 가기 전에 한 번 개울가로 나와 달라는 말을 못해 둔 것이었다. 바보 같은것, 바보 같은것.이튿날, 소년이 학교에서 돌아오니, 아버지가 나들이옷으로 갈아입고 닭 한 마리를 안고 있었다.어디 가시느냐고 물었다.그 말에도 대꾸도 없이, 아버지는 안고 있는 닭의 무게를 겨냥해 보면서,“이만하면 될까?”어머니가 망태기를 내주며,“벌써 며칠째 ‘걀걀’하고 알 날 자리를 보던데요.\\n\\npage_label: 3\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 3 -\\n돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다.거기서부터 산 밑까지는 밭이었다.수숫단을 세워 놓은 밭머리를 지났다.“저게 뭐니?”“원두막.”“여기 참외, 맛있니?”“그럼, 참외 맛도 좋지만 수박 맛은 더 좋다.”“하나 먹어 봤으면.”소년이 참외 그루에 심은 무우밭으로 들어가, 무우 두 밑을 뽑아 왔다. 아직 밑이 덜 들어 있었다. 잎을 비틀어 팽개친 후, 소녀에게 한개 건넨다. 그리고는 이렇게 먹어야 한다는 듯이, 먼저 대강이를 한 입 베물어 낸 다음, 손톱으로 한 돌이 껍질을 벗겨 우쩍 깨문다.소녀도 따라 했다. 그러나, 세 입도 못 먹고,“아, 맵고 지려.”하며 집어던지고 만다.“참, 맛 없어 못 먹겠다.”소년이 더 멀리 팽개쳐 버렸다.산이 가까워졌다.단풍이 눈에 따가웠다.“야아!”소녀가 산을 향해 달려갔다. 이번은 소년이 뒤따라 달리지 않았다. 그러고도 곧 소녀보다 더 많은 꽃을 꺾었다.“이게 들국화, 이게 싸리꽃, 이게 도라지꽃,…….”“도라지꽃이 이렇게 예쁜 줄은 몰랐네. 난 보랏빛이 좋아!…… 그런데, 이 양산 같이 생긴 노란 꽃이 뭐지?”“마타리꽃.”소녀는 마타리꽃을 양산 받듯이 해 보인다. 약간 상기된 얼굴에 살포시 보조개를 떠올리며.다시 소년은 꽃 한 옴큼을 꺾어 왔다. 싱싱한 꽃가지만 골라 소녀에게 건넨다.그러나 소녀는“하나도 버리지 마라.”산마루께로 올라갔다.맞은편 골짜기에 오순도순 초가집이 몇 모여 있었다.누가 말할 것도 아닌데, 바위에 나란히 걸터앉았다. 유달리 주위가 조용해진 것 같았다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 앙기모딱.\\nAnswer: '}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.2}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3b5164a5-acb3-404a-9107-dcdd04cf6cb6', 'json_data': {'messages': [{'role': 'system', 'content': \"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\"}, {'role': 'user', 'content': 'Context information is below.\\n---------------------\\npage_label: 6\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 6 -\\n소년은 소녀네가 이사해 오기 전에 벌써 어른들의 이야기를 들어서, 윤 초시 손자(孫子)가 서울서 사업에 실패해 가지고 고향에 돌아오지 않을 수 없게 되었다는 걸 알고 있었다. 그것이 이번에는 고향 집마저 남의 손에 넘기게 된 모양이었다.“왜 그런지 난 이사 가는 게 싫어졌다. 어른들이 하는 일이니 어쩔 수 없지만…….”전에 없이, 소녀의 까만 눈에 쓸쓸한 빛이 떠돌았다.소녀와 헤어져 돌아오는 길에, 소년은 혼잣속으로, 소녀가 이사를 간다는 말을 수없이 되뇌어 보았다. 무어 그리 안타까울 것도 서러울 것도 없었다. 그렇건만, 소년은 지금 자기가 씹고 있는 대추알의 단맛을 모르고 있었다.이 날 밤, 소년은 몰래 덕쇠 할아버지네 호두밭으로 갔다.낯에 봐 두었던 나무로 올라갔다. 그리고, 봐 두었던 가지를 향해 작대기를 내리쳤다. 호두송이 떨어지는 소리가 별나게 크게 들렸다. 가슴이 선뜩했다. 그러나 다음 순간, 굵은 호두야 많이 떨어져라, 많이 떨어져라, 저도 모를 힘에 이끌려 마구 작대기를 내리 치는 것이었다.돌아오는 길에는 열 이틀 달이 지우는 그늘만 골라 디뎠다. 그늘의 고마움을 처음 느꼈다.불룩한 주머니를 어루만졌다. 호두송이를 맨손으로 깠다가는 옴이 오르기 쉽다는 말 같은 건 아무렇지도 않았다. 그저 근동에서 제일 가는 이 덕쇠 할아버지네 호두를 어서 소녀에게 맛보여야 한다는 생각만이 앞섰다.그러다, 아차 하는 생각이 들었다. 소녀더러 병이 좀 낫거들랑 이사 가기 전에 한 번 개울가로 나와 달라는 말을 못해 둔 것이었다. 바보 같은것, 바보 같은것.이튿날, 소년이 학교에서 돌아오니, 아버지가 나들이옷으로 갈아입고 닭 한 마리를 안고 있었다.어디 가시느냐고 물었다.그 말에도 대꾸도 없이, 아버지는 안고 있는 닭의 무게를 겨냥해 보면서,“이만하면 될까?”어머니가 망태기를 내주며,“벌써 며칠째 ‘걀걀’하고 알 날 자리를 보던데요.\\n\\npage_label: 3\\nfile_path: /home/eons/work/koshipa-llm-2025-1st/src/exercise/eons/day03/llama_index_streamilt/data/소나기.pdf\\n\\n- 3 -\\n돌아다보니, 소녀는 지금 자기가 지나쳐 온 허수아비를 흔들고 있다. 좀 전 허수아비보다 더 우쭐거린다.논이 끝난 곳에 도랑이 하나 있었다. 소녀가 먼저 뛰어 건넜다.거기서부터 산 밑까지는 밭이었다.수숫단을 세워 놓은 밭머리를 지났다.“저게 뭐니?”“원두막.”“여기 참외, 맛있니?”“그럼, 참외 맛도 좋지만 수박 맛은 더 좋다.”“하나 먹어 봤으면.”소년이 참외 그루에 심은 무우밭으로 들어가, 무우 두 밑을 뽑아 왔다. 아직 밑이 덜 들어 있었다. 잎을 비틀어 팽개친 후, 소녀에게 한개 건넨다. 그리고는 이렇게 먹어야 한다는 듯이, 먼저 대강이를 한 입 베물어 낸 다음, 손톱으로 한 돌이 껍질을 벗겨 우쩍 깨문다.소녀도 따라 했다. 그러나, 세 입도 못 먹고,“아, 맵고 지려.”하며 집어던지고 만다.“참, 맛 없어 못 먹겠다.”소년이 더 멀리 팽개쳐 버렸다.산이 가까워졌다.단풍이 눈에 따가웠다.“야아!”소녀가 산을 향해 달려갔다. 이번은 소년이 뒤따라 달리지 않았다. 그러고도 곧 소녀보다 더 많은 꽃을 꺾었다.“이게 들국화, 이게 싸리꽃, 이게 도라지꽃,…….”“도라지꽃이 이렇게 예쁜 줄은 몰랐네. 난 보랏빛이 좋아!…… 그런데, 이 양산 같이 생긴 노란 꽃이 뭐지?”“마타리꽃.”소녀는 마타리꽃을 양산 받듯이 해 보인다. 약간 상기된 얼굴에 살포시 보조개를 떠올리며.다시 소년은 꽃 한 옴큼을 꺾어 왔다. 싱싱한 꽃가지만 골라 소녀에게 건넨다.그러나 소녀는“하나도 버리지 마라.”산마루께로 올라갔다.맞은편 골짜기에 오순도순 초가집이 몇 모여 있었다.누가 말할 것도 아닌데, 바위에 나란히 걸터앉았다. 유달리 주위가 조용해진 것 같았다.\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: 앙기모딱.\\nAnswer: '}], 'model': 'gpt-4o-mini', 'stream': False, 'temperature': 0.2}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "close.started\n",
      "close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "close.complete\n",
      "close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=60.0 socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a7d0>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a7d0>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f998802a7d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9987e4ebc0> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9987e4ebc0> server_hostname='api.openai.com' timeout=60.0\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x7f9987e4ebc0> server_hostname='api.openai.com' timeout=60.0\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f99880ca740>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f99880ca740>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7f99880ca740>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'700'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'702'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'7ms'), (b'x-request-id', b'4f391905-2ced-4fa1-97cf-fd5a5f11e319'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9639714bf8443103-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'700'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'702'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'7ms'), (b'x-request-id', b'4f391905-2ced-4fa1-97cf-fd5a5f11e319'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9639714bf8443103-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 23 Jul 2025 07:25:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'weable-inc'), (b'openai-processing-ms', b'700'), (b'openai-project', b'proj_W8BgdzHlUbC22dU3x8XdXgqH'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'702'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9998746'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'7ms'), (b'x-request-id', b'4f391905-2ced-4fa1-97cf-fd5a5f11e319'), (b'cf-cache-status', b'DYNAMIC'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9639714bf8443103-ICN'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:13 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'weable-inc', 'openai-processing-ms': '700', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '702', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '10000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '9998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '7ms', 'x-request-id': '4f391905-2ced-4fa1-97cf-fd5a5f11e319', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9639714bf8443103-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:13 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'weable-inc', 'openai-processing-ms': '700', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '702', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '10000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '9998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '7ms', 'x-request-id': '4f391905-2ced-4fa1-97cf-fd5a5f11e319', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9639714bf8443103-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 23 Jul 2025 07:25:13 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'weable-inc', 'openai-processing-ms': '700', 'openai-project': 'proj_W8BgdzHlUbC22dU3x8XdXgqH', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '702', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '10000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '9998746', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '7ms', 'x-request-id': '4f391905-2ced-4fa1-97cf-fd5a5f11e319', 'cf-cache-status': 'DYNAMIC', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '9639714bf8443103-ICN', 'content-encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "DEBUG:openai._base_client:request_id: 4f391905-2ced-4fa1-97cf-fd5a5f11e319\n",
      "request_id: 4f391905-2ced-4fa1-97cf-fd5a5f11e319\n",
      "request_id: 4f391905-2ced-4fa1-97cf-fd5a5f11e319\n",
      "앙기모딱은 특정한 의미나 설명이 제공되지 않았습니다. 추가적인 정보나 문맥이 필요할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"아카키의 직업은 무엇인가요? 한국어로 대답하세요.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f24f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
