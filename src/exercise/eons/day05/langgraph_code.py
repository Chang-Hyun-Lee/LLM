# filename: code_review_graph.
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import os
import openai
from typing import TypedDict

# LangGraphì—ì„œ ìƒíƒœ íƒ€ì… ëª…ì‹œ
class CodeReviewState(TypedDict):
    code: str
    style_feedback: str
    bug_feedback: str
    opt_feedback: str
    final_code: str

openai.api_key = os.getenv("OPENAI_API_KEY")

# ğŸ’¬ LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4", temperature=0)

# ğŸ§© Node í•¨ìˆ˜ ì •ì˜

def input_code_node(state):
    return state

def style_check_node(state):
    code = state["code"]
    prompt = f"""ì•„ë˜ íŒŒì´ì¬ ì½”ë“œì˜ ìŠ¤íƒ€ì¼ì„ ì ê²€í•´ ì£¼ì„¸ìš”.
- PEP8 ìŠ¤íƒ€ì¼ ìœ„ë°˜ì„ ì°¾ì•„ ìˆ˜ì • ì œì•ˆ
- ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª… ë“± ë„¤ì´ë° ê°œì„ 
- ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ, ë“¤ì—¬ì“°ê¸° ë¬¸ì œ ë“±
ìˆ˜ì •ëœ ì½”ë“œë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”."""
    result = llm.invoke([HumanMessage(content=prompt)])
    return {**state, "style_feedback": result.content}

def bug_check_node(state):
    code = state["code"]
    prompt = f"""ì•„ë˜ íŒŒì´ì¬ ì½”ë“œì—ì„œ ë²„ê·¸ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”.
- ì˜ˆì™¸ì²˜ë¦¬ ëˆ„ë½
- ë³€ìˆ˜ ì´ˆê¸°í™” ì˜¤ë¥˜
- ë…¼ë¦¬ì  ì˜¤ë¥˜
- ê²½ê³  ë°œìƒ ê°€ëŠ¥ì„± ë“±
ë¬¸ì œê°€ ë˜ëŠ” ë¶€ë¶„ê³¼ ê·¸ ì´ìœ , ê·¸ë¦¬ê³  ìˆ˜ì • ì œì•ˆ ì½”ë“œë¥¼ í•¨ê»˜ ì œì‹œí•´ì£¼ì„¸ìš”."""
    result = llm.invoke([HumanMessage(content=prompt)])
    return {**state, "bug_feedback": result.content}

def optimize_check_node(state):
    code = state["code"]
    prompt = f"""ì•„ë˜ íŒŒì´ì¬ ì½”ë“œì˜ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”.
- ë°˜ë³µë¬¸ ìµœì í™”
- ë¶ˆí•„ìš”í•œ ì—°ì‚° ì œê±°
- íš¨ìœ¨ì  ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ ë“±
ê°œì„  ì‚¬í•­ê³¼ í•¨ê»˜ ìµœì í™”ëœ ì½”ë“œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ì„¸ìš”."""
    result = llm.invoke([HumanMessage(content=prompt)])
    return {**state, "opt_feedback": result.content}

def merge_fix_node(state):
    code = state["code"]
    style = state.get("style_feedback", "")
    bug = state.get("bug_feedback", "")
    opt = state.get("opt_feedback", "")

    prompt = f"""ë‹¤ìŒì€ í•œ íŒŒì´ì¬ ì½”ë“œì— ëŒ€í•´ ìŠ¤íƒ€ì¼, ë²„ê·¸, ì„±ëŠ¥ ë¶„ì„ì„ ê°ê° ìˆ˜í–‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

ì›ë³¸ ì½”ë“œ:

[ìŠ¤íƒ€ì¼ í”¼ë“œë°±]
{style}

[ë²„ê·¸ í”¼ë“œë°±]
{bug}

[ì„±ëŠ¥ ìµœì í™” í”¼ë“œë°±]
{opt}

ìœ„ í”¼ë“œë°±ë“¤ì„ ëª¨ë‘ ë°˜ì˜í•œ ìµœì¢… ê°œì„  ì½”ë“œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ì£¼ì„ ì—†ì´ ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
    result = llm.invoke([HumanMessage(content=prompt)])
    return {**state, "final_code": result.content}

def final_output_node(state):
    print("\nğŸ’¡ ìµœì¢… ê°œì„ ëœ ì½”ë“œ:\n")
    print(state["final_code"])
    return state

# ğŸ§± LangGraph êµ¬ì„±
builder = StateGraph(state_type=dict)

builder.add_node("InputCode", RunnableLambda(input_code_node))
builder.add_node("StyleCheck", RunnableLambda(style_check_node))
builder.add_node("BugCheck", RunnableLambda(bug_check_node))
builder.add_node("OptimizeCheck", RunnableLambda(optimize_check_node))
builder.add_node("MergeFix", RunnableLambda(merge_fix_node))
builder.add_node("FinalOutput", RunnableLambda(final_output_node))

builder.set_entry_point("InputCode")

builder.add_edge("InputCode", "StyleCheck")
builder.add_edge("StyleCheck", "BugCheck")
builder.add_edge("BugCheck", "OptimizeCheck")
builder.add_edge("OptimizeCheck", "MergeFix")
builder.add_edge("MergeFix", "FinalOutput")
builder.add_edge("FinalOutput", END)

graph = builder.compile()

# ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    user_code = """
def add(a,b):
 return a+  b
"""
    print("ğŸ“ ì…ë ¥ ì½”ë“œ:\n", user_code)
    graph.invoke({"code": user_code})
