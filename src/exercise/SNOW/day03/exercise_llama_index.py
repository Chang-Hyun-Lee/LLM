import os
import streamlit as st
from pathlib import Path
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI

# ğŸ”‘ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
openai_key = st.secrets["OPENAI_API_KEY"]

# ğŸ“ ë¬¸ì„œ ì €ì¥ ê²½ë¡œ
UPLOAD_DIR = Path("docs")
UPLOAD_DIR.mkdir(exist_ok=True)

# ğŸ§  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
for key, default in {
    "index": None,
    "query_engine": None,
    "chat_history": [],
    "selected_file": None
}.items():
    if key not in st.session_state:
        st.session_state[key] = default

st.title("ğŸ“š GPT-4o-mini + LlamaIndex ë¬¸ì„œ ì±—ë´‡")
st.caption("íŒŒì¼ ì—…ë¡œë“œ, ìš”ì•½, ë©€í‹°í„´ ëŒ€í™” ë° ì‚­ì œ ê¸°ëŠ¥ í¬í•¨")

# ğŸ“¤ íŒŒì¼ ë‹¤ì¤‘ ì—…ë¡œë“œ
uploaded_files = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "txt", "md"], accept_multiple_files=True)
if uploaded_files:
    for uploaded_file in uploaded_files:
        file_path = UPLOAD_DIR / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())
    st.success(f"âœ… {len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")
    st.rerun()

# ğŸ“‹ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë° ì„ íƒ
files = sorted(UPLOAD_DIR.glob("*"))
file_names = [f.name for f in files]
selected = st.selectbox("ë¬¸ì„œ ì„ íƒ", file_names if file_names else ["(íŒŒì¼ ì—†ìŒ)"])

# ğŸ§¹ ì„ íƒëœ íŒŒì¼ ì‚­ì œ
if selected and selected != "(íŒŒì¼ ì—†ìŒ)":
    col1, col2 = st.columns([4, 1])
    with col2:
        if st.button("ì‚­ì œ"):
            os.remove(UPLOAD_DIR / selected)
            st.success(f"`{selected}` ì‚­ì œë¨")
            st.rerun()

# â³ ì„ íƒëœ íŒŒì¼ì˜ ì¸ë±ìŠ¤ ì¬ìƒì„±
if selected != "(íŒŒì¼ ì—†ìŒ)" and selected != st.session_state.selected_file:
    try:
        file_path = UPLOAD_DIR / selected
        docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
        llm = OpenAI(model="gpt-4o-mini", api_key=openai_key)
        index = VectorStoreIndex.from_documents(docs)
        query_engine = index.as_query_engine(llm=llm)

        st.session_state.index = index
        st.session_state.query_engine = query_engine
        st.session_state.selected_file = selected
        st.session_state.chat_history = []

        st.success("ğŸ“š ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
    except Exception as e:
        st.error(f"â— ì¸ë±ìŠ¤ ì˜¤ë¥˜: {e}")

# ğŸ’¬ ì±—ë´‡ UI
if st.session_state.query_engine:
    if not st.session_state.chat_history:
        st.markdown("ğŸ‘‹ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì±—ë´‡ì´ ì‘ë‹µí•©ë‹ˆë‹¤.")

    for q, a in st.session_state.chat_history:
        with st.chat_message("user"):
            st.markdown(q)
        with st.chat_message("assistant"):
            st.markdown(a)

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)
        try:
            response = st.session_state.query_engine.query(user_input).response
        except Exception as e:
            response = f"â— ì‘ë‹µ ì˜¤ë¥˜: {e}"
        with st.chat_message("assistant"):
            st.markdown(response)
        st.session_state.chat_history.append((user_input, response))

    # ğŸ“ ìš”ì•½ ìš”ì²­ ë²„íŠ¼
    if st.button("ë¬¸ì„œ ìš”ì•½ ë°›ê¸°"):
        with st.chat_message("user"):
            st.markdown("ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜")
        try:
            summary = st.session_state.query_engine.query("ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜").response
        except Exception as e:
            summary = f"â— ìš”ì•½ ì˜¤ë¥˜: {e}"
        with st.chat_message("assistant"):
            st.markdown(summary)
        st.session_state.chat_history.append(("ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì¤˜", summary))

# ğŸ§ª ìƒíƒœ ë””ë²„ê·¸ (ì‚¬ì´ë“œë°”)
st.sidebar.header("ğŸ” ìƒíƒœ ë³´ê¸°")
st.sidebar.write("ğŸ“„ ì„ íƒëœ ë¬¸ì„œ:", st.session_state.selected_file or "ì—†ìŒ")
st.sidebar.write("ğŸ§  Query Engine:", "âœ… ìˆìŒ" if st.session_state.query_engine else "âŒ ì—†ìŒ")
st.sidebar.write("ğŸ“œ ëŒ€í™” ìˆ˜:", len(st.session_state.chat_history))