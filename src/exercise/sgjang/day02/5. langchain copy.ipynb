{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967c53a9-cb65-49a5-99f4-145651f38bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/sgjang/.local/lib/python3.10/site-packages (1.97.1)\n",
      "Requirement already satisfied: langchain in /home/sgjang/.local/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-openai in /home/sgjang/.local/lib/python3.10/site-packages (0.3.28)\n",
      "Requirement already satisfied: langchain-community in /home/sgjang/.local/lib/python3.10/site-packages (0.3.27)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (0.3.71)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (3.12.14)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/sgjang/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/sgjang/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/sgjang/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/sgjang/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/sgjang/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/sgjang/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/sgjang/.local/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sgjang/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/sgjang/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/sgjang/.local/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/sgjang/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install openai\n",
    "#!pip install langchain-core==0.2.17\n",
    "#!pip install langchain-openai==0.1.17\n",
    "#!pip install pydantic==1.10.8\n",
    "#!pip install pydantic_core==2.18.2\n",
    "!pip install -U openai langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020041bc-3149-4195-b28f-4d38c65cbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/sgjang/.local/lib/python3.10/site-packages (from google-search-results) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from requests->google-search-results) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sgjang/.local/lib/python3.10/site-packages (from requests->google-search-results) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->google-search-results) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->google-search-results) (3.3)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32016 sha256=b7d5e5e9bfb0ff53473064007f25750211b30db0a8bafa4f8e589a62f6026b5f\n",
      "  Stored in directory: /home/sgjang/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.3/187.3 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /home/sgjang/.local/lib/python3.10/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sgjang/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=864b65f237ec394d661f43f595bd332603dd44d05a6aa3d3477df32121eada9c\n",
      "  Stored in directory: /home/sgjang/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7 wikipedia-1.4.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/sgjang/.local/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/sgjang/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /home/sgjang/.local/lib/python3.10/site-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/sgjang/.local/lib/python3.10/site-packages (from sentence_transformers) (0.33.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.14.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /home/sgjang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: filelock in /home/sgjang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/sgjang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
      "Requirement already satisfied: requests in /home/sgjang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sgjang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.7.0)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.1.2\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.1\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.7.77\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 KB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.6.80\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.6.4.1\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.5.1.17\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.6.77\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufile-cu12==1.11.1.6\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.4.2\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.3\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.6.85\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence_transformers) (59.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sgjang/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/sgjang/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /home/sgjang/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.6.20)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, threadpoolctl, sympy, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence_transformers\n",
      "Successfully installed joblib-1.5.1 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.15.3 sentence_transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.3 triton-3.3.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/sgjang/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/sgjang/.local/lib/python3.10/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/sgjang/.local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sgjang/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/sgjang/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results\n",
    "!pip install wikipedia\n",
    "!pip install faiss-cpu \n",
    "!pip install sentence_transformers \n",
    "!pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a332d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field # 예전 방식\n",
    "\n",
    "# from pydantic import BaseModel # 새로운 방식\n",
    "\n",
    "from pydantic.v1 import BaseModel # pydantic.v1 호환이 필요한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd2a51b-6740-4a14-8ad9-fb59e3ec001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/2744496630.py:15: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  aimsg = chat([sys, msg])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1990년대의 재즈는 다양한 스타일과 아티스트들이 혼합된 시기로, 매우 흥미로운 음악들이 많이 나왔습니다. 다음은 1990년대에 발표된 재즈 곡 다섯 곡을 추천해 드립니다:\\n\\n1. **\"Tutu\" - Miles Davis (1986)**\\n   - 비록 1986년에 발매되었지만, 1990년대 재즈 음악에도 큰 영향을 미친 곡입니다. 전자재즈의 새로운 경향을 제시한 곡으로, 마일스 데이비스의 독창적인 스타일이 돋보입니다.\\n\\n2. **\"Love from the Sun\" - Wayne Shorter (1997)**\\n   - 웨인 쇼터의 앨범 \"High Life\"에 수록된 곡으로, 그의 독특한 색채감과 아름다운 멜로디가 특징입니다.\\n\\n3. **\"Dance of the Infidels\" - McCoy Tyner (1992)**\\n   - 맥코이 타이너의 앨범 \"Inception\"에서 이 곡은 그의 뛰어난 피아노 연주와 창의적인 작곡이 잘 드러나는 작품입니다.\\n\\n4. **\"Deep Song\" - Charles Lloyd (1998)**\\n   - 찰스 로이드의 앨범 \"The Water Is Wide\"에 수록된 곡으로, 그의 트럼펫 소리와 감정이 풍부한 연주가 인상적입니다.\\n\\n5. **\"After the Rain\" - Bill Evans (1997)**\\n   - 재즈 피아니스트 빌 에반스의 앨범 \"From Left to Right\"에서 이 곡은 그의 서정적이고 감성적인 스타일을 잘 보여줍니다.\\n\\n이 곡들은 1990년대 재즈의 다양성과 매력을 잘 담고 있습니다. 즐감해 주세요!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 음악 추천을 해주는 전문 AI입니다.\")\n",
    "msg = HumanMessage(content='1990년대 재즈 5곡만 추천해줘.')\n",
    "\n",
    "aimsg = chat([sys, msg])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e5f17c-6a25-4489-919f-b1e477368296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/1939471710.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1박 2일 여행으로 추천할 만한 곳은 다음과 같습니다:\\n\\n1. **하롱베이**: 아름다운 석회암 카르스트 지형과 바다 풍경을 즐길 수 있습니다. 크루즈, 카약, 동굴 탐험 등 다양한 액티비티가 가능합니다.\\n\\n2. **호이안**: 전통적인 건축물과 아름다운 해변이 있는 곳입니다. 구시가지 탐방, 요리 수업, 다낭 근처의 해변 방문 등을 할 수 있습니다.\\n\\n3. **다낭**: 아름다운 해변과 맛있는 음식, 그리고 바나힐과 같은 관광 명소가 있어 즐길 거리가 많습니다.\\n\\n4. **니트란**: 조용하고 아늑한 해변 마을로, 리조트에서 편안히 쉬거나 스노클링 등을 즐길 수 있습니다.\\n\\n이 외에도 많은 멋진 장소가 있으니, 관심 있는 활동이나 풍경에 따라 선택하시면 좋을 것 같습니다!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 여행을 추천해주는 전문 AI입니다.\")\n",
    "msg1 = HumanMessage(content='베트남 사파에 1박 2일 여행 가면 충분할까?')\n",
    "msg2 = AIMessage(content='사파는 1박 2일로 가기에는 너무 멉니다. 일정을 조정하거나 다른 여행지를 선택하시면 어떨까요?')\n",
    "msg3 = HumanMessage(content='음 ... 그러면 어디가 좋을까?')\n",
    "\n",
    "aimsg = chat([sys, msg1, msg2, msg3])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5f966f-2923-4af7-9e96-bab696d6c956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI 여행 추천 서비스 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate를 사용하여 각 LLM에 맞는 prompt를 지정할 수 있음\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"상품\"],\n",
    "    template=\"{상품} 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로\",\n",
    ")\n",
    "\n",
    "prompt.format(상품=\"AI 여행 추천 서비스\") # 변수가 여러 개 일 때, 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ae1232-2911-463a-838f-47eb2cbf5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/2518977364.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=prompt)\n",
      "/tmp/ipykernel_45969/2518977364.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(상품=\"학원 추천 서비스\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'물론입니다! 학원 추천 서비스에 어울리는 기억에 남는 한글 이름 몇 가지를 제안해드릴게요.\\n\\n1. **학원찾기**\\n2. **배움지기**\\n3. **학원리스트**\\n4. **스승의손길**\\n5. **학원동행**\\n6. **지식나래**\\n7. **공부길잡이**\\n8. **학습마당**\\n9. **배움터**\\n10. **학원서칭**\\n\\n이 중에서 마음에 드는 이름이 있으면 좋겠네요! 추가적인 도움이 필요하시면 말씀해 주세요.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "chain.run(상품=\"학원 추천 서비스\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cede08e-1a08-40de-b79a-edee95d7950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 프로그래밍을 사랑해.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 Prompt 별 template 지정도 가능\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)  \n",
    "\n",
    "template=\"You are a helpful assisstant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "chatchain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chatchain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e138db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (399 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.8/399.8 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.0 in /home/sgjang/.local/lib/python3.10/site-packages (from numexpr) (2.2.6)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96ce4ed-c8cd-453a-941f-67f937037653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/2673388559.py:10: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm=chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m아마존의 창업자는 제프 베조스(Jeff Bezos)이다. 그의 나이를 알아내고, 배우자의 나이도 알아내야 한다. 제프 베조스의 나이를 확인하기 위해 위키피디아를 검색하겠다. \n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"Jeff Bezos\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Jeff Bezos\n",
      "Summary: Jeffrey Preston Bezos ( BAY-zohss; né Jorgensen; born January 12, 1964) is an American businessman best known as the founder, executive chairman, and former president and CEO of Amazon, the world's largest e-commerce and cloud computing company. According to Forbes, as of May 2025, Bezos's estimated net worth exceeded $220 billion, making him the third richest person in the world. He was the wealthiest person from 2017 to 2021, according to Forbes and the Bloomberg Billionaires Index.\n",
      "Bezos was born in Albuquerque and raised in Houston and Miami. He graduated from Princeton University in 1986 with degrees in electrical engineering and computer science. He worked on Wall Street in a variety of related fields from 1986 to early 1994. Bezos founded Amazon in mid-1994 on a road trip from New York City to Seattle. The company began as an online bookstore and has since expanded to a variety of other e-commerce products and services, including video and audio streaming, cloud computing, and artificial intelligence. It is the world's largest online sales company, the largest Internet company by revenue, and the largest provider of virtual assistants and cloud infrastructure services through its Amazon Web Services branch.\n",
      "Bezos founded the aerospace manufacturer and sub-orbital spaceflight services company Blue Origin in 2000. Blue Origin's New Shepard vehicle reached space in 2015 and afterwards successfully landed back on Earth; he flew into space on Blue Origin NS-16 in 2021. He purchased the major American newspaper The Washington Post in 2013 for $250 million and manages many other investments through his venture capital firm, Bezos Expeditions. In September 2021, Bezos co-founded Altos Labs with Mail.ru founder Yuri Milner.\n",
      "The first centibillionaire on the Forbes Real Time Billionaires Index and the second ever to have achieved the feat since Bill Gates in 1999, Bezos was named the \"richest man in modern history\" after his net worth increased to $150 billion in July 2018. In August 2020, according to Forbes, he had a net worth exceeding $200 billion. On July 5, 2021, Bezos stepped down as the CEO and president of Amazon and took over the role of executive chairman. Amazon Web Services CEO Andy Jassy succeeded Bezos as the CEO and president of Amazon.\n",
      "\n",
      "Page: Wedding of Jeff Bezos and Lauren Sánchez\n",
      "Summary: The wedding of American businessman Jeff Bezos and media personality Lauren Sánchez took place in Venice, Italy, between June 26–28, 2025. The multi-day event, held at various historic sites across the city, drew significant media attention due to its high-profile guest list, lavish celebrations, and extensive security measures. It also sparked local protests and criticism from Venetian activists over the city's commercialization and use as a venue for private luxury events.\n",
      "\n",
      "\n",
      "\n",
      "Page: Family of Jeff Bezos\n",
      "Summary: The family of Jeff Bezos, who is an American business magnate, is active in business and philanthropy. Bezos and his former wife MacKenzie Scott co-founded Amazon.\n",
      "Jeff's mother and stepfather are the co-founders of the Bezos Family Foundation.\n",
      "The family members have Danish-American, Mexican American and Cuban heritage.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found information about Jeff Bezos, including his birth date, which is January 12, 1964. This means he is currently 59 years old (as of 2023). However, I need to find the age of his spouse, Lauren Sánchez, to calculate the product of their ages.\n",
      "\n",
      "Action: wikipedia  \n",
      "Action Input: \"Lauren Sánchez\"  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Lauren Sánchez\n",
      "Summary: Lauren Wendy Sánchez Bezos (née Sánchez; born December 19, 1969) is an American philanthropist and former journalist. She is a licensed pilot and founder of Black Ops Aviation, an aerial film and production company. Sánchez has been a guest host on The View, co-host on KTTV Fox 11's Good Day L.A. and anchor on the Fox 11 News at Ten, and anchor and special correspondent on Extra. She has also been a regular contributor on shows including Larry King Live, The Joy Behar Show and Showbiz Tonight. \n",
      "In 2024, Sánchez released her debut children's book, The Fly Who Flew to Space, which became a New York Times bestseller. On April 14, 2025, Sánchez flew as a passenger in Blue Origin NS-31, an all-female spaceflight aboard the New Shepard rocket.\n",
      "In June 2025, Sánchez married Jeff Bezos, the third-richest person in the world.\n",
      "\n",
      "Page: Wedding of Jeff Bezos and Lauren Sánchez\n",
      "Summary: The wedding of American businessman Jeff Bezos and media personality Lauren Sánchez took place in Venice, Italy, between June 26–28, 2025. The multi-day event, held at various historic sites across the city, drew significant media attention due to its high-profile guest list, lavish celebrations, and extensive security measures. It also sparked local protests and criticism from Venetian activists over the city's commercialization and use as a venue for private luxury events.\n",
      "\n",
      "\n",
      "\n",
      "Page: Jeff Bezos\n",
      "Summary: Jeffrey Preston Bezos ( BAY-zohss; né Jorgensen; born January 12, 1964) is an American businessman best known as the founder, executive chairman, and former president and CEO of Amazon, the world's largest e-commerce and cloud computing company. According to Forbes, as of May 2025, Bezos's estimated net worth exceeded $220 billion, making him the third richest person in the world. He was the wealthiest person from 2017 to 2021, according to Forbes and the Bloomberg Billionaires Index.\n",
      "Bezos was born in Albuquerque and raised in Houston and Miami. He graduated from Princeton University in 1986 with degrees in electrical engineering and computer science. He worked on Wall Street in a variety of related fields from 1986 to early 1994. Bezos founded Amazon in mid-1994 on a road trip from New York City to Seattle. The company began as an online bookstore and has since expanded to a variety of other e-commerce products and services, including video and audio streaming, cloud computing, and artificial intelligence. It is the world's largest online sales company, the largest Internet company by revenue, and the largest provider of virtual assistants and cloud infrastructure services through its Amazon Web Services branch.\n",
      "Bezos founded the aerospace manufacturer and sub-orbital spaceflight services company Blue Origin in 2000. Blue Origin's New Shepard vehicle reached space in 2015 and afterwards successfully landed back on Earth; he flew into space on Blue Origin NS-16 in 2021. He purchased the major American newspaper The Washington Post in 2013 for $250 million and manages many other investments through his venture capital firm, Bezos Expeditions. In September 2021, Bezos co-founded Altos Labs with Mail.ru founder Yuri Milner.\n",
      "The first centibillionaire on the Forbes Real Time Billionaires Index and the second ever to have achieved the feat since Bill Gates in 1999, Bezos was named the \"richest man in modern history\" after his net worth increased to $150 billion in July 2018. In August 2020, according to Forbes, he had a net worth exceeding $200 billion. On July 5, 2021, Bezos stepped down as the CEO and president of Amazon and took over the role of executive chairman. Amazon Web Services CEO Andy Jassy succeeded Bezos as the CEO and president of Amazon.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the birth date of Lauren Sánchez, which is December 19, 1969. This means she is currently 53 years old (as of 2023). Now, I can calculate the product of their ages: Jeff Bezos is 59 years old and Lauren Sánchez is 53 years old.\n",
      "\n",
      "Action: Calculator  \n",
      "Action Input: 59 * 53  \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3127\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: 3127\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3127'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatOpenAI를 LLM으로 사용하는 agent\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0) \n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=chat)\n",
    "\n",
    "agent = initialize_agent(tools, llm=chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) \n",
    "\n",
    "agent.run(\"아마존의 창업자는 누구인가? 그의 나이와 배우자의 나이를 곱하면 얼마인가?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84a01db-ce84-4ddf-be0d-9976ba3580ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/sgjang/.local/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7d281349a500>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7d281349aec0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7d281349a500>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7d281349aec0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47498a86-a6b0-4e43-bbe3-e84d6e338b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Useful for when you need to answer questions about math.\n"
     ]
    }
   ],
   "source": [
    "print(agent.tools[0].description)\n",
    "print(agent.tools[1].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c1c5cf-26f1-448f-8687-784ca932ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/2149845611.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=chat, verbose=True)\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM은 \"대규모 언어 모델\"(Large Language Model)의 약자야. 이러한 모델은 방대한 양의 텍스트 데이터를 기반으로 언어를 이해하고 생성하는 데 사용되지. LLM은 주로 기계 학습 기술을 이용해 훈련되며, 자연어 처리(NLP) 작업에 매우 효과적이야. 예를 들어, LLM은 텍스트 생성, 번역, 질문 응답, 요약 등 다양한 작업을 수행할 수 있어. \\n\\n또한, LLM은 인공지능의 발전에 큰 기여를 하고 있으며, 최근에는 GPT, BERT, T5와 같은 여러 모델들이 주목받고 있어. 이런 모델들은 대규모 데이터셋을 통해 훈련되어서 자연어의 패턴을 효과적으로 학습하게 돼. 더 궁금한 점이 있으면 언제든지 물어봐!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "conversation = ConversationChain(llm=chat, verbose=True)\n",
    "conversation.predict(input=\"인공지능에서 LLM이 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bd1f46e-daa1-4098-8d3e-4c36f4ec0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI: LLM은 \"대규모 언어 모델\"(Large Language Model)의 약자야. 이러한 모델은 방대한 양의 텍스트 데이터를 기반으로 언어를 이해하고 생성하는 데 사용되지. LLM은 주로 기계 학습 기술을 이용해 훈련되며, 자연어 처리(NLP) 작업에 매우 효과적이야. 예를 들어, LLM은 텍스트 생성, 번역, 질문 응답, 요약 등 다양한 작업을 수행할 수 있어. \n",
      "\n",
      "또한, LLM은 인공지능의 발전에 큰 기여를 하고 있으며, 최근에는 GPT, BERT, T5와 같은 여러 모델들이 주목받고 있어. 이런 모델들은 대규모 데이터셋을 통해 훈련되어서 자연어의 패턴을 효과적으로 학습하게 돼. 더 궁금한 점이 있으면 언제든지 물어봐!\n",
      "Human: PLM과의 차이를 설명해줘.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PLM은 \"프리트레인드 언어 모델\"(Pre-trained Language Model)의 약자로, LLM의 한 종류라고 볼 수 있어. 두 용어 모두 언어 모델을 설명하지만, PLM은 특정한 훈련 방식에 중점을 둬. \\n\\nLLM은 일반적으로 대규모 데이터셋을 사용해서 훈련된 모델을 지칭하는 반면, PLM은 먼저 대규모 데이터로 일반적인 언어 지식을 학습한 후, 특정 작업에 맞게 추가로 훈련되는 방식이야. 즉, PLM은 대규모로 프리트레인된 후 특정 도메인이나 태스크에 맞게 파인튜닝(tuning)되는 과정을 거치는 모델이야.\\n\\n예를 들어, BERT와 GPT는 LLM의 예지만, 그들 모두 PLM으로도 간주될 수 있어. 이러한 모델은 다양한 자연어 처리 작업에서 뛰어난 성능을 보이고 있어. 더 궁금한 점이 있으면 언제든지 물어봐!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"PLM과의 차이를 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b7e4767-87f5-4731-be17-7d74b9a309f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='인공지능에서 LLM이 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='LLM은 \"대규모 언어 모델\"(Large Language Model)의 약자야. 이러한 모델은 방대한 양의 텍스트 데이터를 기반으로 언어를 이해하고 생성하는 데 사용되지. LLM은 주로 기계 학습 기술을 이용해 훈련되며, 자연어 처리(NLP) 작업에 매우 효과적이야. 예를 들어, LLM은 텍스트 생성, 번역, 질문 응답, 요약 등 다양한 작업을 수행할 수 있어. \\n\\n또한, LLM은 인공지능의 발전에 큰 기여를 하고 있으며, 최근에는 GPT, BERT, T5와 같은 여러 모델들이 주목받고 있어. 이런 모델들은 대규모 데이터셋을 통해 훈련되어서 자연어의 패턴을 효과적으로 학습하게 돼. 더 궁금한 점이 있으면 언제든지 물어봐!', additional_kwargs={}, response_metadata={}), HumanMessage(content='PLM과의 차이를 설명해줘.', additional_kwargs={}, response_metadata={}), AIMessage(content='PLM은 \"프리트레인드 언어 모델\"(Pre-trained Language Model)의 약자로, LLM의 한 종류라고 볼 수 있어. 두 용어 모두 언어 모델을 설명하지만, PLM은 특정한 훈련 방식에 중점을 둬. \\n\\nLLM은 일반적으로 대규모 데이터셋을 사용해서 훈련된 모델을 지칭하는 반면, PLM은 먼저 대규모 데이터로 일반적인 언어 지식을 학습한 후, 특정 작업에 맞게 추가로 훈련되는 방식이야. 즉, PLM은 대규모로 프리트레인된 후 특정 도메인이나 태스크에 맞게 파인튜닝(tuning)되는 과정을 거치는 모델이야.\\n\\n예를 들어, BERT와 GPT는 LLM의 예지만, 그들 모두 PLM으로도 간주될 수 있어. 이러한 모델은 다양한 자연어 처리 작업에서 뛰어난 성능을 보이고 있어. 더 궁금한 점이 있으면 언제든지 물어봐!', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67d6347c-2c99-4d28-b9e6-b7585d72a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2293, which is longer than the specified 1000\n",
      "Created a chunk of size 2843, which is longer than the specified 1000\n",
      "Created a chunk of size 6664, which is longer than the specified 1000\n",
      "Created a chunk of size 2194, which is longer than the specified 1000\n",
      "Created a chunk of size 1471, which is longer than the specified 1000\n",
      "Created a chunk of size 2774, which is longer than the specified 1000\n",
      "Created a chunk of size 1873, which is longer than the specified 1000\n",
      "Created a chunk of size 3432, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 2746, which is longer than the specified 1000\n",
      "Created a chunk of size 3230, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "# https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_path=\"https://ko.wikipedia.org/wiki/IU\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692cd1a0-48c1-48b6-977c-a2a48ea96fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='아이유 - 위키백과, 우리 모두의 백과사전\\n\\n\\n본문으로 이동\\n\\n주 메뉴\\n\\n주 메뉴\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t둘러보기\\n\\t\\n\\n\\n대문최근 바뀜요즘 화제임의의 문서로\\n\\n\\t\\t사용자 모임\\n\\t\\n\\n\\n사랑방사용자 모임관리 요청\\n\\n\\t\\t편집 안내\\n\\t\\n\\n\\n소개도움말정책과 지침질문방\\n\\n검색\\n\\n검색\\n\\n\\n보이기\\n\\n\\n기부\\n\\n계정 만들기\\n\\n로그인\\n\\n\\n개인 도구\\n\\n기부 계정 만들기 로그인\\n\\n\\n목차\\n사이드바로 이동\\n숨기기\\n\\n\\n처음 위치\\n\\n1\\n학력\\n\\n\\n2\\n활동\\n\\n\\n활동 하위섹션 토글하기\\n\\n2.1\\n2008-2010\\n\\n\\n2.2\\n2011-2013\\n\\n\\n2.3\\n2014-2016\\n\\n\\n2.4\\n2017-2018\\n\\n\\n2.5\\n2019-2020\\n\\n\\n2.6\\n2021-2022\\n\\n\\n2.7\\n2023-2024\\n\\n\\n2.8\\n2025\\n\\n\\n3\\n음반 목록\\n\\n\\n4\\n음반 외 활동 목록\\n\\n\\n5\\n수상 및 후보 목록\\n\\n\\n6\\n사건\\n\\n\\n7\\n같이 보기\\n\\n\\n8\\n각주\\n\\n\\n각주 하위섹션 토글하기\\n\\n8.1\\n내용주\\n\\n\\n8.2\\n참조주\\n\\n\\n9\\n외부 링크\\n\\n\\n목차 토글\\n\\n아이유\\n\\n63개 언어'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='العربيةمصرىتۆرکجهBasa BaliBikol CentralБългарскиCatalàCebuanoکوردیČeštinaDanskDeutschKadazandusunΕλληνικάEnglishEsperantoEspañolEestiEuskaraفارسیSuomiFrançaisहिन्दीKreyòl ayisyenMagyarՀայերենInterlinguaBahasa IndonesiaItaliano日本語JawaҚазақшаکٲشُرLatinaMāoriമലയാളംМонголBahasa Melayuမြန်မာဘာသာNederlandsNorsk bokmålPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSoomaaligaСрпски / srpskiSesothoSundaSvenskaதமிழ்ไทยTagalogSetswanaTürkçeУкраїнськаOʻzbekcha / ўзбекчаTiếng Việt中文閩南語 / Bân-lâm-gí粵語\\n\\n링크 편집\\n\\n문서토론\\n\\n한국어\\n\\n읽기원본 보기역사 보기\\n\\n도구\\n\\n도구\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t동작\\n\\t\\n\\n\\n읽기원본 보기역사 보기\\n\\n\\t\\t일반\\n\\t\\n\\n\\n여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR코드 다운로드\\n\\n\\t\\t인쇄/내보내기\\n\\t\\n\\n\\n책 만들기PDF로 다운로드인쇄용 판\\n\\n\\t\\t다른 프로젝트\\n\\t\\n\\n\\n위키미디어 공용위키데이터 항목\\n\\n보이기\\n사이드바로 이동\\n숨기기\\n\\n\\n위키백과, 우리 모두의 백과사전.'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='(IU에서 넘어옴)\\n  다른 뜻에 대해서는 아이유 (동음이의) 문서를 참고하십시오. IU는 여기로 연결됩니다. 다른 뜻에 대해서는 IU (동음이의) 문서를 참고하십시오.\\n아이유2022년 11월 25일 청룡영화상에서의 아이유기본 정보본명이지은출생1993년 5월 16일(1993-05-16)(32세)대한민국 서울특별시 성동구 송정동성별여성직업가수, 배우장르댄스, 발라드, 어쿠스틱, R&B활동 시기2008년~학력동덕여자고등학교 (졸업)레이블카카오엔터테인먼트, EMI 뮤직 재팬소속사EDAM 엔터테인먼트서명웹사이트edam-ent.com유튜브채널이지금 [IU Official]분야일상, 음악개설일2017년 2월 13일구독자 수978만\\xa0명(2024년 10월 6일 기준)총 조회수3,150,968,674회(2024년 10월 6일 기준)vte\\n\\n아이유(IU, 본명:\\xa0이지은, 李知恩[1], 1993년 5월 16일~)는 대한민국의 싱어송라이터, 작곡가, 배우이다. 2007년 로엔 엔터테인먼트(현 카카오 엔터테인먼트) 연습생으로 전속 계약을 맺고 15세의 나이에 2008년 첫 EP인 로스트 앤 파운드(Lost and Found)를 통해 가수로 데뷔했다.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36be6f88-571b-4010-bbdb-af25b75a43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45969/4030962451.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#embeddings = HuggingFaceEmbeddings() # sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    ).from_loaders([loader])\n",
    "\n",
    "# 파일로 저장\n",
    "index.vectorstore.save_local(\"faiss-nj-IU\")\n",
    "# db = FAISS.load_local(\"faiss-nj-IU\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c656357-f0e7-439b-b6c6-14927f4e7c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유의 데뷔곡은 \"미아\"입니다. 이 곡은 그녀가 2008년 첫 EP인 \"로스트 앤 파운드\"를 통해 발표한 곡입니다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 데뷔곡은?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aebc9b7c-64b9-4cd5-be7d-f2e525e6a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유의 나이는 2024년 10월 6일 기준으로 32세입니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1917167b-5b64-4e99-9b8e-17a6047d5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유는 1993년 5월 16일에 태어났으므로, 2025년 7월 22일 기준으로 32세입니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는? (오늘은 2025년 7월 22일)\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403356b0-5ef9-45a1-b0d7-743866575946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그에 대한 정보는 제시된 문맥에 포함되어 있지 않습니다. 따라서 아이유가 지금까지 총 몇 회의 콘서트를 했는지에 대해서는 알지 못합니다.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유는 지금까지 총 콘서트를 몇회했는가?\", llm=chat, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
