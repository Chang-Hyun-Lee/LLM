# pdf_qa_app.py

import streamlit as st
import os
import tempfile

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.chains.qa_with_sources import load_qa_with_sources_chain

# ğŸ” OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "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"

# ğŸ“˜ Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="LangChain PDF ì§ˆë¬¸ ì‘ë‹µê¸°", page_icon="ğŸ“„")
st.title("ğŸ“„ LangChain PDF ì§ˆë¬¸ ì‘ë‹µê¸°")

# ğŸ“¤ PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")

if uploaded_file:
    # ì„ì‹œ ì €ì¥ì†Œì— íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_pdf_path = tmp_file.name

    st.success("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ. ì¸ë±ì‹± ì¤‘...")

    # ğŸ“„ ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    loader = PyPDFLoader(tmp_pdf_path)
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)

    # ğŸ” ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(split_docs, embeddings)

    retriever = db.as_retriever()

    # ğŸ¤– ê³ ê¸‰ QA ì²´ì¸ (map_reduce)
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="map_reduce",  # í•µì‹¬ ê°œì„ !
        retriever=retriever,
        return_source_documents=False,
    )

    # ğŸ§  ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    st.markdown("#### ì˜ˆì‹œ ì§ˆë¬¸:")
    st.markdown("- ì†Œë…€ê°€ ë˜ì§„ ì¡°ì•½ëŒì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
    st.markdown("- ë§ˆì§€ë§‰ ì¥ë©´ì—ì„œ ë¬´ìŠ¨ ì¼ì´ ë²Œì–´ì¡Œë‚˜ìš”?")
    st.markdown("- ì†Œë‚˜ê¸°ì˜ ìƒì§•ì  ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")

    query = st.text_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if query:
        with st.spinner("ğŸ¤– GPTê°€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                result = qa_chain.run(query)
                st.markdown("#### ğŸ§  ë‹µë³€:")
                st.success(result)
            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
