{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967c53a9-cb65-49a5-99f4-145651f38bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/ubuntu/.local/lib/python3.10/site-packages (1.97.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66\n",
      "  Downloading langchain_core-0.3.70-py3-none-any.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.3/442.3 KB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langsmith>=0.1.17\n",
      "  Downloading langsmith-0.4.8-py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.0/368.0 KB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-community) (2.2.6)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 KB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 KB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 KB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ubuntu/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ubuntu/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Collecting greenlet>=1\n",
      "  Downloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.2/582.2 KB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: zstandard, tenacity, regex, propcache, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, async-timeout, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.7.0 greenlet-3.2.3 httpx-sse-0.4.1 jsonpatch-1.33 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.70 langchain-openai-0.3.28 langchain-text-splitters-0.3.8 langsmith-0.4.8 marshmallow-3.26.1 multidict-6.6.3 mypy-extensions-1.1.0 propcache-0.3.2 pydantic-settings-2.10.1 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 typing-inspect-0.9.0 yarl-1.20.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install openai\n",
    "#!pip install langchain-core==0.2.17\n",
    "#!pip install langchain-openai==0.1.17\n",
    "#!pip install pydantic==1.10.8\n",
    "#!pip install pydantic_core==2.18.2\n",
    "!pip install -U openai langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020041bc-3149-4195-b28f-4d38c65cbc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-search-results in /home/ubuntu/.local/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from google-search-results) (2.32.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->google-search-results) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->google-search-results) (2025.7.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->google-search-results) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->google-search-results) (3.4.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikipedia in /home/ubuntu/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ubuntu/.local/lib/python3.10/site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (4.14.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /home/ubuntu/.local/lib/python3.10/site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in /home/ubuntu/.local/lib/python3.10/site-packages (5.0.0)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (1.15.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (4.53.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (4.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (0.33.4)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sentence_transformers) (2.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.5)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=1.11.0->sentence_transformers) (59.6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.7.14)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results\n",
    "!pip install wikipedia\n",
    "!pip install faiss-cpu \n",
    "!pip install sentence_transformers \n",
    "!pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a332d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field # 예전 방식\n",
    "\n",
    "# from pydantic import BaseModel # 새로운 방식\n",
    "\n",
    "from pydantic.v1 import BaseModel # pydantic.v1 호환이 필요한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd2a51b-6740-4a14-8ad9-fb59e3ec001c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1990년대 재즈의 멋진 곡들을 몇 곡 추천해드릴게요:\\n\\n1. **\"Tutu\" - Miles Davis (1986)**  \\n   비록 1986년에 발표된 곡이지만, 1990년대에도 많은 사랑을 받은 곡입니다. Miles Davis의 전자 재즈 스타일을 잘 보여주는 곡입니다.\\n\\n2. **\"Hymn of the Seventh Galaxy\" - Chick Corea Elektric Band (1990)**  \\n   Chick Corea의 전자 재즈 그룹에서 나온 이 곡은 복잡한 리듬과 멜로디가 인상적입니다.\\n\\n3. **\"Come Away With Me\" - Norah Jones (2002)**  \\n   이 곡은 2000년대 초반의 곡이지만, 노라 조너스는 1990년대 후반에 음악 경력을 시작했으며, 그녀의 스타일은 90년대 재즈 영향을 받았습니다.\\n\\n4. **\"The Way Up\" - Pat Metheny Group (2005)**  \\n   이 앨범은 1990년대 재즈 퓨전의 영향을 받으며, Pat Metheny의 독창적인 사운드를 들려줍니다.\\n\\n5. **\"Blackstar\" - David Bowie (2016)**  \\n   이 곡은 90년대 재즈의 영향이 느껴지는 현대 재즈와 퓨전의 하나의 예로 볼 수 있습니다.\\n\\n이 곡들을 통해 1990년대 재즈의 다양성과 매력을 느껴보실 수 있기를 바랍니다!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 음악 추천을 해주는 전문 AI입니다.\")\n",
    "msg = HumanMessage(content='1990년대 재즈 5곡만 추천해줘.')\n",
    "\n",
    "aimsg = chat([sys, msg])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e5f17c-6a25-4489-919f-b1e477368296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105303/1939471710.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'베트남에는 여러 매력적인 여행지가 있습니다. 아래 몇 가지 추천지를 소개할게요:\\n\\n1. **하롱 베이**: 유네스코 세계유산으로 유명하며, 아름다운 섬과 카르스트 지형을 감상할 수 있습니다. 1박 2일로 크루즈를 즐기기 좋은 곳입니다.\\n\\n2. **호이안**: 역사적인 도시로, 전통적인 건축물과 맛있는 길거리 음식이 유명합니다. 자전거를 타고 주변 마을을 탐험하는 것도 추천합니다.\\n\\n3. **다낭**: 해변과 도시의 매력을 동시에 느낄 수 있는 곳으로, 마블 마운틴과 바나힐스, 해변에서의 자유로운 시간을 즐길 수 있습니다.\\n\\n4. **하노이**: 베트남의 수도로, 다양한 문화 유산과 맛있는 음식, 활기찬 시장을 경험할 수 있습니다. 짧은 여행에 적합합니다.\\n\\n각 장소마다 특색이 있으니, 취향에 맞는 곳을 선택해서 다녀오시면 좋을 것 같습니다!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예전에 사용한대로 AI, Human, System message를 배열로 전달\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "sys = SystemMessage(content=\"당신은 여행을 추천해주는 전문 AI입니다.\")\n",
    "msg1 = HumanMessage(content='베트남 사파에 1박 2일 여행 가면 충분할까?')\n",
    "msg2 = AIMessage(content='사파는 1박 2일로 가기에는 너무 멉니다. 일정을 조정하거나 다른 여행지를 선택하시면 어떨까요?')\n",
    "msg3 = HumanMessage(content='음 ... 그러면 어디가 좋을까?')\n",
    "\n",
    "aimsg = chat([sys, msg1, msg2, msg3])\n",
    "aimsg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5f966f-2923-4af7-9e96-bab696d6c956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI 여행 추천 서비스 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PromptTemplate를 사용하여 각 LLM에 맞는 prompt를 지정할 수 있음\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"상품\"],\n",
    "    template=\"{상품} 만드는 회사 이름 추천해줘. 기억에 남는 한글 이름으로\",\n",
    ")\n",
    "\n",
    "prompt.format(상품=\"AI 여행 추천 서비스\") # 변수가 여러 개 일 때, 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ae1232-2911-463a-838f-47eb2cbf5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105303/2518977364.py:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat, prompt=prompt)\n",
      "/tmp/ipykernel_105303/2518977364.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(상품=\"학원 추천 서비스\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'물론입니다! 기억에 남는 한글 이름 몇 가지를 추천해 드릴게요:\\n\\n1. **학원나무** - 학원이 자라는 나무처럼 학생들을 키워낸다는 의미.\\n2. **배움길** - 학습의 길을 함께 안내해 준다는 느낌.\\n3. **스승의소리** - 스승의 가르침을 듣고 배운다는 의미로, 신뢰성을 강조.\\n4. **지혜터** - 지혜를 나누고 배우는 공간이라는 느낌.\\n5. **콘텐츠하우스** - 다양한 학원 정보를 제공하는 집이라는 의미.\\n6. **아카데미플래너** - 체계적인 학원 추천을 계획하는 서비스.\\n7. **이노베이션학원** - 혁신적인 학원 추천을 강조.\\n8. **학습별자리** - 학습을 통해 빛나는 별처럼 성장하자는 의미.\\n9. **하이러닝** - 높은 학습을 지향하는 서비스.\\n10. **꿈터** - 학생들의 꿈을 키우는 공간이라는 느낌.\\n\\n이 중에서 마음에 드는 이름이 있으면 좋겠네요!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "chain = LLMChain(llm=chat, prompt=prompt)\n",
    "\n",
    "chain.run(상품=\"학원 추천 서비스\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cede08e-1a08-40de-b79a-edee95d7950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 프로그래밍을 사랑해요.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 Prompt 별 template 지정도 가능\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)  \n",
    "\n",
    "template=\"You are a helpful assisstant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "chatchain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chatchain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")\n",
    "#chatchain.run(input_language=\"Korean\", output_language=\"English\", text=\"나는 프로그래밍을 사랑해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e138db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.11.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (399 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.8/399.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numexpr) (2.2.6)\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96ce4ed-c8cd-453a-941f-67f937037653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m아마존의 창업자를 찾고 그의 나이와 배우자의 나이를 알아야 한다. 먼저 아마존의 창업자를 검색해 보겠다.  \n",
      "Action: wikipedia  \n",
      "Action Input: 아마존 창립자  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m아마존의 창립자에 대한 정보를 찾기 위해 다른 검색어를 사용해 보겠다.  \n",
      "Action: wikipedia  \n",
      "Action Input: Jeff Bezos  \u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Jeff Bezos\n",
      "Summary: Jeffrey Preston Bezos ( BAY-zohss; né Jorgensen; born January 12, 1964) is an American businessman best known as the founder, executive chairman, and former president and CEO of Amazon, the world's largest e-commerce and cloud computing company. According to Forbes, as of May 2025, Bezos's estimated net worth exceeded $220 billion, making him the third richest person in the world. He was the wealthiest person from 2017 to 2021, according to Forbes and the Bloomberg Billionaires Index.\n",
      "Bezos was born in Albuquerque and raised in Houston and Miami. He graduated from Princeton University in 1986 with degrees in electrical engineering and computer science. He worked on Wall Street in a variety of related fields from 1986 to early 1994. Bezos founded Amazon in mid-1994 on a road trip from New York City to Seattle. The company began as an online bookstore and has since expanded to a variety of other e-commerce products and services, including video and audio streaming, cloud computing, and artificial intelligence. It is the world's largest online sales company, the largest Internet company by revenue, and the largest provider of virtual assistants and cloud infrastructure services through its Amazon Web Services branch.\n",
      "Bezos founded the aerospace manufacturer and sub-orbital spaceflight services company Blue Origin in 2000. Blue Origin's New Shepard vehicle reached space in 2015 and afterwards successfully landed back on Earth; he flew into space on Blue Origin NS-16 in 2021. He purchased the major American newspaper The Washington Post in 2013 for $250 million and manages many other investments through his venture capital firm, Bezos Expeditions. In September 2021, Bezos co-founded Altos Labs with Mail.ru founder Yuri Milner.\n",
      "The first centibillionaire on the Forbes Real Time Billionaires Index and the second ever to have achieved the feat since Bill Gates in 1999, Bezos was named the \"richest man in modern history\" after his net worth increased to $150 billion in July 2018. In August 2020, according to Forbes, he had a net worth exceeding $200 billion. On July 5, 2021, Bezos stepped down as the CEO and president of Amazon and took over the role of executive chairman. Amazon Web Services CEO Andy Jassy succeeded Bezos as the CEO and president of Amazon.\n",
      "\n",
      "Page: Wedding of Jeff Bezos and Lauren Sánchez\n",
      "Summary: The wedding of American businessman Jeff Bezos and media personality Lauren Sánchez took place in Venice, Italy, between June 26–28, 2025. The multi-day event, held at various historic sites across the city, drew significant media attention due to its high-profile guest list, lavish celebrations, and extensive security measures. It also sparked local protests and criticism from Venetian activists over the city's commercialization and use as a venue for private luxury events.\n",
      "\n",
      "Page: Family of Jeff Bezos\n",
      "Summary: The family of Jeff Bezos, who is an American business magnate, is active in business and philanthropy. Bezos and his former wife MacKenzie Scott co-founded Amazon.\n",
      "Jeff's mother and stepfather are the co-founders of the Bezos Family Foundation.\n",
      "The family members have Danish-American, Mexican American and Cuban heritage.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mJeff Bezos는 아마존의 창립자이며, 그의 생년월일은 1964년 1월 12일이다. 따라서 그의 나이는 2023년 기준으로 59세이다. 그의 배우자는 Lauren Sánchez이며, 그들의 결혼식은 2025년에 열릴 예정이므로 그녀의 나이에 대한 정보는 현재로서는 확인할 수 없다. 그러나 Lauren Sánchez는 1969년 12월 19일에 태어났으므로, 그녀의 나이는 2023년 기준으로 53세이다. 이제 두 사람의 나이를 곱해 보겠다.  \n",
      "Action: Calculator  \n",
      "Action Input: 59 * 53  \u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3127\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: 3127\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3127'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatOpenAI를 LLM으로 사용하는 agent\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0) \n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=chat)\n",
    "\n",
    "agent = initialize_agent(tools, llm=chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) \n",
    "\n",
    "agent.run(\"아마존의 창업자는 누구인가? 그의 나이와 배우자의 나이를 곱하면 얼마인가?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84a01db-ce84-4ddf-be0d-9976ba3580ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/home/ubuntu/.local/lib/python3.10/site-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)),\n",
       " Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f5e5f2b65f0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f5e5f32b910>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f5e5f2b65f0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f5e5f32b910>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy=''), output_parser=StrOutputParser(), llm_kwargs={}))>)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47498a86-a6b0-4e43-bbe3-e84d6e338b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\n",
      "Useful for when you need to answer questions about math.\n"
     ]
    }
   ],
   "source": [
    "print(agent.tools[0].description)\n",
    "print(agent.tools[1].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4c1c5cf-26f1-448f-8687-784ca932ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105303/2149845611.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=chat, verbose=True)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLM은 \"대형 언어 모델\"(Large Language Model)의 약자로, 대규모 데이터셋을 학습하여 자연어를 이해하고 생성할 수 있는 인공지능 모델을 의미해. 이러한 모델은 수백만에서 수십억 개의 단어로 훈련되어, 문서 작성, 질문에 대한 답변, 번역 등 다양한 언어 관련 작업을 수행할 수 있어. 예를 들어, OpenAI의 GPT-3나 그 후속 모델들이 LLM의 좋은 예야. LLM은 문맥을 이해하고, 의미를 파악하며, 언어의 규칙을 적용하여 자연스러운 문장을 생성할 수 있는 능력을 가지고 있어. 무엇을 더 궁금해?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import ConversationChain\n",
    "\n",
    "chat = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.9)\n",
    "conversation = ConversationChain(llm=chat, verbose=True)\n",
    "conversation.predict(input=\"인공지능에서 LLM이 뭐야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd1f46e-daa1-4098-8d3e-4c36f4ec0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 인공지능에서 LLM이 뭐야?\n",
      "AI: LLM은 \"대형 언어 모델\"(Large Language Model)의 약자로, 대규모 데이터셋을 학습하여 자연어를 이해하고 생성할 수 있는 인공지능 모델을 의미해. 이러한 모델은 수백만에서 수십억 개의 단어로 훈련되어, 문서 작성, 질문에 대한 답변, 번역 등 다양한 언어 관련 작업을 수행할 수 있어. 예를 들어, OpenAI의 GPT-3나 그 후속 모델들이 LLM의 좋은 예야. LLM은 문맥을 이해하고, 의미를 파악하며, 언어의 규칙을 적용하여 자연스러운 문장을 생성할 수 있는 능력을 가지고 있어. 무엇을 더 궁금해?\n",
      "Human: PLM과의 차이를 설명해줘.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PLM은 \"프리트레인드 언어 모델\"(Pre-trained Language Model)의 약자로, LLM과 밀접하게 관련된 개념이야. 두 용어 모두 언어 모델을 다루지만, 주된 차이는 사용하는 방법과 모델의 훈련 과정에 있어.\\n\\n1. **프리트레이닝 vs. 파인튜닝**: PLM은 대량의 텍스트 데이터를 사용해 미리 훈련된 모델을 의미해. 그런 다음 특정 작업에 대해 추가적으로 훈련(파인튜닝)될 수 있어. 예를 들어, 특정 도메인에 대한 질문 응답 시스템을 구축하기 위해 PLM을 기업의 데이터로 다시 훈련시킬 수 있어.\\n\\n2. **용어의 범위**: LLM은 더 큰 스케일의 언어 모델을 강조하는 경향이 있어. 즉, LLM은 대체로 거대한 네트워크 아키텍처와 대량의 매개변수를 가진 모델을 뜻하고, PLM은 더 일반적인 범주로 사용될 수 있어, 큰 모델뿐만 아니라 작은 모델도 포함될 수 있지.\\n\\n3. **응용 분야**: PLM은 특정 태스크에 맞춰 조정될 수 있는 유연성을 강조하는 반면, LLM은 그 규모와 성능을 강조해. 그래서 LLM은 자연어 처리에서 최첨단 성능을 자랑하는 모델들이며, PLM은 특정 작업에 대한 적응 가능성을 중시해.\\n\\n즉, LLM은 PLM의 한 형태라고 볼 수 있어. 더 궁금한 점이 있다면 언제든지 물어봐!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"PLM과의 차이를 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b7e4767-87f5-4731-be17-7d74b9a309f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='인공지능에서 LLM이 뭐야?', additional_kwargs={}, response_metadata={}), AIMessage(content='LLM은 \"대형 언어 모델\"(Large Language Model)의 약자로, 대규모 데이터셋을 학습하여 자연어를 이해하고 생성할 수 있는 인공지능 모델을 의미해. 이러한 모델은 수백만에서 수십억 개의 단어로 훈련되어, 문서 작성, 질문에 대한 답변, 번역 등 다양한 언어 관련 작업을 수행할 수 있어. 예를 들어, OpenAI의 GPT-3나 그 후속 모델들이 LLM의 좋은 예야. LLM은 문맥을 이해하고, 의미를 파악하며, 언어의 규칙을 적용하여 자연스러운 문장을 생성할 수 있는 능력을 가지고 있어. 무엇을 더 궁금해?', additional_kwargs={}, response_metadata={}), HumanMessage(content='PLM과의 차이를 설명해줘.', additional_kwargs={}, response_metadata={}), AIMessage(content='PLM은 \"프리트레인드 언어 모델\"(Pre-trained Language Model)의 약자로, LLM과 밀접하게 관련된 개념이야. 두 용어 모두 언어 모델을 다루지만, 주된 차이는 사용하는 방법과 모델의 훈련 과정에 있어.\\n\\n1. **프리트레이닝 vs. 파인튜닝**: PLM은 대량의 텍스트 데이터를 사용해 미리 훈련된 모델을 의미해. 그런 다음 특정 작업에 대해 추가적으로 훈련(파인튜닝)될 수 있어. 예를 들어, 특정 도메인에 대한 질문 응답 시스템을 구축하기 위해 PLM을 기업의 데이터로 다시 훈련시킬 수 있어.\\n\\n2. **용어의 범위**: LLM은 더 큰 스케일의 언어 모델을 강조하는 경향이 있어. 즉, LLM은 대체로 거대한 네트워크 아키텍처와 대량의 매개변수를 가진 모델을 뜻하고, PLM은 더 일반적인 범주로 사용될 수 있어, 큰 모델뿐만 아니라 작은 모델도 포함될 수 있지.\\n\\n3. **응용 분야**: PLM은 특정 태스크에 맞춰 조정될 수 있는 유연성을 강조하는 반면, LLM은 그 규모와 성능을 강조해. 그래서 LLM은 자연어 처리에서 최첨단 성능을 자랑하는 모델들이며, PLM은 특정 작업에 대한 적응 가능성을 중시해.\\n\\n즉, LLM은 PLM의 한 형태라고 볼 수 있어. 더 궁금한 점이 있다면 언제든지 물어봐!', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67d6347c-2c99-4d28-b9e6-b7585d72a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Created a chunk of size 2293, which is longer than the specified 1000\n",
      "Created a chunk of size 2843, which is longer than the specified 1000\n",
      "Created a chunk of size 6664, which is longer than the specified 1000\n",
      "Created a chunk of size 2194, which is longer than the specified 1000\n",
      "Created a chunk of size 1471, which is longer than the specified 1000\n",
      "Created a chunk of size 2774, which is longer than the specified 1000\n",
      "Created a chunk of size 1873, which is longer than the specified 1000\n",
      "Created a chunk of size 3432, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 2746, which is longer than the specified 1000\n",
      "Created a chunk of size 3230, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "# https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_path=\"https://ko.wikipedia.org/wiki/IU\")\n",
    "documents = loader.load()\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "692cd1a0-48c1-48b6-977c-a2a48ea96fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='아이유 - 위키백과, 우리 모두의 백과사전\\n\\n\\n본문으로 이동\\n\\n주 메뉴\\n\\n주 메뉴\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t둘러보기\\n\\t\\n\\n\\n대문최근 바뀜요즘 화제임의의 문서로\\n\\n\\t\\t사용자 모임\\n\\t\\n\\n\\n사랑방사용자 모임관리 요청\\n\\n\\t\\t편집 안내\\n\\t\\n\\n\\n소개도움말정책과 지침질문방\\n\\n검색\\n\\n검색\\n\\n\\n보이기\\n\\n\\n기부\\n\\n계정 만들기\\n\\n로그인\\n\\n\\n개인 도구\\n\\n기부 계정 만들기 로그인\\n\\n\\n목차\\n사이드바로 이동\\n숨기기\\n\\n\\n처음 위치\\n\\n1\\n학력\\n\\n\\n2\\n활동\\n\\n\\n활동 하위섹션 토글하기\\n\\n2.1\\n2008-2010\\n\\n\\n2.2\\n2011-2013\\n\\n\\n2.3\\n2014-2016\\n\\n\\n2.4\\n2017-2018\\n\\n\\n2.5\\n2019-2020\\n\\n\\n2.6\\n2021-2022\\n\\n\\n2.7\\n2023-2024\\n\\n\\n2.8\\n2025\\n\\n\\n3\\n음반 목록\\n\\n\\n4\\n음반 외 활동 목록\\n\\n\\n5\\n수상 및 후보 목록\\n\\n\\n6\\n사건\\n\\n\\n7\\n같이 보기\\n\\n\\n8\\n각주\\n\\n\\n각주 하위섹션 토글하기\\n\\n8.1\\n내용주\\n\\n\\n8.2\\n참조주\\n\\n\\n9\\n외부 링크\\n\\n\\n목차 토글\\n\\n아이유\\n\\n63개 언어'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='العربيةمصرىتۆرکجهBasa BaliBikol CentralБългарскиCatalàCebuanoکوردیČeštinaDanskDeutschKadazandusunΕλληνικάEnglishEsperantoEspañolEestiEuskaraفارسیSuomiFrançaisहिन्दीKreyòl ayisyenMagyarՀայերենInterlinguaBahasa IndonesiaItaliano日本語JawaҚазақшаکٲشُرLatinaMāoriമലയാളംМонголBahasa Melayuမြန်မာဘာသာNederlandsNorsk bokmålPolskiPortuguêsRomânăРусскийSimple EnglishSlovenčinaSoomaaligaСрпски / srpskiSesothoSundaSvenskaதமிழ்ไทยTagalogSetswanaTürkçeУкраїнськаOʻzbekcha / ўзбекчаTiếng Việt中文閩南語 / Bân-lâm-gí粵語\\n\\n링크 편집\\n\\n문서토론\\n\\n한국어\\n\\n읽기원본 보기역사 보기\\n\\n도구\\n\\n도구\\n사이드바로 이동\\n숨기기\\n\\n\\t\\t동작\\n\\t\\n\\n\\n읽기원본 보기역사 보기\\n\\n\\t\\t일반\\n\\t\\n\\n\\n여기를 가리키는 문서가리키는 글의 최근 바뀜파일 올리기고유 링크문서 정보이 문서 인용하기축약된 URL 얻기QR코드 다운로드\\n\\n\\t\\t인쇄/내보내기\\n\\t\\n\\n\\n책 만들기PDF로 다운로드인쇄용 판\\n\\n\\t\\t다른 프로젝트\\n\\t\\n\\n\\n위키미디어 공용위키데이터 항목\\n\\n보이기\\n사이드바로 이동\\n숨기기\\n\\n\\n위키백과, 우리 모두의 백과사전.'),\n",
       " Document(metadata={'source': 'https://ko.wikipedia.org/wiki/IU', 'title': '아이유 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}, page_content='(IU에서 넘어옴)\\n  다른 뜻에 대해서는 아이유 (동음이의) 문서를 참고하십시오. IU는 여기로 연결됩니다. 다른 뜻에 대해서는 IU (동음이의) 문서를 참고하십시오.\\n아이유2022년 11월 25일 청룡영화상에서의 아이유기본 정보본명이지은출생1993년 5월 16일(1993-05-16)(32세)대한민국 서울특별시 성동구 송정동성별여성직업가수, 배우장르댄스, 발라드, 어쿠스틱, R&B활동 시기2008년~학력동덕여자고등학교 (졸업)레이블카카오엔터테인먼트, EMI 뮤직 재팬소속사EDAM 엔터테인먼트서명웹사이트edam-ent.com유튜브채널이지금 [IU Official]분야일상, 음악개설일2017년 2월 13일구독자 수978만\\xa0명(2024년 10월 6일 기준)총 조회수3,150,968,674회(2024년 10월 6일 기준)vte\\n\\n아이유(IU, 본명:\\xa0이지은, 李知恩[1], 1993년 5월 16일~)는 대한민국의 싱어송라이터, 작곡가, 배우이다. 2007년 로엔 엔터테인먼트(현 카카오 엔터테인먼트) 연습생으로 전속 계약을 맺고 15세의 나이에 2008년 첫 EP인 로스트 앤 파운드(Lost and Found)를 통해 가수로 데뷔했다.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36be6f88-571b-4010-bbdb-af25b75a43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105303/4030962451.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "#embeddings = HuggingFaceEmbeddings() # sentence-transformers/all-mpnet-base-v2\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    ).from_loaders([loader])\n",
    "\n",
    "# 파일로 저장\n",
    "index.vectorstore.save_local(\"faiss-nj-IU\")\n",
    "# db = FAISS.load_local(\"faiss-nj-IU\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c656357-f0e7-439b-b6c6-14927f4e7c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유의 데뷔곡은 2008년에 발매된 EP 앨범 \\'로스트 앤 파운드\\'에 수록된 곡들 중 하나입니다. 첫 번째 노래인 \"미리 메리 크리스마스\"가 데뷔곡으로 알려져 있습니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 데뷔곡은?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aebc9b7c-64b9-4cd5-be7d-f2e525e6a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유는 1993년 5월 16일에 태어났으므로, 2024년 기준으로 32세입니다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는?\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1917167b-5b64-4e99-9b8e-17a6047d5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아이유는 1993년 5월 16일에 태어났으므로, 2025년 7월 22일 기준으로 그녀의 나이는 32세입니다.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유의 나이는? (오늘은 2025년 7월 22일)\", llm=chat, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "403356b0-5ef9-45a1-b0d7-743866575946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그에 대한 정보는 알려져 있지 않습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(\"아이유는 지금까지 총 콘서트를 몇회했는가?\", llm=chat, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
