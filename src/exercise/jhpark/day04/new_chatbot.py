import os
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings

# ğŸŒ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (OPENAI API í‚¤)
openai_api_key = os.getenv("OPENAI_API_KEY")
Settings.llm = OpenAI(model="gpt-4o", temperature=0)

# ğŸ“„ ë¬¸ì„œ ë¡œë”© ë° ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
@st.cache_resource
def load_index():
    documents = SimpleDirectoryReader("data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    return index

# ğŸ¤– RAG ê¸°ë°˜ ì±—ì—”ì§„ ì´ˆê¸°í™”
@st.cache_resource
def init_rag_chat_engine():
    index = load_index()
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        memory=ChatMemoryBuffer(token_limit=3000)
    )
    return chat_engine

# ğŸ’¬ Streamlit ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
def main():
    st.set_page_config(page_title="RAG ê¸°ë°˜ ì±—ë´‡", page_icon="ğŸ§ ")
    st.title("ğŸ§  RAG ê¸°ë°˜ GPT ì±—ë´‡")
    st.write("ìº í•‘, ì£¼ì‹, ì‹ë‹¹ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")

    chat_engine = init_rag_chat_engine()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    if user_input:
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = chat_engine.chat(user_input)
            answer = response.response

        st.session_state.chat_history.append({"role": "assistant", "content": answer})
        with st.chat_message("assistant"):
            st.markdown(answer)

if __name__ == "__main__":
    main()
