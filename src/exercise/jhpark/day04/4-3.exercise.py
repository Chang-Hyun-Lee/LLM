import os
import streamlit as st
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    ServiceContext,  
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="streamlit and llama", layout="centered")
st.title("ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ Q&A ì–´í”Œë¦¬ì¼€ì´ì…˜")

# ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥ í´ë”
UPLOAD_DIR = "uploaded_docs"
os.makedirs(UPLOAD_DIR, exist_ok=True)

uploaded_files = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, TXT ë“±)", type=["pdf", "txt"], accept_multiple_files=True)

# íŒŒì¼ ì €ì¥
if uploaded_files:
    for uploaded_file in uploaded_files:
        file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
    st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")

    # ë¬¸ì„œ ì½ê¸° ë° ì¸ë±ì‹±
    with st.spinner("ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ëŠ” ì¤‘..."):
        documents = SimpleDirectoryReader(UPLOAD_DIR).load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model="gpt-4o-mini"))
        index = VectorStoreIndex.from_documents(documents, service_context=service_context)
        query_engine = index.as_query_engine()
    st.success("ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")

    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    if question:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            answer = query_engine.query(question)
        st.subheader("ğŸ§  ë‹µë³€")
        st.write(answer.response)
