import openai
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext
from llama_index.llms.openai import OpenAI
import os
import tempfile
from llama_index.core import Settings

openai.api_key = os.getenv("OPENAI_API_KEY")

st.set_page_config(page_title="íŒŒì¼ ì„¤ëª…ê¸°", layout="centered")

st.title("ğŸ“„ íŒŒì¼ ì „ì²´ ì„¤ëª…ê¸° (LlamaIndex + Streamlit)")

uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, TXT, DOCX)", type=["pdf", "txt", "docx"])

if uploaded_file:
    with tempfile.TemporaryDirectory() as temp_dir:
        file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. ì²˜ë¦¬ ì¤‘...")

        # ë¬¸ì„œ ì½ê¸°
        reader = SimpleDirectoryReader(temp_dir)
        docs = reader.load_data()

        @st.cache_resource
         def get_index():
             Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")

             documents = []
             try:
               documents = SimpleDirectoryReader("./data").load_data()
             except Exception as e:
               print(f"An error occurred: {e}")        
        
             index = VectorStoreIndex.from_documents(
               documents,
           )
           return index
        
        # ìš”ì•½ìš© ì¿¼ë¦¬ì—”ì§„
        query_engine = index.as_query_engine()

        # íŒŒì¼ ë‚´ìš© ìš”ì•½
        response = query_engine.query("ì´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.")

        st.subheader("ğŸ“Œ ë¬¸ì„œ ìš”ì•½")
        st.write(response.response)
