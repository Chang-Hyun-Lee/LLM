import openai
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
import os
import tempfile
from llama_index.core import Settings

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# Streamlit ì„¤ì •
st.set_page_config(page_title="íŒŒì¼ ì„¤ëª…ê¸°", layout="centered")
st.title("ğŸ“„ íŒŒì¼ ì „ì²´ ì„¤ëª…ê¸° (LlamaIndex + Streamlit)")

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, TXT, DOCX)", type=["pdf", "txt", "docx"])

# ì´ì „ ê²°ê³¼ ì´ˆê¸°í™”
if "summary" not in st.session_state:
    st.session_state.summary = None

if uploaded_file:
    with tempfile.TemporaryDirectory() as temp_dir:
        file_path = os.path.join(temp_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. ì²˜ë¦¬ ì¤‘...")

        # ë¬¸ì„œ ì½ê¸°
        reader = SimpleDirectoryReader(temp_dir)
        docs = reader.load_data()

        # LLM ì„¤ì • (ì „ì—­)
        Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")

        @st.cache_resource
        def get_index(_documents):
            return VectorStoreIndex.from_documents(_documents)

        # ì¸ë±ìŠ¤ ìƒì„±
        index = get_index(docs)
        
        # ì¿¼ë¦¬ ì—”ì§„ ìƒì„± ë° ìš”ì•½ ì‹¤í–‰
        query_engine = index.as_query_engine()
        response = query_engine.query("ì´ ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.")
        st.session_state.summary = response.response

        # ê²°ê³¼ ì¶œë ¥
        st.subheader("ğŸ“Œ ë¬¸ì„œ ìš”ì•½")
        st.write(response.response)
