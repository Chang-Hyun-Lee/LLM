import os
import streamlit as st
import openai
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer

# OpenAI API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# íŒŒì¼ ì €ì¥ í•¨ìˆ˜
def save_uploaded_file(directory, file):
    if not os.path.exists(directory):
        os.makedirs(directory)
    
    with open(os.path.join(directory, file.name), 'wb') as f:
        f.write(file.getbuffer())

# ì¸ë±ìŠ¤ ìºì‹±
@st.cache_resource
def get_index():
    Settings.llm = OpenAI(temperature=0, model="gpt-4o-mini")

    documents = []
    try:
        documents = SimpleDirectoryReader("../data").load_data()
    except Exception as e:
        print(f"An error occurred: {e}")        
        
    index = VectorStoreIndex.from_documents(documents)
    return index

# ë©”ëª¨ë¦¬ ìºì‹±
@st.cache_resource
def get_memory():
    return ChatMemoryBuffer.from_defaults()

# ì±—ë´‡ê³¼ ëŒ€í™” í•¨ìˆ˜
def chat_with_bot(user_input, index):
    chat_engine = index.as_chat_engine(
        chat_mode="context",
        memory=get_memory(),
        system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.",
    )
    response = chat_engine.stream_chat(user_input)
    return response

# Streamlit ì•± ë©”ì¸ í•¨ìˆ˜
def main():
    st.set_page_config(page_title="RAG Chatbot", layout="wide")
    st.title("ğŸ§  RAG Chatbot with Streamlit, LlamaIndex and OpenAI")

    # ì´ˆê¸°í™”
    if "file_uploader_key" not in st.session_state:
        st.session_state["file_uploader_key"] = 0

    # ì‚¬ì´ë“œë°” UI
    with st.sidebar:
        st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
        data_dir = "../data"
        upload_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=['pdf'], key=st.session_state["file_uploader_key"])

        if upload_file is not None:
            save_uploaded_file(data_dir, upload_file)
            get_index.clear()
            st.session_state["file_uploader_key"] += 1
            st.rerun()

        # ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ë° ì‚­ì œ UI
        st.subheader("ğŸ“‚ ì—…ë¡œë“œëœ íŒŒì¼")
        if os.path.exists(data_dir):
            files = os.listdir(data_dir)
            if files:
                for file in files:
                    file_path = os.path.join(data_dir, file)
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.markdown(f"ğŸ“„ {file}")
                    with col2:
                        if st.button("âŒ", key=f"delete_{file}"):
                            os.remove(file_path)
                            get_index.clear()
                            st.rerun()
            else:
                st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ì¸ë±ìŠ¤ ìƒì„±
    index = get_index()

    # ë©”ì‹œì§€ ì„¸ì…˜ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    user_input = st.chat_input("ëŒ€í™”ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})

        response = chat_with_bot(user_input, index)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            for chunk in response.response_gen:
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})

if __name__ == "__main__":
    main()
