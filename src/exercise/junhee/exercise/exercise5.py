#1. llama_indexì™€ Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ëœ íŒŒì¼ì— ëŒ€í•´ì„œ ë‹µë³€í•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì‘ì„±í•˜ì‹œì˜¤.
#2. llama_indexì™€ Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ëœ íŒŒì¼ì— ëŒ€í•´ì„œ ë‹µë³€í•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ë©€í‹°í„´ìœ¼ë¡œ ë°”ê¾¸ì‹œì˜¤.
#3. llama_indexì™€ Streamlitì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì—…ë¡œë“œí•œ íŒŒì¼ ì „ì²´ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ì‘ì„±í•˜ì‹œì˜¤.
#4. 3ì˜ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ íŒŒì¼ì„ ë¦¬ìŠ¤íŠ¸ì—…í•˜ê³  ì‚­ì œí•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ì‹œì˜¤.


import streamlit as st
import time
import os
import openai
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage

openai.api_key = os.getenv("OPENAI_API_KEY")
Settings.llm = OpenAI(temperature = 0.2, model = "gpt-4o-mini")


# UI êµ¬í˜„
# ----------------------------------------------------------------------------
with st.sidebar:
    st.subheader("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type = ["pdf", "txt", "md"], accept_multiple_files = True)

st.title("ğŸ“„ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”!")
# ----------------------------------------------------------------------------


# íŒŒì¼ í´ë” ìœ„ì¹˜ ì§€ì •
PERSIST_DIR = "./storage"
UPLOAD_DIR = "uploaded_files"

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(UPLOAD_DIR, exist_ok = True)

# ğŸ“ íŒŒì¼ ì—…ë¡œë“œ
if uploaded_files:
    for file in uploaded_files:
        with open(os.path.join(UPLOAD_DIR, file.name), "wb") as f:
            f.write(file.getbuffer())
    st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")
    


# ì±„íŒ… ì´ˆê¸° í™”ë©´ ì¶œë ¥ 
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•! ë¬´ì—‡ì´ ê¶ê¸ˆí•´?"}
    ]



if "query_engine" not in st.session_state:
    st.session_state.query_engine = None

for message in st.session_state.messages[:]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):

    if not os.path.exists(PERSIST_DIR):
        if uploaded_files:
            documents = SimpleDirectoryReader(UPLOAD_DIR).load_data()
            index = VectorStoreIndex.from_documents(documents)
            index.storage_context.persist(persist_dir = PERSIST_DIR)
    else:
        storage_context = StorageContext.from_defaults(persist_dir = PERSIST_DIR)
        index = load_index_from_storage(storage_context)
    
    # ì¿¼ë¦¬ ì—”ì§„ ì„¤ì •
    st.session_state.query_engine = index.as_query_engine()

    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì¶”ê°€í•˜ê¸°
    st.session_state.messages.append({"role": "user", "content": prompt})
    # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = st.session_state.query_engine.query(prompt)
        st.markdown(response)
    # ë‹µë³€ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": response})




