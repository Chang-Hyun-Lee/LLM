import streamlit as st
import os
import pandas as pd
from pykrx import stock
import requests
from langchain.chat_models import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain import hub
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory
from llama_index.core import VectorStoreIndex, Document

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TAVILY_API_KEY"] = "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"

# ğŸ” ì£¼ê°€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° + ìºì‹±
@st.cache_data(show_spinner="ğŸ“Š ì£¼ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def load_all_ohlcv(start_date: str, end_date: str) -> pd.DataFrame:
    ticker_list = stock.get_market_ticker_list()
    all_data = []

    for ticker in ticker_list:
        try:
            df = stock.get_market_ohlcv_by_date(start_date, end_date, ticker)
            df = df.copy()
            df["í‹°ì»¤"] = ticker
            all_data.append(df)
        except Exception as e:
            print(f"{ticker} ì—ëŸ¬ ë°œìƒ: {e}")

    result_df = pd.concat(all_data)
    result_df.reset_index(inplace=True)
    return result_df

def search_naver_local(query, count):
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/local.json?query={encText}&display={count}"
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)
    response = urllib.request.urlopen(request)
    rescode = response.getcode()

    if rescode == 200:
        response_body = response.read()
        data = json.loads(response_body.decode('utf-8'))
        return data.get("items", [])
    else:
        return []


# ğŸ” LlamaIndex ì¸ë±ìŠ¤ ìƒì„± + ìºì‹±
@st.cache_resource(show_spinner="ğŸ” ì¸ë±ìŠ¤ë¥¼ ë§Œë“œëŠ” ì¤‘ì…ë‹ˆë‹¤...")
def create_index_from_df(df: pd.DataFrame) -> VectorStoreIndex:
    documents = []
    for ticker in df["í‹°ì»¤"].unique():
        sub_df = df[df["í‹°ì»¤"] == ticker]
        text = f"ì¢…ëª©ì½”ë“œ: {ticker}\n"
        for _, row in sub_df.iterrows():
            text += f"{row['ë‚ ì§œ'].date()} - ì‹œê°€: {row['ì‹œê°€']}, ê³ ê°€: {row['ê³ ê°€']}, ì €ê°€: {row['ì €ê°€']}, ì¢…ê°€: {row['ì¢…ê°€']}, ê±°ë˜ëŸ‰: {row['ê±°ë˜ëŸ‰']}\n"
        documents.append(Document(text=text))
    return VectorStoreIndex.from_documents(documents)

# ğŸ” ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬ ì •ì˜
class StreamHandler(BaseCallbackHandler):
    def __init__(self):
        self.tokens = ""
        self.container = st.empty()

    def on_llm_new_token(self, token, **kwargs):
        self.tokens += token
        self.container.markdown(self.tokens + "â–Œ")

# ğŸ“ˆ LangChain êµ¬ì„±
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
search_tool = TavilySearchResults(k=5)

# ë°ì´í„° ì¤€ë¹„
start_date = "2024-01-01"
end_date = "2025-07-20"
df = load_all_ohlcv(start_date, end_date)
index = create_index_from_df(df)
query_engine = index.as_query_engine()

# LlamaIndex íˆ´ ì •ì˜
llama_tool = Tool(
    name = "StockIndexQA",
    func = lambda q: query_engine.query(q).response,
    description="ì£¼ì‹ ì¸ë±ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ì˜ˆ: 'ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜'"
)

tools = [search_tool, llama_tool]
prompt = hub.pull("hwchase17/openai-functions-agent")

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)


# Streamlit ì„¤ì •
st.set_page_config(page_title="ğŸ“ˆ ì¢…ëª© ì£¼ê°€ ë¶„ì„ ì—ì´ì „íŠ¸", page_icon="ğŸ“Š")
st.title("ğŸ“ˆ ì£¼ê°€ ë¶„ì„ AI ì±—ë´‡")
st.caption("ì¢…ëª© ì´ë¦„ì´ë‚˜ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ë©´ AIê°€ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•´ë“œë¦½ë‹ˆë‹¤.")

# ì±„íŒ… ì´ë ¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì±„íŒ… ì¶œë ¥
for role, content in st.session_state.chat_history:
    with st.chat_message(role):
        st.markdown(content)
        

# ì…ë ¥ í•„ë“œ
user_input = st.chat_input("ì˜ˆ: ì‚¼ì„±ì „ì ì£¼ê°€ ì•Œë ¤ì¤˜")

# ë©”ì‹œì§€ ì²˜ë¦¬
if user_input:
    st.session_state.chat_history.append(("user", user_input))
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        handler = StreamHandler()
        callback_manager = CallbackManager([handler])
        streaming_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True, callback_manager=callback_manager)

        agent = create_openai_functions_agent(streaming_llm, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, memory = st.session_state.memory, tools=tools, verbose=True)

        for _ in agent_executor.stream({"input": user_input}):
            pass

        st.session_state.chat_history.append(("assistant", handler.tokens))
