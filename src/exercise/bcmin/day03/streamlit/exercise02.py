import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
import os

# ğŸ”‘ OpenAI API í‚¤ ì„¤ì •
openai_api_key = os.getenv("OPENAI_API_KEY")

# ğŸ“ Streamlit ì•± ì œëª©
st.title("ğŸ“„ PDF ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ ì–´í”Œë¦¬ì¼€ì´ì…˜")

# ğŸ“ ë²¡í„° DB ë¡œë”©
@st.cache_resource
def load_vectorstore():
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    db = FAISS.load_local("shower1", embeddings=embeddings, allow_dangerous_deserialization=True)
    return db

# ğŸ¤– ì±— ëª¨ë¸ + ê²€ìƒ‰ ì²´ì¸ ì„¤ì •
@st.cache_resource
def get_qa_chain():
    vectorstore = load_vectorstore()
    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(openai_api_key=openai_api_key, model="gpt-4o-mini")
    chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
    return chain

chain = get_qa_chain()

# ğŸ§‘ ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
query = st.text_input("ğŸ“Œ PDFì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

# ğŸ” ì§ˆë¬¸ ì²˜ë¦¬
if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        answer = chain.run(query)
    st.markdown(f"### ğŸ¤– ë‹µë³€\n{answer}")
