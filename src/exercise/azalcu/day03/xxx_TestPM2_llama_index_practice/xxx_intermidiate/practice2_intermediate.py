# intermediate/practice2_intermediate.py
import streamlit as st
import os
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.memory import ChatMemoryBuffer

st.set_page_config(page_title="ë©€í‹°í„´ ì±„íŒ…", page_icon="ğŸ’¬")
st.title("ğŸ’¬ ì¤‘ê¸‰: ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'chat_engine' not in st.session_state:
    st.session_state.chat_engine = None

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", type="password")
    docs_folder = st.text_input("ë¬¸ì„œ í´ë”", value="../docs")
    
    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory_limit = st.slider("ë©”ëª¨ë¦¬ í† í° ì œí•œ", 500, 3000, 1500)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
    system_message = st.text_area(
        "ì‹œìŠ¤í…œ ë©”ì‹œì§€",
        value="ë‹¹ì‹ ì€ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
        height=100
    )
    
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.chat_engine = None
        st.rerun()
    
    # ëŒ€í™” í†µê³„
    if st.session_state.messages:
        user_count = len([m for m in st.session_state.messages if m['role'] == 'user'])
        st.metric("ëŒ€í™” íšŸìˆ˜", user_count)

@st.cache_resource
def setup_chat_engine(docs_folder, _api_key, memory_limit, system_message):
    try:
        # LlamaIndex ì„¤ì •
        Settings.llm = OpenAI(api_key=_api_key)
        Settings.embed_model = OpenAIEmbedding(api_key=_api_key)
        
        # ë¬¸ì„œ ë¡œë“œ
        if not os.path.exists(docs_folder):
            return None, f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_folder}"
        
        documents = SimpleDirectoryReader(docs_folder).load_data()
        if not documents:
            return None, "ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¸ë±ìŠ¤ ìƒì„±
        index = VectorStoreIndex.from_documents(documents)
        
        # ë©”ëª¨ë¦¬ ì„¤ì •
        memory = ChatMemoryBuffer.from_defaults(token_limit=memory_limit)
        
        # ì±„íŒ… ì—”ì§„ ìƒì„±
        chat_engine = index.as_chat_engine(
            chat_mode="context",
            memory=memory,
            system_prompt=system_message
        )
        
        return chat_engine, f"âœ… {len(documents)}ê°œ ë¬¸ì„œë¡œ ì±„íŒ… ì¤€ë¹„ ì™„ë£Œ!"
        
    except Exception as e:
        return None, f"ì„¤ì • ì‹¤íŒ¨: {str(e)}"

if api_key:
    # ì±„íŒ… ì—”ì§„ ì„¤ì •
    if st.session_state.chat_engine is None:
        with st.spinner("ì±„íŒ… ì‹œìŠ¤í…œì„ ì¤€ë¹„í•˜ëŠ” ì¤‘..."):
            chat_engine, message = setup_chat_engine(docs_folder, api_key, memory_limit, system_message)
        
        if chat_engine:
            st.session_state.chat_engine = chat_engine
            st.success(message)
        else:
            st.error(message)
            st.stop()
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            try:
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    response = st.session_state.chat_engine.chat(prompt)
                    answer = str(response)
                
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ (ìˆë‹¤ë©´)
                if hasattr(response, 'source_nodes') and response.source_nodes:
                    with st.expander("ğŸ“„ ì°¸ì¡° ë¬¸ì„œ"):
                        for i, node in enumerate(response.source_nodes[:2]):  # ìµœëŒ€ 2ê°œë§Œ
                            st.caption(f"**ì†ŒìŠ¤ {i+1}:** {node.text[:150]}...")
                            
            except Exception as e:
                error_msg = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
    
else:
    st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
    st.markdown("""
    ### ğŸ’¡ ì‚¬ìš© ë°©ë²•:
    1. **API í‚¤ ì…ë ¥**: ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ ì„¤ì •
    2. **ë¬¸ì„œ ì¤€ë¹„**: `../docs` í´ë”ì— ë¬¸ì„œ íŒŒì¼ë“¤ ì €ì¥
    3. **ëŒ€í™” ì‹œì‘**: ì•„ë˜ ì…ë ¥ì°½ì—ì„œ ììœ ë¡­ê²Œ ì§ˆë¬¸
    4. **ì—°ì† ëŒ€í™”**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ë©° ë‹µë³€
    
    ### ğŸ”§ ê³ ê¸‰ ì„¤ì •:
    - **ë©”ëª¨ë¦¬ ì œí•œ**: ëŒ€í™” ê¸°ë¡ ìœ ì§€ ë²”ìœ„ ì¡°ì ˆ
    - **ì‹œìŠ¤í…œ ë©”ì‹œì§€**: AIì˜ ì‘ë‹µ ìŠ¤íƒ€ì¼ ì¡°ì •
    """)