# advanced/practice4_advanced.py
import streamlit as st
import os
import sys
import json
import pickle
import shutil
import zipfile
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from concurrent.futures import ThreadPoolExecutor
import schedule
import threading
import time

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, load_index_from_storage
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.memory import ChatMemoryBuffer
import logging

# Í≥µÏö© Î™®Îìà import
sys.path.append(str(Path(__file__).parent.parent / "shared"))

# ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
st.set_page_config(
    page_title="Í≥†Í∏â ÌååÏùº Í¥ÄÎ¶¨ & QA",
    page_icon="üóÇÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnterpriseFileManager:
    """ÏóîÌÑ∞ÌîÑÎùºÏù¥Ï¶àÍ∏â ÌååÏùº Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        self.base_dir = Path("../file_management_enterprise")
        self.base_dir.mkdir(exist_ok=True)
        
        # ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ Íµ¨Ï°∞
        self.files_dir = self.base_dir / "files"
        self.indices_dir = self.base_dir / "indices"
        self.backups_dir = self.base_dir / "backups"
        self.logs_dir = self.base_dir / "logs"
        self.exports_dir = self.base_dir / "exports"
        
        for dir_path in [self.files_dir, self.indices_dir, self.backups_dir, self.logs_dir, self.exports_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî
        self.db_path = self.base_dir / "file_management.db"
        self._init_database()
        
        # ÏãúÏä§ÌÖú ÏÑ§Ï†ï
        self.api_key = None
        self.unified_index = None
        self.chat_engines = {}
        
        # ÌÉúÍπÖ ÏãúÏä§ÌÖú
        self.tag_system = TagSystem(self.db_path)
        
        # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Îß§ÎãàÏ†Ä
        self.workflow_manager = WorkflowManager(self)
        
        # ÏÑ±Îä• Î™®ÎãàÌÑ∞
        self.performance_monitor = PerformanceMonitor()
    
    def _init_database(self):
        """SQLite Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌÖåÏù¥Î∏î
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT,
                    size INTEGER,
                    mime_type TEXT,
                    upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'uploaded',
                    version INTEGER DEFAULT 1,
                    parent_id INTEGER,
                    metadata TEXT,
                    FOREIGN KEY (parent_id) REFERENCES files (id)
                )
            ''')
            
            # ÌÉúÍ∑∏ ÌÖåÏù¥Î∏î
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS tags (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE NOT NULL,
                    color TEXT DEFAULT '#007acc',
                    description TEXT
                )
            ''')
            
            # ÌååÏùº-ÌÉúÍ∑∏ Í¥ÄÍ≥Ñ ÌÖåÏù¥Î∏î
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_tags (
                    file_id INTEGER,
                    tag_id INTEGER,
                    PRIMARY KEY (file_id, tag_id),
                    FOREIGN KEY (file_id) REFERENCES files (id),
                    FOREIGN KEY (tag_id) REFERENCES tags (id)
                )
            ''')
            
            # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÌÖåÏù¥Î∏î
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS workflows (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    description TEXT,
                    trigger_type TEXT,
                    trigger_config TEXT,
                    action_type TEXT,
                    action_config TEXT,
                    is_active BOOLEAN DEFAULT TRUE,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ÌôúÎèô Î°úÍ∑∏ ÌÖåÏù¥Î∏î
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    action TEXT NOT NULL,
                    file_id INTEGER,
                    user_id TEXT,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (file_id) REFERENCES files (id)
                )
            ''')
            
            conn.commit()
    
    def initialize_system(self, api_key: str, model_name: str = "gpt-3.5-turbo") -> bool:
        """ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî"""
        try:
            self.api_key = api_key
            
            # LlamaIndex ÏÑ§Ï†ï
            Settings.llm = OpenAI(model=model_name, api_key=api_key, temperature=0.1)
            Settings.embed_model = OpenAIEmbedding(api_key=api_key)
            
            # Í∏∞Ï°¥ Ïù∏Îç±Ïä§ Î°úÎìú ÏãúÎèÑ
            self._load_unified_index()
            
            logger.info(f"Enterprise file manager initialized with {model_name}")
            return True
        except Exception as e:
            logger.error(f"System initialization failed: {e}")
            return False
    
    def _load_unified_index(self):
        """ÌÜµÌï© Ïù∏Îç±Ïä§ Î°úÎìú"""
        index_path = self.indices_dir / "unified_index"
        if index_path.exists():
            try:
                storage_context = StorageContext.from_defaults(persist_dir=str(index_path))
                self.unified_index = load_index_from_storage(storage_context)
                logger.info("Loaded existing unified index")
            except Exception as e:
                logger.warning(f"Failed to load existing index: {e}")
    
    def add_file(self, uploaded_file, tags: List[str] = None, metadata: Dict = None) -> Tuple[bool, str]:
        """ÌååÏùº Ï∂îÍ∞Ä"""
        try:
            # ÌååÏùº Ï†ÄÏû•
            file_content = uploaded_file.getbuffer()
            file_hash = self._calculate_hash(file_content)
            
            # Ï§ëÎ≥µ Í≤ÄÏÇ¨
            if self._is_duplicate(file_hash):
                return False, "Ï§ëÎ≥µÎêú ÌååÏùºÏûÖÎãàÎã§."
            
            # ÌååÏùº Ï†ÄÏû• Í≤ΩÎ°ú ÏÉùÏÑ±
            file_path = self._get_unique_path(uploaded_file.name)
            
            with open(file_path, "wb") as f:
                f.write(file_content)
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïóê Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
            file_id = self._save_file_metadata(
                filename=uploaded_file.name,
                file_path=str(file_path),
                file_hash=file_hash,
                size=uploaded_file.size,
                mime_type=uploaded_file.type,
                metadata=metadata
            )
            
            # ÌÉúÍ∑∏ Ï∂îÍ∞Ä
            if tags:
                self.tag_system.add_tags_to_file(file_id, tags)
            
            # ÌôúÎèô Î°úÍ∑∏
            self._log_activity("file_upload", file_id, details=f"Uploaded {uploaded_file.name}")
            
            # Ïù∏Îç±Ïä§ ÏóÖÎç∞Ïù¥Ìä∏ (Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú)
            self._schedule_index_update()
            
            logger.info(f"File added: {uploaded_file.name}")
            return True, f"ÌååÏùºÏù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§. ID: {file_id}"
            
        except Exception as e:
            logger.error(f"Failed to add file: {e}")
            return False, f"ÌååÏùº Ï∂îÍ∞Ä Ïã§Ìå®: {str(e)}"
    
    def _calculate_hash(self, content: bytes) -> str:
        """ÌååÏùº Ìï¥Ïãú Í≥ÑÏÇ∞"""
        import hashlib
        return hashlib.sha256(content).hexdigest()
    
    def _is_duplicate(self, file_hash: str) -> bool:
        """Ï§ëÎ≥µ ÌååÏùº Í≤ÄÏÇ¨"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM files WHERE file_hash = ?", (file_hash,))
            return cursor.fetchone() is not None
    
    def _get_unique_path(self, filename: str) -> Path:
        """Í≥†Ïú†Ìïú ÌååÏùº Í≤ΩÎ°ú ÏÉùÏÑ±"""
        base_path = self.files_dir / filename
        
        if not base_path.exists():
            return base_path
        
        name, ext = os.path.splitext(filename)
        counter = 1
        
        while True:
            new_path = self.files_dir / f"{name}_{counter}{ext}"
            if not new_path.exists():
                return new_path
            counter += 1
    
    def _save_file_metadata(self, **kwargs) -> int:
        """ÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†ÄÏû•"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            metadata_json = json.dumps(kwargs.get('metadata', {}))
            
            cursor.execute('''
                INSERT INTO files (filename, file_path, file_hash, size, mime_type, metadata)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                kwargs['filename'],
                kwargs['file_path'],
                kwargs['file_hash'],
                kwargs['size'],
                kwargs['mime_type'],
                metadata_json
            ))
            
            file_id = cursor.lastrowid
            conn.commit()
            return file_id
    
    def get_files(self, filters: Dict = None) -> pd.DataFrame:
        """ÌååÏùº Î™©Î°ù Ï°∞Ìöå"""
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT f.*, GROUP_CONCAT(t.name) as tags
                FROM files f
                LEFT JOIN file_tags ft ON f.id = ft.file_id
                LEFT JOIN tags t ON ft.tag_id = t.id
            '''
            
            conditions = []
            params = []
            
            if filters:
                if 'status' in filters:
                    conditions.append("f.status IN ({})".format(','.join(['?'] * len(filters['status']))))
                    params.extend(filters['status'])
                
                if 'tags' in filters:
                    tag_conditions = "t.name IN ({})".format(','.join(['?'] * len(filters['tags'])))
                    conditions.append(tag_conditions)
                    params.extend(filters['tags'])
                
                if 'date_from' in filters:
                    conditions.append("f.upload_timestamp >= ?")
                    params.append(filters['date_from'])
                
                if 'date_to' in filters:
                    conditions.append("f.upload_timestamp <= ?")
                    params.append(filters['date_to'])
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += " GROUP BY f.id ORDER BY f.upload_timestamp DESC"
            
            df = pd.read_sql_query(query, conn, params=params)
            return df
    
    def delete_file(self, file_id: int) -> Tuple[bool, str]:
        """ÌååÏùº ÏÇ≠Ï†ú"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ÌååÏùº Ï†ïÎ≥¥ Ï°∞Ìöå
                cursor.execute("SELECT filename, file_path FROM files WHERE id = ?", (file_id,))
                result = cursor.fetchone()
                
                if not result:
                    return False, "ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
                
                filename, file_path = result
                
                # Î¨ºÎ¶¨Ï†Å ÌååÏùº ÏÇ≠Ï†ú
                if os.path.exists(file_path):
                    os.remove(file_path)
                
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏóêÏÑú ÏÇ≠Ï†ú
                cursor.execute("DELETE FROM file_tags WHERE file_id = ?", (file_id,))
                cursor.execute("DELETE FROM files WHERE id = ?", (file_id,))
                conn.commit()
                
                # ÌôúÎèô Î°úÍ∑∏
                self._log_activity("file_delete", file_id, details=f"Deleted {filename}")
                
                # Ïù∏Îç±Ïä§ ÏóÖÎç∞Ïù¥Ìä∏
                self._schedule_index_update()
                
                logger.info(f"File deleted: {filename}")
                return True, f"ÌååÏùºÏù¥ ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§: {filename}"
                
        except Exception as e:
            logger.error(f"Failed to delete file: {e}")
            return False, f"ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå®: {str(e)}"
    
    def create_backup(self) -> Tuple[bool, str]:
        """ÏãúÏä§ÌÖú Î∞±ÏóÖ ÏÉùÏÑ±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backups_dir / f"backup_{timestamp}.zip"
            
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Î∞±ÏóÖ
                zipf.write(self.db_path, "database.db")
                
                # ÌååÏùºÎì§ Î∞±ÏóÖ
                for file_path in self.files_dir.rglob("*"):
                    if file_path.is_file():
                        arcname = f"files/{file_path.relative_to(self.files_dir)}"
                        zipf.write(file_path, arcname)
                
                # Ïù∏Îç±Ïä§ Î∞±ÏóÖ
                if self.indices_dir.exists():
                    for index_path in self.indices_dir.rglob("*"):
                        if index_path.is_file():
                            arcname = f"indices/{index_path.relative_to(self.indices_dir)}"
                            zipf.write(index_path, arcname)
            
            logger.info(f"Backup created: {backup_path}")
            return True, f"Î∞±ÏóÖÏù¥ ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§: {backup_path.name}"
            
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            return False, f"Î∞±ÏóÖ Ïã§Ìå®: {str(e)}"
    
    def rebuild_unified_index(self) -> Tuple[bool, str]:
        """ÌÜµÌï© Ïù∏Îç±Ïä§ Ïû¨Íµ¨Ï∂ï"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT file_path FROM files WHERE status = 'uploaded'")
                file_paths = [row[0] for row in cursor.fetchall()]
            
            if not file_paths:
                return False, "Ïù∏Îç±Ïã±Ìï† ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§."
            
            # Í∏∞Ï°¥ ÌååÏùºÎì§Î°úÎ∂ÄÌÑ∞ Î¨∏ÏÑú Î°úÎìú
            documents = []
            for file_path in file_paths:
                if os.path.exists(file_path):
                    try:
                        file_docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
                        for doc in file_docs:
                            doc.metadata['source_file'] = os.path.basename(file_path)
                        documents.extend(file_docs)
                    except Exception as e:
                        logger.warning(f"Failed to load {file_path}: {e}")
            
            if documents:
                # Ïù∏Îç±Ïä§ ÏÉùÏÑ±
                self.unified_index = VectorStoreIndex.from_documents(documents)
                
                # Ïù∏Îç±Ïä§ Ï†ÄÏû•
                index_path = self.indices_dir / "unified_index"
                if index_path.exists():
                    shutil.rmtree(index_path)
                
                self.unified_index.storage_context.persist(persist_dir=str(index_path))
                
                logger.info(f"Unified index rebuilt with {len(documents)} documents")
                return True, f"ÌÜµÌï© Ïù∏Îç±Ïä§Í∞Ä Ïû¨Íµ¨Ï∂ïÎêòÏóàÏäµÎãàÎã§. ({len(documents)}Í∞ú Î¨∏ÏÑú)"
            else:
                return False, "Î°úÎìúÌï† Ïàò ÏûàÎäî Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§."
                
        except Exception as e:
            logger.error(f"Index rebuild failed: {e}")
            return False, f"Ïù∏Îç±Ïä§ Ïû¨Íµ¨Ï∂ï Ïã§Ìå®: {str(e)}"
    
    def advanced_search(self, query: str, filters: Dict = None) -> Dict:
        """Í≥†Í∏â Í≤ÄÏÉâ Í∏∞Îä•"""
        try:
            if not self.unified_index:
                return {"error": "Ïù∏Îç±Ïä§Í∞Ä Ï§ÄÎπÑÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."}
            
            # Î≤°ÌÑ∞ Í≤ÄÏÉâ
            query_engine = self.unified_index.as_query_engine(
                similarity_top_k=10,
                response_mode="tree_summarize"
            )
            
            response = query_engine.query(query)
            
            # Í≤ÄÏÉâ Í≤∞Í≥º Íµ¨ÏÑ±
            search_results = {
                "answer": str(response),
                "sources": [],
                "search_time": None
            }
            
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    source_info = {
                        "text": node.text[:300] + "...",
                        "score": getattr(node, 'score', 0),
                        "source_file": node.metadata.get('source_file', 'Unknown'),
                        "metadata": node.metadata
                    }
                    search_results["sources"].append(source_info)
            
            # ÌôúÎèô Î°úÍ∑∏
            self._log_activity("search", details=f"Query: {query}")
            
            return search_results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return {"error": f"Í≤ÄÏÉâ Ïã§Ìå®: {str(e)}"}
    
    def _schedule_index_update(self):
        """Ïù∏Îç±Ïä§ ÏóÖÎç∞Ïù¥Ìä∏ Ïä§ÏºÄÏ§ÑÎßÅ"""
        # Ïã§Ï†ú Íµ¨ÌòÑÏóêÏÑúÎäî Î∞±Í∑∏ÎùºÏö¥Îìú ÌÉúÏä§ÌÅ¨Î°ú Ï≤òÎ¶¨
        pass
    
    def _log_activity(self, action: str, file_id: int = None, details: str = None):
        """ÌôúÎèô Î°úÍ∑∏ Í∏∞Î°ù"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO activity_logs (action, file_id, details)
                    VALUES (?, ?, ?)
                ''', (action, file_id, details))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to log activity: {e}")
    
    def get_activity_logs(self, limit: int = 100) -> pd.DataFrame:
        """ÌôúÎèô Î°úÍ∑∏ Ï°∞Ìöå"""
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT al.*, f.filename
                FROM activity_logs al
                LEFT JOIN files f ON al.file_id = f.id
                ORDER BY al.timestamp DESC
                LIMIT ?
            '''
            return pd.read_sql_query(query, conn, params=[limit])
    
    def export_data(self, format_type: str = "csv") -> Tuple[bool, str]:
        """Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if format_type == "csv":
                # ÌååÏùº Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞
                df = self.get_files()
                export_path = self.exports_dir / f"files_export_{timestamp}.csv"
                df.to_csv(export_path, index=False)
                
            elif format_type == "json":
                # JSON ÌòïÌÉúÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞
                df = self.get_files()
                export_data = {
                    "export_timestamp": datetime.now().isoformat(),
                    "total_files": len(df),
                    "files": df.to_dict('records')
                }
                
                export_path = self.exports_dir / f"files_export_{timestamp}.json"
                with open(export_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            return True, f"Îç∞Ïù¥ÌÑ∞Í∞Ä ÎÇ¥Î≥¥ÎÇ¥Ï°åÏäµÎãàÎã§: {export_path.name}"
            
        except Exception as e:
            logger.error(f"Export failed: {e}")
            return False, f"ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïã§Ìå®: {str(e)}"

class TagSystem:
    """ÌÉúÍ∑∏ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
    
    def create_tag(self, name: str, color: str = "#007acc", description: str = "") -> Tuple[bool, str]:
        """ÏÉà ÌÉúÍ∑∏ ÏÉùÏÑ±"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO tags (name, color, description)
                    VALUES (?, ?, ?)
                ''', (name, color, description))
                conn.commit()
                
                return True, f"ÌÉúÍ∑∏Í∞Ä ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§: {name}"
        except sqlite3.IntegrityError:
            return False, "Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî ÌÉúÍ∑∏Î™ÖÏûÖÎãàÎã§."
        except Exception as e:
            return False, f"ÌÉúÍ∑∏ ÏÉùÏÑ± Ïã§Ìå®: {str(e)}"
    
    def get_all_tags(self) -> List[Dict]:
        """Î™®Îì† ÌÉúÍ∑∏ Ï°∞Ìöå"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM tags ORDER BY name")
            
            tags = []
            for row in cursor.fetchall():
                tags.append({
                    "id": row[0],
                    "name": row[1],
                    "color": row[2],
                    "description": row[3]
                })
            
            return tags
    
    def add_tags_to_file(self, file_id: int, tag_names: List[str]):
        """ÌååÏùºÏóê ÌÉúÍ∑∏ Ï∂îÍ∞Ä"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            for tag_name in tag_names:
                # ÌÉúÍ∑∏Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
                cursor.execute("SELECT id FROM tags WHERE name = ?", (tag_name,))
                result = cursor.fetchone()
                
                if result:
                    tag_id = result[0]
                else:
                    cursor.execute("INSERT INTO tags (name) VALUES (?)", (tag_name,))
                    tag_id = cursor.lastrowid
                
                # ÌååÏùº-ÌÉúÍ∑∏ Í¥ÄÍ≥Ñ Ï∂îÍ∞Ä
                cursor.execute('''
                    INSERT OR IGNORE INTO file_tags (file_id, tag_id)
                    VALUES (?, ?)
                ''', (file_id, tag_id))
            
            conn.commit()

class WorkflowManager:
    """ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í¥ÄÎ¶¨Ïûê"""
    
    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.active_workflows = {}
    
    def create_workflow(self, name: str, trigger_type: str, action_type: str, config: Dict) -> bool:
        """ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÉùÏÑ±"""
        # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÌòÑ (Ïòà: ÌååÏùº ÏóÖÎ°úÎìú Ïãú ÏûêÎèô ÌÉúÍπÖ, Ï†ïÍ∏∞Ï†Å Î∞±ÏóÖ Îì±)
        pass

class PerformanceMonitor:
    """ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ"""
    
    def __init__(self):
        self.metrics = {
            "search_times": [],
            "index_build_times": [],
            "file_operations": []
        }
    
    def record_search_time(self, duration: float):
        """Í≤ÄÏÉâ ÏãúÍ∞Ñ Í∏∞Î°ù"""
        self.metrics["search_times"].append({
            "timestamp": datetime.now(),
            "duration": duration
        })
    
    def get_performance_summary(self) -> Dict:
        """ÏÑ±Îä• ÏöîÏïΩ Î∞òÌôò"""
        if not self.metrics["search_times"]:
            return {"message": "ÏÑ±Îä• Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§."}
        
        search_times = [m["duration"] for m in self.metrics["search_times"]]
        
        return {
            "avg_search_time": sum(search_times) / len(search_times),
            "total_searches": len(search_times),
            "max_search_time": max(search_times),
            "min_search_time": min(search_times)
        }

# ÏãúÏä§ÌÖú Ïù∏Ïä§ÌÑ¥Ïä§
@st.cache_resource
def get_file_manager():
    return EnterpriseFileManager()

def render_dashboard(file_manager: EnterpriseFileManager):
    """ÎåÄÏãúÎ≥¥Îìú Î†åÎçîÎßÅ"""
    st.subheader("üìä ÏãúÏä§ÌÖú ÎåÄÏãúÎ≥¥Îìú")
    
    # Ï£ºÏöî Î©îÌä∏Î¶≠
    df_files = file_manager.get_files()
    total_files = len(df_files)
    total_size = df_files['size'].sum() if not df_files.empty else 0
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Ï¥ù ÌååÏùº Ïàò", total_files)
    
    with col2:
        st.metric("Ï¥ù ÌÅ¨Í∏∞", f"{total_size//1024//1024}MB")
    
    with col3:
        uploaded_today = len(df_files[df_files['upload_timestamp'].str.contains(datetime.now().strftime('%Y-%m-%d'))])
        st.metric("Ïò§Îäò ÏóÖÎ°úÎìú", uploaded_today)
    
    with col4:
        active_tags = len(file_manager.tag_system.get_all_tags())
        st.metric("ÌôúÏÑ± ÌÉúÍ∑∏", active_tags)
    
    # ÌååÏùº ÏóÖÎ°úÎìú Ìä∏Î†åÎìú Ï∞®Ìä∏
    if not df_files.empty:
        df_files['upload_date'] = pd.to_datetime(df_files['upload_timestamp']).dt.date
        daily_uploads = df_files.groupby('upload_date').size().reset_index(name='count')
        
        fig = px.line(daily_uploads, x='upload_date', y='count', title='ÏùºÎ≥Ñ ÌååÏùº ÏóÖÎ°úÎìú ÌòÑÌô©')
        st.plotly_chart(fig, use_container_width=True)

def render_file_management(file_manager: EnterpriseFileManager):
    """ÌååÏùº Í¥ÄÎ¶¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§"""
    st.subheader("üóÇÔ∏è ÌååÏùº Í¥ÄÎ¶¨")
    
    # ÌïÑÌÑ∞ ÏòµÏÖò
    with st.expander("üîç ÌïÑÌÑ∞ Î∞è Í≤ÄÏÉâ", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            all_tags = file_manager.tag_system.get_all_tags()
            tag_names = [tag['name'] for tag in all_tags]
            selected_tags = st.multiselect("ÌÉúÍ∑∏ ÌïÑÌÑ∞", tag_names)
        
        with col2:
            status_filter = st.multiselect(
                "ÏÉÅÌÉú ÌïÑÌÑ∞",
                ["uploaded", "analyzed", "archived"],
                default=["uploaded", "analyzed"]
            )
        
        with col3:
            date_range = st.date_input(
                "ÏóÖÎ°úÎìú ÎÇ†Ïßú Î≤îÏúÑ",
                value=[datetime.now().date() - timedelta(days=30), datetime.now().date()]
            )
    
    # ÌïÑÌÑ∞ Ï†ÅÏö©
    filters = {
        "status": status_filter,
        "tags": selected_tags
    }
    
    if len(date_range) == 2:
        filters["date_from"] = date_range[0].strftime('%Y-%m-%d')
        filters["date_to"] = date_range[1].strftime('%Y-%m-%d')
    
    # ÌååÏùº Î™©Î°ù Ï°∞Ìöå
    df_files = file_manager.get_files(filters)
    
    if not df_files.empty:
        # ÌååÏùº Î™©Î°ù ÌëúÏãú
        for _, file_row in df_files.iterrows():
            with st.container():
                col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
                
                with col1:
                    st.write(f"üìÑ **{file_row['filename']}**")
                    if file_row['tags']:
                        tags_html = ""
                        for tag in file_row['tags'].split(','):
                            tags_html += f'<span style="background-color: #007acc; color: white; padding: 2px 6px; margin: 2px; border-radius: 3px; font-size: 12px;">{tag}</span>'
                        st.markdown(tags_html, unsafe_allow_html=True)
                
                with col2:
                    st.write(f"{file_row['size']//1024}KB")
                    st.caption(file_row['upload_timestamp'][:10])
                
                with col3:
                    st.write(file_row['status'])
                
                with col4:
                    if st.button("üóëÔ∏è", key=f"delete_{file_row['id']}", help="ÌååÏùº ÏÇ≠Ï†ú"):
                        success, message = file_manager.delete_file(file_row['id'])
                        if success:
                            st.success(message)
                            st.rerun()
                        else:
                            st.error(message)
                
                st.divider()
    else:
        st.info("Ï°∞Í±¥Ïóê ÎßûÎäî ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")

def main():
    st.title("üóÇÔ∏è ÏóîÌÑ∞ÌîÑÎùºÏù¥Ï¶à ÌååÏùº Í¥ÄÎ¶¨ & QA ÏãúÏä§ÌÖú")
    st.markdown("---")
    
    # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    file_manager = get_file_manager()
    
    # ÏÇ¨Ïù¥ÎìúÎ∞î ÏÑ§Ï†ï
    with st.sidebar:
        st.header("‚öôÔ∏è ÏãúÏä§ÌÖú ÏÑ§Ï†ï")
        
        # API ÌÇ§ ÏûÖÎ†•
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", None)
        except:
            api_key = None
        
        if not api_key:
            api_key = st.text_input("OpenAI API Key", type="password")
        
        # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
        if st.button("üöÄ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî", type="primary"):
            if api_key:
                if file_manager.initialize_system(api_key):
                    st.success("‚úÖ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å!")
                    st.session_state.system_ready = True
                else:
                    st.error("ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            else:
                st.warning("API ÌÇ§Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
        
        st.markdown("---")
        
        # ÏãúÏä§ÌÖú ÏûëÏóÖ
        if getattr(st.session_state, 'system_ready', False):
            st.subheader("üîß ÏãúÏä§ÌÖú ÏûëÏóÖ")
            
            if st.button("üîÑ Ïù∏Îç±Ïä§ Ïû¨Íµ¨Ï∂ï"):
                with st.spinner("Ïù∏Îç±Ïä§Î•º Ïû¨Íµ¨Ï∂ïÌïòÎäî Ï§ë..."):
                    success, message = file_manager.rebuild_unified_index()
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
            
            if st.button("üíæ Î∞±ÏóÖ ÏÉùÏÑ±"):
                with st.spinner("Î∞±ÏóÖÏùÑ ÏÉùÏÑ±ÌïòÎäî Ï§ë..."):
                    success, message = file_manager.create_backup()
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
            
            # Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞
            export_format = st.selectbox("ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÌòïÏãù", ["csv", "json"])
            if st.button("üì§ Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞"):
                success, message = file_manager.export_data(export_format)
                if success:
                    st.success(message)
                else:
                    st.error(message)
    
    # Î©îÏù∏ Ïª®ÌÖêÏ∏†
    if not api_key:
        st.warning("‚ö†Ô∏è OpenAI API ÌÇ§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")
        return
    
    if not getattr(st.session_state, 'system_ready', False):
        st.info("üëà ÏÇ¨Ïù¥ÎìúÎ∞îÏóêÏÑú ÏãúÏä§ÌÖúÏùÑ Ï¥àÍ∏∞ÌôîÌï¥Ï£ºÏÑ∏Ïöî.")
        
        # ÏãúÏä§ÌÖú ÏÜåÍ∞ú
        st.markdown("""
        ### üåü ÏóîÌÑ∞ÌîÑÎùºÏù¥Ï¶à ÌååÏùº Í¥ÄÎ¶¨ ÏãúÏä§ÌÖúÏùò ÌäπÏßï:
        
        **üìÅ ÏôÑÏ†ÑÌïú ÌååÏùº Í¥ÄÎ¶¨**
        - ÌååÏùº Î≤ÑÏ†Ñ Í¥ÄÎ¶¨ Î∞è Ï§ëÎ≥µ Î∞©ÏßÄ
        - Í≥†Í∏â Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞è ÌÉúÍπÖ ÏãúÏä§ÌÖú
        - ÏûêÎèôÌôîÎêú Î∞±ÏóÖ Î∞è Î≥µÏõê
        
        **üîç ÏßÄÎä•Ìòï Í≤ÄÏÉâ**
        - AI Í∏∞Î∞ò ÏùòÎØ∏Î°†Ï†Å Í≤ÄÏÉâ
        - Í≥†Í∏â ÌïÑÌÑ∞ÎßÅ Î∞è Ï†ïÎ†¨
        - Ïã§ÏãúÍ∞Ñ Í≤ÄÏÉâ Í≤∞Í≥º
        
        **‚ö° ÏÑ±Îä• ÏµúÏ†ÅÌôî**
        - Î∂ÑÏÇ∞ Ïù∏Îç±Ïã± Î∞è Ï∫êÏã±
        - Î∞∞Ïπò Ï≤òÎ¶¨ Î∞è ÏõåÌÅ¨ÌîåÎ°úÏö∞
        - Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
        
        **üîí ÏóîÌÑ∞ÌîÑÎùºÏù¥Ï¶à Í∏∞Îä•**
        - ÌôúÎèô Î°úÍ∑∏ Î∞è Í∞êÏÇ¨ Ï∂îÏ†Å
        - Îç∞Ïù¥ÌÑ∞ ÎÇ¥Î≥¥ÎÇ¥Í∏∞/Í∞ÄÏ†∏Ïò§Í∏∞
        - API ÌÜµÌï© ÏßÄÏõê
        """)
        return
    
    # ÌÉ≠ Íµ¨ÏÑ±
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üì§ ÌååÏùº ÏóÖÎ°úÎìú",
        "üóÇÔ∏è ÌååÏùº Í¥ÄÎ¶¨", 
        "üîç Í≥†Í∏â Í≤ÄÏÉâ",
        "üè∑Ô∏è ÌÉúÍ∑∏ Í¥ÄÎ¶¨",
        "üìä ÎåÄÏãúÎ≥¥Îìú",
        "üìã ÌôúÎèô Î°úÍ∑∏"
    ])
    
    with tab1:
        st.header("üì§ ÌååÏùº ÏóÖÎ°úÎìú")
        
        # ÌååÏùº ÏóÖÎ°úÎìú
        uploaded_files = st.file_uploader(
            "ÌååÏùº ÏÑ†ÌÉù",
            type=['pdf', 'txt', 'docx', 'md', 'html'],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            # ÌÉúÍ∑∏ ÏûÖÎ†•
            all_tags = file_manager.tag_system.get_all_tags()
            existing_tag_names = [tag['name'] for tag in all_tags]
            
            tags_input = st.text_input(
                "ÌÉúÍ∑∏ (ÏâºÌëúÎ°ú Íµ¨Î∂Ñ)",
                help="Í∏∞Ï°¥ ÌÉúÍ∑∏: " + ", ".join(existing_tag_names[:10]) if existing_tag_names else "ÌÉúÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§."
            )
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏûÖÎ†•
            with st.expander("üìù Ï∂îÍ∞Ä Î©îÌÉÄÎç∞Ïù¥ÌÑ∞"):
                description = st.text_area("ÏÑ§Î™Ö")
                category = st.selectbox("Ïπ¥ÌÖåÍ≥†Î¶¨", ["Î¨∏ÏÑú", "Î≥¥Í≥†ÏÑú", "Îß§Îâ¥Ïñº", "Í∏∞ÌÉÄ"])
                importance = st.slider("Ï§ëÏöîÎèÑ", 1, 5, 3)
            
            if st.button("üì• ÏóÖÎ°úÎìú ÏãúÏûë", type="primary"):
                tags = [tag.strip() for tag in tags_input.split(',')] if tags_input else []
                metadata = {
                    "description": description,
                    "category": category,
                    "importance": importance
                }
                
                success_count = 0
                for uploaded_file in uploaded_files:
                    success, message = file_manager.add_file(uploaded_file, tags, metadata)
                    if success:
                        success_count += 1
                    else:
                        st.error(f"{uploaded_file.name}: {message}")
                
                if success_count > 0:
                    st.success(f"‚úÖ {success_count}Í∞ú ÌååÏùºÏù¥ ÏóÖÎ°úÎìúÎêòÏóàÏäµÎãàÎã§!")
    
    with tab2:
        render_file_management(file_manager)
    
    with tab3:
        st.header("üîç Í≥†Í∏â Í≤ÄÏÉâ")
        
        search_query = st.text_input(
            "Í≤ÄÏÉâÏñ¥Î•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî:",
            placeholder="Ïòà: Î¨∏ÏÑúÏóêÏÑú Ï§ëÏöîÌïú Ï†ïÎ≥¥Î•º Ï∞æÏïÑÏ£ºÏÑ∏Ïöî"
        )
        
        if search_query and st.button("üîç Í≤ÄÏÉâ", type="primary"):
            with st.spinner("Í≤ÄÏÉâÌïòÎäî Ï§ë..."):
                results = file_manager.advanced_search(search_query)
            
            if "error" in results:
                st.error(results["error"])
            else:
                st.write("### üìù ÎãµÎ≥Ä")
                st.write(results["answer"])
                
                if results["sources"]:
                    st.write("### üìÑ Ï∞∏Ï°∞ Î¨∏ÏÑú")
                    for i, source in enumerate(results["sources"]):
                        with st.expander(f"ÏÜåÏä§ {i+1}: {source['source_file']} (Ïú†ÏÇ¨ÎèÑ: {source['score']:.3f})"):
                            st.write(source["text"])
    
    with tab4:
        st.header("üè∑Ô∏è ÌÉúÍ∑∏ Í¥ÄÎ¶¨")
        
        # ÏÉà ÌÉúÍ∑∏ ÏÉùÏÑ±
        with st.expander("‚ûï ÏÉà ÌÉúÍ∑∏ ÏÉùÏÑ±"):
            col1, col2 = st.columns(2)
            with col1:
                new_tag_name = st.text_input("ÌÉúÍ∑∏ Ïù¥Î¶Ñ")
                new_tag_color = st.color_picker("ÌÉúÍ∑∏ ÏÉâÏÉÅ", "#007acc")
            with col2:
                new_tag_desc = st.text_area("ÌÉúÍ∑∏ ÏÑ§Î™Ö", height=100)
            
            if st.button("‚ú® ÌÉúÍ∑∏ ÏÉùÏÑ±"):
                if new_tag_name:
                    success, message = file_manager.tag_system.create_tag(
                        new_tag_name, new_tag_color, new_tag_desc
                    )
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
        
        # Í∏∞Ï°¥ ÌÉúÍ∑∏ Î™©Î°ù
        st.subheader("üìã Í∏∞Ï°¥ ÌÉúÍ∑∏")
        tags = file_manager.tag_system.get_all_tags()
        
        if tags:
            for tag in tags:
                col1, col2, col3 = st.columns([2, 3, 1])
                with col1:
                    st.markdown(f'<span style="background-color: {tag["color"]}; color: white; padding: 4px 8px; border-radius: 4px;">{tag["name"]}</span>', unsafe_allow_html=True)
                with col2:
                    st.write(tag["description"] or "ÏÑ§Î™Ö ÏóÜÏùå")
                with col3:
                    st.caption(f"ID: {tag['id']}")
        else:
            st.info("ÏÉùÏÑ±Îêú ÌÉúÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§.")
    
    with tab5:
        render_dashboard(file_manager)
    
    with tab6:
        st.header("üìã ÌôúÎèô Î°úÍ∑∏")
        
        # Î°úÍ∑∏ ÌïÑÌÑ∞
        log_limit = st.slider("ÌëúÏãúÌï† Î°úÍ∑∏ Ïàò", 10, 500, 100)
        
        # ÌôúÎèô Î°úÍ∑∏ Ï°∞Ìöå
        df_logs = file_manager.get_activity_logs(log_limit)
        
        if not df_logs.empty:
            st.dataframe(df_logs, use_container_width=True)
        else:
            st.info("ÌôúÎèô Î°úÍ∑∏Í∞Ä ÏóÜÏäµÎãàÎã§.")

if __name__ == "__main__":
    main()