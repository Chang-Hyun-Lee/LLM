# advanced/practice4_advanced.py
import streamlit as st
import os
import sys
import json
import pickle
import shutil
import zipfile
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from concurrent.futures import ThreadPoolExecutor
import schedule
import threading
import time

from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, load_index_from_storage
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.memory import ChatMemoryBuffer
import logging

# ê³µìš© ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent / "shared"))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ê¸‰ íŒŒì¼ ê´€ë¦¬ & QA",
    page_icon="ğŸ—‚ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnterpriseFileManager:
    """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_dir = Path("../file_management_enterprise")
        self.base_dir.mkdir(exist_ok=True)
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        self.files_dir = self.base_dir / "files"
        self.indices_dir = self.base_dir / "indices"
        self.backups_dir = self.base_dir / "backups"
        self.logs_dir = self.base_dir / "logs"
        self.exports_dir = self.base_dir / "exports"
        
        for dir_path in [self.files_dir, self.indices_dir, self.backups_dir, self.logs_dir, self.exports_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db_path = self.base_dir / "file_management.db"
        self._init_database()
        
        # ì‹œìŠ¤í…œ ì„¤ì •
        self.api_key = None
        self.unified_index = None
        self.chat_engines = {}
        
        # íƒœê¹… ì‹œìŠ¤í…œ
        self.tag_system = TagSystem(self.db_path)
        
        # ì›Œí¬í”Œë¡œìš° ë§¤ë‹ˆì €
        self.workflow_manager = WorkflowManager(self)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°
        self.performance_monitor = PerformanceMonitor()
    
    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    filename TEXT NOT NULL,
                    file_path TEXT UNIQUE NOT NULL,
                    file_hash TEXT,
                    size INTEGER,
                    mime_type TEXT,
                    upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'uploaded',
                    version INTEGER DEFAULT 1,
                    parent_id INTEGER,
                    metadata TEXT,
                    FOREIGN KEY (parent_id) REFERENCES files (id)
                )
            ''')
            
            # íƒœê·¸ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS tags (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE NOT NULL,
                    color TEXT DEFAULT '#007acc',
                    description TEXT
                )
            ''')
            
            # íŒŒì¼-íƒœê·¸ ê´€ê³„ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS file_tags (
                    file_id INTEGER,
                    tag_id INTEGER,
                    PRIMARY KEY (file_id, tag_id),
                    FOREIGN KEY (file_id) REFERENCES files (id),
                    FOREIGN KEY (tag_id) REFERENCES tags (id)
                )
            ''')
            
            # ì›Œí¬í”Œë¡œìš° í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS workflows (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT NOT NULL,
                    description TEXT,
                    trigger_type TEXT,
                    trigger_config TEXT,
                    action_type TEXT,
                    action_config TEXT,
                    is_active BOOLEAN DEFAULT TRUE,
                    created_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # í™œë™ ë¡œê·¸ í…Œì´ë¸”
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    action TEXT NOT NULL,
                    file_id INTEGER,
                    user_id TEXT,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (file_id) REFERENCES files (id)
                )
            ''')
            
            conn.commit()
    
    def initialize_system(self, api_key: str, model_name: str = "gpt-3.5-turbo") -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.api_key = api_key
            
            # LlamaIndex ì„¤ì •
            Settings.llm = OpenAI(model=model_name, api_key=api_key, temperature=0.1)
            Settings.embed_model = OpenAIEmbedding(api_key=api_key)
            
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            self._load_unified_index()
            
            logger.info(f"Enterprise file manager initialized with {model_name}")
            return True
        except Exception as e:
            logger.error(f"System initialization failed: {e}")
            return False
    
    def _load_unified_index(self):
        """í†µí•© ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_path = self.indices_dir / "unified_index"
        if index_path.exists():
            try:
                storage_context = StorageContext.from_defaults(persist_dir=str(index_path))
                self.unified_index = load_index_from_storage(storage_context)
                logger.info("Loaded existing unified index")
            except Exception as e:
                logger.warning(f"Failed to load existing index: {e}")
    
    def add_file(self, uploaded_file, tags: List[str] = None, metadata: Dict = None) -> Tuple[bool, str]:
        """íŒŒì¼ ì¶”ê°€"""
        try:
            # íŒŒì¼ ì €ì¥
            file_content = uploaded_file.getbuffer()
            file_hash = self._calculate_hash(file_content)
            
            # ì¤‘ë³µ ê²€ì‚¬
            if self._is_duplicate(file_hash):
                return False, "ì¤‘ë³µëœ íŒŒì¼ì…ë‹ˆë‹¤."
            
            # íŒŒì¼ ì €ì¥ ê²½ë¡œ ìƒì„±
            file_path = self._get_unique_path(uploaded_file.name)
            
            with open(file_path, "wb") as f:
                f.write(file_content)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ë©”íƒ€ë°ì´í„° ì €ì¥
            file_id = self._save_file_metadata(
                filename=uploaded_file.name,
                file_path=str(file_path),
                file_hash=file_hash,
                size=uploaded_file.size,
                mime_type=uploaded_file.type,
                metadata=metadata
            )
            
            # íƒœê·¸ ì¶”ê°€
            if tags:
                self.tag_system.add_tags_to_file(file_id, tags)
            
            # í™œë™ ë¡œê·¸
            self._log_activity("file_upload", file_id, details=f"Uploaded {uploaded_file.name}")
            
            # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
            self._schedule_index_update()
            
            logger.info(f"File added: {uploaded_file.name}")
            return True, f"íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ID: {file_id}"
            
        except Exception as e:
            logger.error(f"Failed to add file: {e}")
            return False, f"íŒŒì¼ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}"
    
    def _calculate_hash(self, content: bytes) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        import hashlib
        return hashlib.sha256(content).hexdigest()
    
    def _is_duplicate(self, file_hash: str) -> bool:
        """ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM files WHERE file_hash = ?", (file_hash,))
            return cursor.fetchone() is not None
    
    def _get_unique_path(self, filename: str) -> Path:
        """ê³ ìœ í•œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        base_path = self.files_dir / filename
        
        if not base_path.exists():
            return base_path
        
        name, ext = os.path.splitext(filename)
        counter = 1
        
        while True:
            new_path = self.files_dir / f"{name}_{counter}{ext}"
            if not new_path.exists():
                return new_path
            counter += 1
    
    def _save_file_metadata(self, **kwargs) -> int:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            metadata_json = json.dumps(kwargs.get('metadata', {}))
            
            cursor.execute('''
                INSERT INTO files (filename, file_path, file_hash, size, mime_type, metadata)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                kwargs['filename'],
                kwargs['file_path'],
                kwargs['file_hash'],
                kwargs['size'],
                kwargs['mime_type'],
                metadata_json
            ))
            
            file_id = cursor.lastrowid
            conn.commit()
            return file_id
    
    def get_files(self, filters: Dict = None) -> pd.DataFrame:
        """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT f.*, GROUP_CONCAT(t.name) as tags
                FROM files f
                LEFT JOIN file_tags ft ON f.id = ft.file_id
                LEFT JOIN tags t ON ft.tag_id = t.id
            '''
            
            conditions = []
            params = []
            
            if filters:
                if 'status' in filters:
                    conditions.append("f.status IN ({})".format(','.join(['?'] * len(filters['status']))))
                    params.extend(filters['status'])
                
                if 'tags' in filters:
                    tag_conditions = "t.name IN ({})".format(','.join(['?'] * len(filters['tags'])))
                    conditions.append(tag_conditions)
                    params.extend(filters['tags'])
                
                if 'date_from' in filters:
                    conditions.append("f.upload_timestamp >= ?")
                    params.append(filters['date_from'])
                
                if 'date_to' in filters:
                    conditions.append("f.upload_timestamp <= ?")
                    params.append(filters['date_to'])
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += " GROUP BY f.id ORDER BY f.upload_timestamp DESC"
            
            df = pd.read_sql_query(query, conn, params=params)
            return df
    
    def delete_file(self, file_id: int) -> Tuple[bool, str]:
        """íŒŒì¼ ì‚­ì œ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # íŒŒì¼ ì •ë³´ ì¡°íšŒ
                cursor.execute("SELECT filename, file_path FROM files WHERE id = ?", (file_id,))
                result = cursor.fetchone()
                
                if not result:
                    return False, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                filename, file_path = result
                
                # ë¬¼ë¦¬ì  íŒŒì¼ ì‚­ì œ
                if os.path.exists(file_path):
                    os.remove(file_path)
                
                # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚­ì œ
                cursor.execute("DELETE FROM file_tags WHERE file_id = ?", (file_id,))
                cursor.execute("DELETE FROM files WHERE id = ?", (file_id,))
                conn.commit()
                
                # í™œë™ ë¡œê·¸
                self._log_activity("file_delete", file_id, details=f"Deleted {filename}")
                
                # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                self._schedule_index_update()
                
                logger.info(f"File deleted: {filename}")
                return True, f"íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {filename}"
                
        except Exception as e:
            logger.error(f"Failed to delete file: {e}")
            return False, f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {str(e)}"
    
    def create_backup(self) -> Tuple[bool, str]:
        """ì‹œìŠ¤í…œ ë°±ì—… ìƒì„±"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backups_dir / f"backup_{timestamp}.zip"
            
            with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
                zipf.write(self.db_path, "database.db")
                
                # íŒŒì¼ë“¤ ë°±ì—…
                for file_path in self.files_dir.rglob("*"):
                    if file_path.is_file():
                        arcname = f"files/{file_path.relative_to(self.files_dir)}"
                        zipf.write(file_path, arcname)
                
                # ì¸ë±ìŠ¤ ë°±ì—…
                if self.indices_dir.exists():
                    for index_path in self.indices_dir.rglob("*"):
                        if index_path.is_file():
                            arcname = f"indices/{index_path.relative_to(self.indices_dir)}"
                            zipf.write(index_path, arcname)
            
            logger.info(f"Backup created: {backup_path}")
            return True, f"ë°±ì—…ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {backup_path.name}"
            
        except Exception as e:
            logger.error(f"Backup failed: {e}")
            return False, f"ë°±ì—… ì‹¤íŒ¨: {str(e)}"
    
    def rebuild_unified_index(self) -> Tuple[bool, str]:
        """í†µí•© ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT file_path FROM files WHERE status = 'uploaded'")
                file_paths = [row[0] for row in cursor.fetchall()]
            
            if not file_paths:
                return False, "ì¸ë±ì‹±í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            
            # ê¸°ì¡´ íŒŒì¼ë“¤ë¡œë¶€í„° ë¬¸ì„œ ë¡œë“œ
            documents = []
            for file_path in file_paths:
                if os.path.exists(file_path):
                    try:
                        file_docs = SimpleDirectoryReader(input_files=[file_path]).load_data()
                        for doc in file_docs:
                            doc.metadata['source_file'] = os.path.basename(file_path)
                        documents.extend(file_docs)
                    except Exception as e:
                        logger.warning(f"Failed to load {file_path}: {e}")
            
            if documents:
                # ì¸ë±ìŠ¤ ìƒì„±
                self.unified_index = VectorStoreIndex.from_documents(documents)
                
                # ì¸ë±ìŠ¤ ì €ì¥
                index_path = self.indices_dir / "unified_index"
                if index_path.exists():
                    shutil.rmtree(index_path)
                
                self.unified_index.storage_context.persist(persist_dir=str(index_path))
                
                logger.info(f"Unified index rebuilt with {len(documents)} documents")
                return True, f"í†µí•© ì¸ë±ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤. ({len(documents)}ê°œ ë¬¸ì„œ)"
            else:
                return False, "ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"Index rebuild failed: {e}")
            return False, f"ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}"
    
    def advanced_search(self, query: str, filters: Dict = None) -> Dict:
        """ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥"""
        try:
            if not self.unified_index:
                return {"error": "ì¸ë±ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            # ë²¡í„° ê²€ìƒ‰
            query_engine = self.unified_index.as_query_engine(
                similarity_top_k=10,
                response_mode="tree_summarize"
            )
            
            response = query_engine.query(query)
            
            # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
            search_results = {
                "answer": str(response),
                "sources": [],
                "search_time": None
            }
            
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    source_info = {
                        "text": node.text[:300] + "...",
                        "score": getattr(node, 'score', 0),
                        "source_file": node.metadata.get('source_file', 'Unknown'),
                        "metadata": node.metadata
                    }
                    search_results["sources"].append(source_info)
            
            # í™œë™ ë¡œê·¸
            self._log_activity("search", details=f"Query: {query}")
            
            return search_results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return {"error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}
    
    def _schedule_index_update(self):
        """ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ë§"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì²˜ë¦¬
        pass
    
    def _log_activity(self, action: str, file_id: int = None, details: str = None):
        """í™œë™ ë¡œê·¸ ê¸°ë¡"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO activity_logs (action, file_id, details)
                    VALUES (?, ?, ?)
                ''', (action, file_id, details))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to log activity: {e}")
    
    def get_activity_logs(self, limit: int = 100) -> pd.DataFrame:
        """í™œë™ ë¡œê·¸ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT al.*, f.filename
                FROM activity_logs al
                LEFT JOIN files f ON al.file_id = f.id
                ORDER BY al.timestamp DESC
                LIMIT ?
            '''
            return pd.read_sql_query(query, conn, params=[limit])
    
    def export_data(self, format_type: str = "csv") -> Tuple[bool, str]:
        """ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if format_type == "csv":
                # íŒŒì¼ ë©”íƒ€ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                df = self.get_files()
                export_path = self.exports_dir / f"files_export_{timestamp}.csv"
                df.to_csv(export_path, index=False)
                
            elif format_type == "json":
                # JSON í˜•íƒœë¡œ ë‚´ë³´ë‚´ê¸°
                df = self.get_files()
                export_data = {
                    "export_timestamp": datetime.now().isoformat(),
                    "total_files": len(df),
                    "files": df.to_dict('records')
                }
                
                export_path = self.exports_dir / f"files_export_{timestamp}.json"
                with open(export_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            return True, f"ë°ì´í„°ê°€ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤: {export_path.name}"
            
        except Exception as e:
            logger.error(f"Export failed: {e}")
            return False, f"ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}"

class TagSystem:
    """íƒœê·¸ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
    
    def create_tag(self, name: str, color: str = "#007acc", description: str = "") -> Tuple[bool, str]:
        """ìƒˆ íƒœê·¸ ìƒì„±"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO tags (name, color, description)
                    VALUES (?, ?, ?)
                ''', (name, color, description))
                conn.commit()
                
                return True, f"íƒœê·¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {name}"
        except sqlite3.IntegrityError:
            return False, "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íƒœê·¸ëª…ì…ë‹ˆë‹¤."
        except Exception as e:
            return False, f"íƒœê·¸ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def get_all_tags(self) -> List[Dict]:
        """ëª¨ë“  íƒœê·¸ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM tags ORDER BY name")
            
            tags = []
            for row in cursor.fetchall():
                tags.append({
                    "id": row[0],
                    "name": row[1],
                    "color": row[2],
                    "description": row[3]
                })
            
            return tags
    
    def add_tags_to_file(self, file_id: int, tag_names: List[str]):
        """íŒŒì¼ì— íƒœê·¸ ì¶”ê°€"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            for tag_name in tag_names:
                # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ìƒì„±
                cursor.execute("SELECT id FROM tags WHERE name = ?", (tag_name,))
                result = cursor.fetchone()
                
                if result:
                    tag_id = result[0]
                else:
                    cursor.execute("INSERT INTO tags (name) VALUES (?)", (tag_name,))
                    tag_id = cursor.lastrowid
                
                # íŒŒì¼-íƒœê·¸ ê´€ê³„ ì¶”ê°€
                cursor.execute('''
                    INSERT OR IGNORE INTO file_tags (file_id, tag_id)
                    VALUES (?, ?)
                ''', (file_id, tag_id))
            
            conn.commit()

class WorkflowManager:
    """ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì"""
    
    def __init__(self, file_manager):
        self.file_manager = file_manager
        self.active_workflows = {}
    
    def create_workflow(self, name: str, trigger_type: str, action_type: str, config: Dict) -> bool:
        """ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        # ì›Œí¬í”Œë¡œìš° êµ¬í˜„ (ì˜ˆ: íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ íƒœê¹…, ì •ê¸°ì  ë°±ì—… ë“±)
        pass

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics = {
            "search_times": [],
            "index_build_times": [],
            "file_operations": []
        }
    
    def record_search_time(self, duration: float):
        """ê²€ìƒ‰ ì‹œê°„ ê¸°ë¡"""
        self.metrics["search_times"].append({
            "timestamp": datetime.now(),
            "duration": duration
        })
    
    def get_performance_summary(self) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        if not self.metrics["search_times"]:
            return {"message": "ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        search_times = [m["duration"] for m in self.metrics["search_times"]]
        
        return {
            "avg_search_time": sum(search_times) / len(search_times),
            "total_searches": len(search_times),
            "max_search_time": max(search_times),
            "min_search_time": min(search_times)
        }

# ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_file_manager():
    return EnterpriseFileManager()

def render_dashboard(file_manager: EnterpriseFileManager):
    """ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ")
    
    # ì£¼ìš” ë©”íŠ¸ë¦­
    df_files = file_manager.get_files()
    total_files = len(df_files)
    total_size = df_files['size'].sum() if not df_files.empty else 0
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ íŒŒì¼ ìˆ˜", total_files)
    
    with col2:
        st.metric("ì´ í¬ê¸°", f"{total_size//1024//1024}MB")
    
    with col3:
        uploaded_today = len(df_files[df_files['upload_timestamp'].str.contains(datetime.now().strftime('%Y-%m-%d'))])
        st.metric("ì˜¤ëŠ˜ ì—…ë¡œë“œ", uploaded_today)
    
    with col4:
        active_tags = len(file_manager.tag_system.get_all_tags())
        st.metric("í™œì„± íƒœê·¸", active_tags)
    
    # íŒŒì¼ ì—…ë¡œë“œ íŠ¸ë Œë“œ ì°¨íŠ¸
    if not df_files.empty:
        df_files['upload_date'] = pd.to_datetime(df_files['upload_timestamp']).dt.date
        daily_uploads = df_files.groupby('upload_date').size().reset_index(name='count')
        
        fig = px.line(daily_uploads, x='upload_date', y='count', title='ì¼ë³„ íŒŒì¼ ì—…ë¡œë“œ í˜„í™©')
        st.plotly_chart(fig, use_container_width=True)

def render_file_management(file_manager: EnterpriseFileManager):
    """íŒŒì¼ ê´€ë¦¬ ì¸í„°í˜ì´ìŠ¤"""
    st.subheader("ğŸ—‚ï¸ íŒŒì¼ ê´€ë¦¬")
    
    # í•„í„° ì˜µì…˜
    with st.expander("ğŸ” í•„í„° ë° ê²€ìƒ‰", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            all_tags = file_manager.tag_system.get_all_tags()
            tag_names = [tag['name'] for tag in all_tags]
            selected_tags = st.multiselect("íƒœê·¸ í•„í„°", tag_names)
        
        with col2:
            status_filter = st.multiselect(
                "ìƒíƒœ í•„í„°",
                ["uploaded", "analyzed", "archived"],
                default=["uploaded", "analyzed"]
            )
        
        with col3:
            date_range = st.date_input(
                "ì—…ë¡œë“œ ë‚ ì§œ ë²”ìœ„",
                value=[datetime.now().date() - timedelta(days=30), datetime.now().date()]
            )
    
    # í•„í„° ì ìš©
    filters = {
        "status": status_filter,
        "tags": selected_tags
    }
    
    if len(date_range) == 2:
        filters["date_from"] = date_range[0].strftime('%Y-%m-%d')
        filters["date_to"] = date_range[1].strftime('%Y-%m-%d')
    
    # íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    df_files = file_manager.get_files(filters)
    
    if not df_files.empty:
        # íŒŒì¼ ëª©ë¡ í‘œì‹œ
        for _, file_row in df_files.iterrows():
            with st.container():
                col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
                
                with col1:
                    st.write(f"ğŸ“„ **{file_row['filename']}**")
                    if file_row['tags']:
                        tags_html = ""
                        for tag in file_row['tags'].split(','):
                            tags_html += f'<span style="background-color: #007acc; color: white; padding: 2px 6px; margin: 2px; border-radius: 3px; font-size: 12px;">{tag}</span>'
                        st.markdown(tags_html, unsafe_allow_html=True)
                
                with col2:
                    st.write(f"{file_row['size']//1024}KB")
                    st.caption(file_row['upload_timestamp'][:10])
                
                with col3:
                    st.write(file_row['status'])
                
                with col4:
                    if st.button("ğŸ—‘ï¸", key=f"delete_{file_row['id']}", help="íŒŒì¼ ì‚­ì œ"):
                        success, message = file_manager.delete_file(file_row['id'])
                        if success:
                            st.success(message)
                            st.rerun()
                        else:
                            st.error(message)
                
                st.divider()
    else:
        st.info("ì¡°ê±´ì— ë§ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

def main():
    st.title("ğŸ—‚ï¸ ì—”í„°í”„ë¼ì´ì¦ˆ íŒŒì¼ ê´€ë¦¬ & QA ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    file_manager = get_file_manager()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", None)
        except:
            api_key = None
        
        if not api_key:
            api_key = st.text_input("OpenAI API Key", type="password")
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if st.button("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
            if api_key:
                if file_manager.initialize_system(api_key):
                    st.success("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                    st.session_state.system_ready = True
                else:
                    st.error("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            else:
                st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        st.markdown("---")
        
        # ì‹œìŠ¤í…œ ì‘ì—…
        if getattr(st.session_state, 'system_ready', False):
            st.subheader("ğŸ”§ ì‹œìŠ¤í…œ ì‘ì—…")
            
            if st.button("ğŸ”„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"):
                with st.spinner("ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ì¤‘..."):
                    success, message = file_manager.rebuild_unified_index()
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
            
            if st.button("ğŸ’¾ ë°±ì—… ìƒì„±"):
                with st.spinner("ë°±ì—…ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    success, message = file_manager.create_backup()
                    if success:
                        st.success(message)
                    else:
                        st.error(message)
            
            # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
            export_format = st.selectbox("ë‚´ë³´ë‚´ê¸° í˜•ì‹", ["csv", "json"])
            if st.button("ğŸ“¤ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"):
                success, message = file_manager.export_data(export_format)
                if success:
                    st.success(message)
                else:
                    st.error(message)
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    if not getattr(st.session_state, 'system_ready', False):
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        
        # ì‹œìŠ¤í…œ ì†Œê°œ
        st.markdown("""
        ### ğŸŒŸ ì—”í„°í”„ë¼ì´ì¦ˆ íŒŒì¼ ê´€ë¦¬ ì‹œìŠ¤í…œì˜ íŠ¹ì§•:
        
        **ğŸ“ ì™„ì „í•œ íŒŒì¼ ê´€ë¦¬**
        - íŒŒì¼ ë²„ì „ ê´€ë¦¬ ë° ì¤‘ë³µ ë°©ì§€
        - ê³ ê¸‰ ë©”íƒ€ë°ì´í„° ë° íƒœê¹… ì‹œìŠ¤í…œ
        - ìë™í™”ëœ ë°±ì—… ë° ë³µì›
        
        **ğŸ” ì§€ëŠ¥í˜• ê²€ìƒ‰**
        - AI ê¸°ë°˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
        - ê³ ê¸‰ í•„í„°ë§ ë° ì •ë ¬
        - ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼
        
        **âš¡ ì„±ëŠ¥ ìµœì í™”**
        - ë¶„ì‚° ì¸ë±ì‹± ë° ìºì‹±
        - ë°°ì¹˜ ì²˜ë¦¬ ë° ì›Œí¬í”Œë¡œìš°
        - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        
        **ğŸ”’ ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥**
        - í™œë™ ë¡œê·¸ ë° ê°ì‚¬ ì¶”ì 
        - ë°ì´í„° ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸°
        - API í†µí•© ì§€ì›
        """)
        return
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ",
        "ğŸ—‚ï¸ íŒŒì¼ ê´€ë¦¬", 
        "ğŸ” ê³ ê¸‰ ê²€ìƒ‰",
        "ğŸ·ï¸ íƒœê·¸ ê´€ë¦¬",
        "ğŸ“Š ëŒ€ì‹œë³´ë“œ",
        "ğŸ“‹ í™œë™ ë¡œê·¸"
    ])
    
    with tab1:
        st.header("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "íŒŒì¼ ì„ íƒ",
            type=['pdf', 'txt', 'docx', 'md', 'html'],
            accept_multiple_files=True
        )
        
        if uploaded_files:
            # íƒœê·¸ ì…ë ¥
            all_tags = file_manager.tag_system.get_all_tags()
            existing_tag_names = [tag['name'] for tag in all_tags]
            
            tags_input = st.text_input(
                "íƒœê·¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                help="ê¸°ì¡´ íƒœê·¸: " + ", ".join(existing_tag_names[:10]) if existing_tag_names else "íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
            
            # ë©”íƒ€ë°ì´í„° ì…ë ¥
            with st.expander("ğŸ“ ì¶”ê°€ ë©”íƒ€ë°ì´í„°"):
                description = st.text_area("ì„¤ëª…")
                category = st.selectbox("ì¹´í…Œê³ ë¦¬", ["ë¬¸ì„œ", "ë³´ê³ ì„œ", "ë§¤ë‰´ì–¼", "ê¸°íƒ€"])
                importance = st.slider("ì¤‘ìš”ë„", 1, 5, 3)
            
            if st.button("ğŸ“¥ ì—…ë¡œë“œ ì‹œì‘", type="primary"):
                tags = [tag.strip() for tag in tags_input.split(',')] if tags_input else []
                metadata = {
                    "description": description,
                    "category": category,
                    "importance": importance
                }
                
                success_count = 0
                for uploaded_file in uploaded_files:
                    success, message = file_manager.add_file(uploaded_file, tags, metadata)
                    if success:
                        success_count += 1
                    else:
                        st.error(f"{uploaded_file.name}: {message}")
                
                if success_count > 0:
                    st.success(f"âœ… {success_count}ê°œ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    with tab2:
        render_file_management(file_manager)
    
    with tab3:
        st.header("ğŸ” ê³ ê¸‰ ê²€ìƒ‰")
        
        search_query = st.text_input(
            "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”"
        )
        
        if search_query and st.button("ğŸ” ê²€ìƒ‰", type="primary"):
            with st.spinner("ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                results = file_manager.advanced_search(search_query)
            
            if "error" in results:
                st.error(results["error"])
            else:
                st.write("### ğŸ“ ë‹µë³€")
                st.write(results["answer"])
                
                if results["sources"]:
                    st.write("### ğŸ“„ ì°¸ì¡° ë¬¸ì„œ")
                    for i, source in enumerate(results["sources"]):
                        with st.expander(f"ì†ŒìŠ¤ {i+1}: {source['source_file']} (ìœ ì‚¬ë„: {source['score']:.3f})"):
                            st.write(source["text"])
    
    with tab4:
        st.header("ğŸ·ï¸ íƒœê·¸ ê´€ë¦¬")
        
        # ìƒˆ íƒœê·¸ ìƒì„±
        with st.expander("â• ìƒˆ íƒœê·¸ ìƒì„±"):
            col1, col2 = st.columns(2)
            with col1:
                new_tag_name = st.text_input("íƒœê·¸ ì´ë¦„")
                new_tag_color = st.color_picker("íƒœê·¸ ìƒ‰ìƒ", "#007acc")
            with col2:
                new_tag_desc = st.text_area("íƒœê·¸ ì„¤ëª…", height=100)
            
            if st.button("âœ¨ íƒœê·¸ ìƒì„±"):
                if new_tag_name:
                    success, message = file_manager.tag_system.create_tag(
                        new_tag_name, new_tag_color, new_tag_desc
                    )
                    if success:
                        st.success(message)
                        st.rerun()
                    else:
                        st.error(message)
        
        # ê¸°ì¡´ íƒœê·¸ ëª©ë¡
        st.subheader("ğŸ“‹ ê¸°ì¡´ íƒœê·¸")
        tags = file_manager.tag_system.get_all_tags()
        
        if tags:
            for tag in tags:
                col1, col2, col3 = st.columns([2, 3, 1])
                with col1:
                    st.markdown(f'<span style="background-color: {tag["color"]}; color: white; padding: 4px 8px; border-radius: 4px;">{tag["name"]}</span>', unsafe_allow_html=True)
                with col2:
                    st.write(tag["description"] or "ì„¤ëª… ì—†ìŒ")
                with col3:
                    st.caption(f"ID: {tag['id']}")
        else:
            st.info("ìƒì„±ëœ íƒœê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab5:
        render_dashboard(file_manager)
    
    with tab6:
        st.header("ğŸ“‹ í™œë™ ë¡œê·¸")
        
        # ë¡œê·¸ í•„í„°
        log_limit = st.slider("í‘œì‹œí•  ë¡œê·¸ ìˆ˜", 10, 500, 100)
        
        # í™œë™ ë¡œê·¸ ì¡°íšŒ
        df_logs = file_manager.get_activity_logs(log_limit)
        
        if not df_logs.empty:
            st.dataframe(df_logs, use_container_width=True)
        else:
            st.info("í™œë™ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()