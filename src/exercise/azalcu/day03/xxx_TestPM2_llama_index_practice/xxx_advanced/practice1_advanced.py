# advanced/practice1_advanced.py
import streamlit as st
import os
import sys
from pathlib import Path
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import logging
from datetime import datetime

# ê³µìš© ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent / "shared"))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ê¸‰ ë¬¸ì„œ QA",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentQASystem:
    """ê³ ê¸‰ ë¬¸ì„œ QA ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.api_key = None
        self.index = None
        self.query_engine = None
        self.docs_info = {"count": 0, "total_size": 0, "last_updated": None}
    
    def initialize_llama_index(self, api_key: str) -> bool:
        """LlamaIndex ì´ˆê¸°í™”"""
        try:
            self.api_key = api_key
            
            # LLM ì„¤ì • (ëª¨ë¸ ì„ íƒ ê°€ëŠ¥)
            model_name = st.session_state.get('model_name', 'gpt-3.5-turbo')
            Settings.llm = OpenAI(model=model_name, api_key=api_key, temperature=0.1)
            Settings.embed_model = OpenAIEmbedding(api_key=api_key)
            
            logger.info(f"LlamaIndex initialized with model: {model_name}")
            return True
        except Exception as e:
            logger.error(f"LlamaIndex initialization failed: {e}")
            st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_documents(self, docs_folder: str, file_extensions: list = None) -> tuple:
        """ë¬¸ì„œ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if not os.path.exists(docs_folder):
                return None, f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_folder}"
            
            # íŒŒì¼ í•„í„°ë§
            if file_extensions:
                reader = SimpleDirectoryReader(
                    docs_folder,
                    required_exts=file_extensions
                )
            else:
                reader = SimpleDirectoryReader(docs_folder)
            
            documents = reader.load_data()
            
            if not documents:
                return None, f"ì§€ì›ë˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤: {docs_folder}"
            
            # ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
            total_size = sum(len(doc.text) for doc in documents)
            self.docs_info = {
                "count": len(documents),
                "total_size": total_size,
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            logger.info(f"Loaded {len(documents)} documents, total size: {total_size}")
            return documents, f"ì„±ê³µì ìœ¼ë¡œ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"Document loading failed: {e}")
            return None, f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}"
    
    def create_index(self, documents) -> bool:
        """ì¸ë±ìŠ¤ ìƒì„±"""
        try:
            # ì²­í¬ ì„¤ì •
            chunk_size = st.session_state.get('chunk_size', 1024)
            chunk_overlap = st.session_state.get('chunk_overlap', 50)
            
            from llama_index.core.node_parser import SentenceSplitter
            splitter = SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            # ì¸ë±ìŠ¤ ìƒì„±
            self.index = VectorStoreIndex.from_documents(
                documents,
                transformations=[splitter]
            )
            
            # ì¿¼ë¦¬ ì—”ì§„ ì„¤ì •
            similarity_top_k = st.session_state.get('similarity_top_k', 3)
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=similarity_top_k,
                response_mode="tree_summarize"
            )
            
            logger.info("Index and query engine created successfully")
            return True
            
        except Exception as e:
            logger.error(f"Index creation failed: {e}")
            st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def query(self, question: str) -> tuple:
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        try:
            if not self.query_engine:
                return None, "ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            response = self.query_engine.query(question)
            
            # ì‘ë‹µ ì •ë³´ ìˆ˜ì§‘
            response_info = {
                "answer": str(response),
                "sources": []
            }
            
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for node in response.source_nodes:
                    source_info = {
                        "text": node.text[:300] + "...",
                        "score": getattr(node, 'score', 0),
                        "metadata": node.metadata
                    }
                    response_info["sources"].append(source_info)
            
            logger.info(f"Query processed: {question[:50]}...")
            return response_info, "ì„±ê³µ"
            
        except Exception as e:
            logger.error(f"Query failed: {e}")
            return None, f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}"

# QA ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_qa_system():
    return DocumentQASystem()

def main():
    st.title("ğŸ“š ê³ ê¸‰ ë¬¸ì„œ QA ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    qa_system = get_qa_system()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", None)
        except:
            api_key = None
        
        if not api_key:
            api_key = st.text_input(
                "OpenAI API Key",
                type="password",
                help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
            )
        
        # ëª¨ë¸ ì„ íƒ
        model_name = st.selectbox(
            "LLM ëª¨ë¸ ì„ íƒ",
            ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"],
            key="model_name"
        )
        
        # ë¬¸ì„œ ì„¤ì •
        st.subheader("ğŸ“ ë¬¸ì„œ ì„¤ì •")
        docs_folder = st.text_input(
            "ë¬¸ì„œ í´ë” ê²½ë¡œ",
            value="../docs",
            help="ë¶„ì„í•  ë¬¸ì„œë“¤ì´ ìˆëŠ” í´ë”"
        )
        
        # íŒŒì¼ í™•ì¥ì í•„í„°
        st.write("ì§€ì›í•  íŒŒì¼ í˜•ì‹:")
        col1, col2 = st.columns(2)
        with col1:
            pdf_support = st.checkbox("PDF", value=True)
            txt_support = st.checkbox("TXT", value=True)
        with col2:
            docx_support = st.checkbox("DOCX", value=True)
            md_support = st.checkbox("MD", value=True)
        
        file_extensions = []
        if pdf_support: file_extensions.append(".pdf")
        if txt_support: file_extensions.append(".txt")
        if docx_support: file_extensions.append(".docx")
        if md_support: file_extensions.append(".md")
        
        # ê³ ê¸‰ ì„¤ì •
        with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
            chunk_size = st.slider("ì²­í¬ í¬ê¸°", 512, 2048, 1024, key="chunk_size")
            chunk_overlap = st.slider("ì²­í¬ ê²¹ì¹¨", 0, 200, 50, key="chunk_overlap")
            similarity_top_k = st.slider("ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜", 1, 10, 3, key="similarity_top_k")
        
        # ë¬¸ì„œ ë¡œë“œ ë²„íŠ¼
        if st.button("ğŸ“ ë¬¸ì„œ ë¡œë“œ & ì¸ë±ì‹±", type="primary"):
            if api_key:
                if qa_system.initialize_llama_index(api_key):
                    with st.spinner("ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•˜ëŠ” ì¤‘..."):
                        # ë¬¸ì„œ ë¡œë“œ
                        documents, load_message = qa_system.load_documents(docs_folder, file_extensions)
                        
                        if documents:
                            st.success(load_message)
                            
                            # ì¸ë±ìŠ¤ ìƒì„±
                            if qa_system.create_index(documents):
                                st.success("âœ… ì¸ë±ì‹± ì™„ë£Œ!")
                                st.session_state.system_ready = True
                            else:
                                st.error("ì¸ë±ì‹± ì‹¤íŒ¨")
                        else:
                            st.error(load_message)
            else:
                st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        if qa_system.docs_info["count"] > 0:
            st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
            st.metric("ë¬¸ì„œ ìˆ˜", qa_system.docs_info["count"])
            st.metric("ì´ í…ìŠ¤íŠ¸ í¬ê¸°", f"{qa_system.docs_info['total_size']:,} ë¬¸ì")
            st.caption(f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {qa_system.docs_info['last_updated']}")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.markdown("""
        ### ğŸ”‘ API í‚¤ ì„¤ì • ë°©ë²•:
        1. [OpenAI Platform](https://platform.openai.com/api-keys)ì—ì„œ API í‚¤ ìƒì„±
        2. ì‚¬ì´ë“œë°”ì— API í‚¤ ì…ë ¥
        3. ë¬¸ì„œ í´ë” ì„¤ì • í›„ 'ë¬¸ì„œ ë¡œë“œ & ì¸ë±ì‹±' í´ë¦­
        """)
        return
    
    if not getattr(st.session_state, 'system_ready', False):
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•´ì£¼ì„¸ìš”.")
        st.markdown("""
        ### ğŸ“‹ ì‹œìŠ¤í…œ íŠ¹ì§•:
        - **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**: GPT-3.5, GPT-4 ë“± ì„ íƒ ê°€ëŠ¥
        - **ê³ ê¸‰ ì²­í‚¹**: ë¬¸ì„œë¥¼ ìµœì  í¬ê¸°ë¡œ ë¶„í• 
        - **ìœ ì‚¬ë„ ê²€ìƒ‰**: ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œ ì²­í¬ ìš°ì„  ê²€ìƒ‰
        - **ì†ŒìŠ¤ ì¶”ì **: ë‹µë³€ì˜ ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ
        - **ì„±ëŠ¥ ìµœì í™”**: ìºì‹± ë° íš¨ìœ¨ì  ì¸ë±ì‹±
        
        ### ğŸ“ ì§€ì› íŒŒì¼ í˜•ì‹:
        - PDF (.pdf) - Adobe PDF ë¬¸ì„œ
        - í…ìŠ¤íŠ¸ (.txt) - í‰ë¬¸ í…ìŠ¤íŠ¸
        - Word (.docx) - Microsoft Word ë¬¸ì„œ  
        - Markdown (.md) - ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ
        """)
        return
    
    # ì§ˆë¬¸ ë‹µë³€ ì„¹ì…˜
    st.markdown("### ğŸ’¬ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”")
    
    # ë¯¸ë¦¬ ì •ì˜ëœ ì§ˆë¬¸ ë²„íŠ¼ë“¤
    st.markdown("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:**")
    col1, col2, col3, col4 = st.columns(4)
    
    example_questions = [
        "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€?",
        "í•µì‹¬ í‚¤ì›Œë“œëŠ”?", 
        "ê²°ë¡ ì´ë‚˜ ìš”ì•½ì€?",
        "ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ëŠ”?"
    ]
    
    for i, (col, question) in enumerate(zip([col1, col2, col3, col4], example_questions)):
        with col:
            if st.button(question, key=f"example_{i}"):
                st.session_state.example_question = question
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_area(
        "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
        value=getattr(st.session_state, 'example_question', ''),
        placeholder="ì˜ˆ: ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        height=100,
        key="main_question"
    )
    
    # ì§ˆë¬¸ ì²˜ë¦¬
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("ğŸ” ì§ˆë¬¸í•˜ê¸°", type="primary", disabled=not question):
            process_question = True
        else:
            process_question = False
    
    with col2:
        if st.button("ğŸ—‘ï¸ ì§ˆë¬¸ ì§€ìš°ê¸°"):
            st.session_state.example_question = ""
            st.session_state.main_question = ""
            st.rerun()
    
    if process_question and question:
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            response_info, status = qa_system.query(question)
        
        if response_info:
            # ë‹µë³€ í‘œì‹œ
            st.markdown("### ğŸ“ ë‹µë³€")
            st.write(response_info["answer"])
            
            # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
            if response_info["sources"]:
                st.markdown("### ğŸ“„ ì°¸ì¡° ë¬¸ì„œ")
                
                for i, source in enumerate(response_info["sources"]):
                    with st.expander(f"ì†ŒìŠ¤ {i+1} (ìœ ì‚¬ë„: {source['score']:.3f})", expanded=False):
                        st.write("**ë‚´ìš©:**")
                        st.write(source["text"])
                        
                        if source["metadata"]:
                            st.write("**ë©”íƒ€ë°ì´í„°:**")
                            st.json(source["metadata"])
            
            # ì§ˆë¬¸ ê¸°ë¡ (ì„ íƒì‚¬í•­)
            if 'question_history' not in st.session_state:
                st.session_state.question_history = []
            
            st.session_state.question_history.append({
                "question": question,
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "answer_length": len(response_info["answer"])
            })
            
            # ìµœê·¼ ì§ˆë¬¸ ê¸°ë¡ í‘œì‹œ
            if len(st.session_state.question_history) > 1:
                with st.expander("ğŸ•’ ìµœê·¼ ì§ˆë¬¸ ê¸°ë¡"):
                    for i, record in enumerate(reversed(st.session_state.question_history[-5:])):
                        st.write(f"**{record['timestamp']}:** {record['question']}")
                        st.caption(f"ë‹µë³€ ê¸¸ì´: {record['answer_length']}ì")
                        if i < 4: st.divider()
        else:
            st.error(status)

if __name__ == "__main__":
    main()