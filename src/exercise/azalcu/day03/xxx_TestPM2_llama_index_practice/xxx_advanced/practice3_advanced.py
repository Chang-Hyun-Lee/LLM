# advanced/practice3_advanced.py
import streamlit as st
import os
import sys
import hashlib
import mimetypes
import threading
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, Document
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import logging
import json
import pickle

# ê³µìš© ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent / "shared"))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ê¸‰ íŒŒì¼ ì—…ë¡œë“œ & ë¶„ì„",
    page_icon="ğŸ“‚",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FileAnalysisSystem:
    """ê³ ê¸‰ íŒŒì¼ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.api_key = None
        self.file_registry = {}  # íŒŒì¼ ë©”íƒ€ë°ì´í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬
        self.analysis_cache = {}  # ë¶„ì„ ê²°ê³¼ ìºì‹œ
        self.processing_queue = []  # ì²˜ë¦¬ ëŒ€ê¸°ì—´
        self.base_upload_dir = Path("../uploads_advanced")
        self.base_upload_dir.mkdir(exist_ok=True)
        
        # ë¶„ì„ ë©”íŠ¸ë¦­
        self.analytics = {
            "total_files_processed": 0,
            "total_processing_time": 0,
            "analysis_history": [],
            "error_count": 0,
            "success_rate": 1.0
        }
    
    def initialize_system(self, api_key: str, model_name: str = "gpt-3.5-turbo") -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.api_key = api_key
            
            # LLM ì„¤ì •
            Settings.llm = OpenAI(
                model=model_name,
                api_key=api_key,
                temperature=0.1,
                max_tokens=2000
            )
            Settings.embed_model = OpenAIEmbedding(api_key=api_key)
            
            # ê¸°ì¡´ íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ
            self._load_file_registry()
            
            logger.info(f"File analysis system initialized with {model_name}")
            return True
        except Exception as e:
            logger.error(f"System initialization failed: {e}")
            return False
    
    def _load_file_registry(self):
        """íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ"""
        registry_file = self.base_upload_dir / "file_registry.json"
        if registry_file.exists():
            try:
                with open(registry_file, 'r', encoding='utf-8') as f:
                    self.file_registry = json.load(f)
                logger.info(f"Loaded {len(self.file_registry)} files from registry")
            except Exception as e:
                logger.error(f"Failed to load file registry: {e}")
    
    def _save_file_registry(self):
        """íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥"""
        registry_file = self.base_upload_dir / "file_registry.json"
        try:
            with open(registry_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_registry, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to save file registry: {e}")
    
    def _calculate_file_hash(self, file_content: bytes) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ê²€ì‚¬ìš©)"""
        return hashlib.md5(file_content).hexdigest()
    
    def process_uploaded_files(self, uploaded_files: List) -> Dict[str, Any]:
        """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        results = {
            "successful": [],
            "failed": [],
            "duplicates": [],
            "total_size": 0,
            "processing_time": 0
        }
        
        start_time = datetime.now()
        
        for uploaded_file in uploaded_files:
            try:
                # íŒŒì¼ ë‚´ìš© ì½ê¸°
                file_content = uploaded_file.getbuffer()
                file_hash = self._calculate_file_hash(file_content)
                
                # ì¤‘ë³µ ê²€ì‚¬
                if self._is_duplicate_file(file_hash):
                    results["duplicates"].append(uploaded_file.name)
                    continue
                
                # íŒŒì¼ ì €ì¥
                file_info = self._save_uploaded_file(uploaded_file, file_content, file_hash)
                if file_info:
                    results["successful"].append(file_info)
                    results["total_size"] += file_info["size"]
                else:
                    results["failed"].append(uploaded_file.name)
                    
            except Exception as e:
                logger.error(f"Failed to process {uploaded_file.name}: {e}")
                results["failed"].append(uploaded_file.name)
        
        results["processing_time"] = (datetime.now() - start_time).total_seconds()
        
        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì €ì¥
        self._save_file_registry()
        
        return results
    
    def _is_duplicate_file(self, file_hash: str) -> bool:
        """íŒŒì¼ ì¤‘ë³µ ê²€ì‚¬"""
        for file_info in self.file_registry.values():
            if file_info.get("hash") == file_hash:
                return True
        return False
    
    def _save_uploaded_file(self, uploaded_file, file_content: bytes, file_hash: str) -> Optional[Dict]:
        """ê°œë³„ íŒŒì¼ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            # íŒŒì¼ ê²½ë¡œ ìƒì„± (ë‚ ì§œë³„ í´ë”)
            date_folder = self.base_upload_dir / datetime.now().strftime("%Y%m%d")
            date_folder.mkdir(exist_ok=True)
            
            # ì¤‘ë³µ ì´ë¦„ ì²˜ë¦¬
            file_path = self._get_unique_file_path(date_folder, uploaded_file.name)
            
            # íŒŒì¼ ì €ì¥
            with open(file_path, "wb") as f:
                f.write(file_content)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            file_info = {
                "filename": uploaded_file.name,
                "stored_path": str(file_path),
                "size": len(file_content),
                "hash": file_hash,
                "mime_type": uploaded_file.type or mimetypes.guess_type(uploaded_file.name)[0],
                "upload_timestamp": datetime.now().isoformat(),
                "status": "uploaded"
            }
            
            # íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì¶”ê°€
            file_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uploaded_file.name}"
            self.file_registry[file_id] = file_info
            
            logger.info(f"Saved file: {uploaded_file.name}")
            return file_info
            
        except Exception as e:
            logger.error(f"Failed to save file {uploaded_file.name}: {e}")
            return None
    
    def _get_unique_file_path(self, directory: Path, filename: str) -> Path:
        """ì¤‘ë³µ íŒŒì¼ëª… ì²˜ë¦¬"""
        base_path = directory / filename
        if not base_path.exists():
            return base_path
        
        name, ext = os.path.splitext(filename)
        counter = 1
        
        while True:
            new_path = directory / f"{name}_{counter}{ext}"
            if not new_path.exists():
                return new_path
            counter += 1
    
    def analyze_files_batch(self, file_ids: List[str]) -> Dict[str, Any]:
        """ë°°ì¹˜ íŒŒì¼ ë¶„ì„"""
        results = {
            "analyses": {},
            "summary": {},
            "processing_time": 0,
            "errors": []
        }
        
        start_time = datetime.now()
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¶„ì„ ìˆ˜í–‰
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_file = {
                executor.submit(self._analyze_single_file, file_id): file_id 
                for file_id in file_ids
            }
            
            for future in future_to_file:
                file_id = future_to_file[future]
                try:
                    analysis_result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                    if analysis_result:
                        results["analyses"][file_id] = analysis_result
                except Exception as e:
                    error_msg = f"Analysis failed for {file_id}: {str(e)}"
                    results["errors"].append(error_msg)
                    logger.error(error_msg)
        
        # ì „ì²´ ìš”ì•½ ìƒì„±
        if results["analyses"]:
            results["summary"] = self._generate_batch_summary(results["analyses"])
        
        results["processing_time"] = (datetime.now() - start_time).total_seconds()
        
        # ë¶„ì„ ì´ë ¥ ì—…ë°ì´íŠ¸
        self._update_analytics(results)
        
        return results
    
    def _analyze_single_file(self, file_id: str) -> Optional[Dict]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        try:
            file_info = self.file_registry.get(file_id)
            if not file_info:
                return None
            
            # ìºì‹œ í™•ì¸
            cache_key = f"{file_id}_{file_info['hash']}"
            if cache_key in self.analysis_cache:
                return self.analysis_cache[cache_key]
            
            # ë¬¸ì„œ ë¡œë“œ
            documents = SimpleDirectoryReader(
                input_files=[file_info["stored_path"]]
            ).load_data()
            
            if not documents:
                return None
            
            # ì¸ë±ìŠ¤ ìƒì„±
            index = VectorStoreIndex.from_documents(documents)
            query_engine = index.as_query_engine()
            
            # ë‹¤ì–‘í•œ ë¶„ì„ ìˆ˜í–‰
            analysis_result = {
                "file_info": file_info,
                "content_analysis": {},
                "metadata_analysis": {},
                "quality_metrics": {}
            }
            
            # ë‚´ìš© ë¶„ì„
            analysis_queries = {
                "summary": "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ 3-5ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "main_topics": "ì´ ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œë‚˜ í‚¤ì›Œë“œë“¤ì„ ë‚˜ì—´í•´ì£¼ì„¸ìš”.",
                "document_type": "ì´ ë¬¸ì„œì˜ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš” (ì˜ˆ: ë³´ê³ ì„œ, ë…¼ë¬¸, ë§¤ë‰´ì–¼, ê³„ì•½ì„œ ë“±).",
                "language": "ì´ ë¬¸ì„œì˜ ì£¼ìš” ì–¸ì–´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
                "complexity": "ì´ ë¬¸ì„œì˜ ë‚´ìš© ë³µì¡ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš” (ê°„ë‹¨/ë³´í†µ/ë³µì¡).",
                "key_insights": "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë‚˜ ì¸ì‚¬ì´íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            }
            
            for key, query in analysis_queries.items():
                try:
                    response = query_engine.query(query)
                    analysis_result["content_analysis"][key] = str(response)
                except Exception as e:
                    analysis_result["content_analysis"][key] = f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
            
            # ë©”íƒ€ë°ì´í„° ë¶„ì„
            doc = documents[0]
            analysis_result["metadata_analysis"] = {
                "character_count": len(doc.text),
                "word_count": len(doc.text.split()),
                "paragraph_count": len(doc.text.split('\n\n')),
                "readability_score": self._calculate_readability_score(doc.text)
            }
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­
            analysis_result["quality_metrics"] = {
                "completeness": self._assess_completeness(doc.text),
                "structure_quality": self._assess_structure(doc.text),
                "information_density": self._calculate_information_density(doc.text)
            }
            
            # ë¶„ì„ ì™„ë£Œ ì‹œê°„ ê¸°ë¡
            analysis_result["analysis_timestamp"] = datetime.now().isoformat()
            
            # ìºì‹œì— ì €ì¥
            self.analysis_cache[cache_key] = analysis_result
            
            # íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
            self.file_registry[file_id]["status"] = "analyzed"
            self.file_registry[file_id]["last_analysis"] = datetime.now().isoformat()
            
            logger.info(f"Analysis completed for file: {file_info['filename']}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"Single file analysis failed for {file_id}: {e}")
            return None
    
    def _calculate_readability_score(self, text: str) -> float:
        """ê°€ë…ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            sentences = text.split('.')
            words = text.split()
            
            if len(sentences) == 0 or len(words) == 0:
                return 0.0
            
            avg_sentence_length = len(words) / len(sentences)
            # ê°„ë‹¨í•œ ê°€ë…ì„± ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì½ê¸° ì‰¬ì›€)
            readability = min(100, avg_sentence_length * 2)
            return round(readability, 2)
        except:
            return 0.0
    
    def _assess_completeness(self, text: str) -> float:
        """ë¬¸ì„œ ì™„ì„±ë„ í‰ê°€"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ê¸¸ì´, êµ¬ì¡° ìš”ì†Œ ì¡´ì¬ ì—¬ë¶€
        score = 0.0
        
        if len(text) > 100:
            score += 0.3
        if len(text) > 1000:
            score += 0.3
        if any(marker in text.lower() for marker in ['ê²°ë¡ ', 'conclusion', 'ìš”ì•½', 'summary']):
            score += 0.2
        if any(marker in text for marker in ['.', '!', '?']):
            score += 0.2
        
        return min(1.0, score)
    
    def _assess_structure(self, text: str) -> float:
        """ë¬¸ì„œ êµ¬ì¡° í’ˆì§ˆ í‰ê°€"""
        score = 0.0
        
        # ë‹¨ë½ êµ¬ì¡°
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            score += 0.4
        
        # ì œëª© êµ¬ì¡° (ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼)
        if '#' in text:
            score += 0.3
        
        # ëª©ë¡ êµ¬ì¡°
        if any(marker in text for marker in ['- ', '* ', '1. ', '2. ']):
            score += 0.3
        
        return min(1.0, score)
    
    def _calculate_information_density(self, text: str) -> float:
        """ì •ë³´ ë°€ë„ ê³„ì‚°"""
        try:
            words = text.split()
            unique_words = set(word.lower().strip('.,!?') for word in words)
            
            if len(words) == 0:
                return 0.0
            
            density = len(unique_words) / len(words)
            return round(density, 3)
        except:
            return 0.0
    
    def _generate_batch_summary(self, analyses: Dict) -> Dict:
        """ë°°ì¹˜ ë¶„ì„ ìš”ì•½ ìƒì„±"""
        summary = {
            "total_files": len(analyses),
            "document_types": {},
            "languages": {},
            "complexity_distribution": {},
            "average_metrics": {},
            "insights": []
        }
        
        total_chars = 0
        total_words = 0
        readability_scores = []
        
        for analysis in analyses.values():
            content = analysis.get("content_analysis", {})
            metadata = analysis.get("metadata_analysis", {})
            
            # ë¬¸ì„œ ìœ í˜• ë¶„í¬
            doc_type = content.get("document_type", "ì•Œ ìˆ˜ ì—†ìŒ")
            summary["document_types"][doc_type] = summary["document_types"].get(doc_type, 0) + 1
            
            # ì–¸ì–´ ë¶„í¬
            language = content.get("language", "ì•Œ ìˆ˜ ì—†ìŒ")
            summary["languages"][language] = summary["languages"].get(language, 0) + 1
            
            # ë³µì¡ë„ ë¶„í¬
            complexity = content.get("complexity", "ì•Œ ìˆ˜ ì—†ìŒ")
            summary["complexity_distribution"][complexity] = summary["complexity_distribution"].get(complexity, 0) + 1
            
            # ë©”íŠ¸ë¦­ ëˆ„ì 
            total_chars += metadata.get("character_count", 0)
            total_words += metadata.get("word_count", 0)
            readability_scores.append(metadata.get("readability_score", 0))
        
        # í‰ê·  ê³„ì‚°
        if len(analyses) > 0:
            summary["average_metrics"] = {
                "avg_characters": round(total_chars / len(analyses)),
                "avg_words": round(total_words / len(analyses)),
                "avg_readability": round(sum(readability_scores) / len(readability_scores), 2)
            }
        
        return summary
    
    def _update_analytics(self, results: Dict):
        """ë¶„ì„ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.analytics["total_files_processed"] += len(results["analyses"])
        self.analytics["total_processing_time"] += results["processing_time"]
        self.analytics["error_count"] += len(results["errors"])
        
        # ì„±ê³µë¥  ê³„ì‚°
        total_attempts = self.analytics["total_files_processed"] + self.analytics["error_count"]
        if total_attempts > 0:
            self.analytics["success_rate"] = self.analytics["total_files_processed"] / total_attempts
        
        # ë¶„ì„ ì´ë ¥ì— ì¶”ê°€
        self.analytics["analysis_history"].append({
            "timestamp": datetime.now().isoformat(),
            "files_processed": len(results["analyses"]),
            "processing_time": results["processing_time"],
            "errors": len(results["errors"])
        })

# ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_analysis_system():
    return FileAnalysisSystem()

def render_analytics_dashboard(system: FileAnalysisSystem):
    """ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë Œë”ë§"""
    st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    
    # ì£¼ìš” ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì´ ì²˜ë¦¬ íŒŒì¼", system.analytics["total_files_processed"])
    
    with col2:
        avg_time = system.analytics["total_processing_time"] / max(1, system.analytics["total_files_processed"])
        st.metric("í‰ê·  ì²˜ë¦¬ ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
    
    with col3:
        st.metric("ì„±ê³µë¥ ", f"{system.analytics['success_rate']*100:.1f}%")
    
    with col4:
        st.metric("ë“±ë¡ëœ íŒŒì¼", len(system.file_registry))
    
    # ì²˜ë¦¬ ì´ë ¥ ì°¨íŠ¸
    if system.analytics["analysis_history"]:
        df_history = pd.DataFrame(system.analytics["analysis_history"])
        df_history['timestamp'] = pd.to_datetime(df_history['timestamp'])
        
        # ì‹œê°„ë³„ ì²˜ë¦¬ëŸ‰ ì°¨íŠ¸
        fig = px.line(df_history, x='timestamp', y='files_processed', 
                     title='ì‹œê°„ë³„ íŒŒì¼ ì²˜ë¦¬ëŸ‰')
        st.plotly_chart(fig, use_container_width=True)

def render_file_registry(system: FileAnalysisSystem):
    """íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í‘œì‹œ"""
    st.subheader("ğŸ“ íŒŒì¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬")
    
    if not system.file_registry:
        st.info("ë“±ë¡ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # íŒŒì¼ ëª©ë¡ì„ DataFrameìœ¼ë¡œ ë³€í™˜
    file_data = []
    for file_id, file_info in system.file_registry.items():
        file_data.append({
            "ID": file_id,
            "íŒŒì¼ëª…": file_info["filename"],
            "í¬ê¸°": f"{file_info['size']//1024}KB",
            "ìƒíƒœ": file_info["status"],
            "ì—…ë¡œë“œ ì‹œê°„": file_info["upload_timestamp"][:19].replace('T', ' '),
            "MIME íƒ€ì…": file_info.get("mime_type", "ì•Œ ìˆ˜ ì—†ìŒ")
        })
    
    df = pd.DataFrame(file_data)
    
    # í•„í„°ë§ ì˜µì…˜
    col1, col2 = st.columns(2)
    with col1:
        status_filter = st.multiselect(
            "ìƒíƒœ í•„í„°",
            options=df["ìƒíƒœ"].unique(),
            default=df["ìƒíƒœ"].unique()
        )
    
    with col2:
        search_term = st.text_input("íŒŒì¼ëª… ê²€ìƒ‰")
    
    # í•„í„° ì ìš©
    if status_filter:
        df = df[df["ìƒíƒœ"].isin(status_filter)]
    
    if search_term:
        df = df[df["íŒŒì¼ëª…"].str.contains(search_term, case=False, na=False)]
    
    # í…Œì´ë¸” í‘œì‹œ
    st.dataframe(df, use_container_width=True)

def main():
    st.title("ğŸ“‚ ê³ ê¸‰ íŒŒì¼ ì—…ë¡œë“œ & ë¶„ì„ ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    analysis_system = get_analysis_system()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", None)
        except:
            api_key = None
        
        if not api_key:
            api_key = st.text_input("OpenAI API Key", type="password")
        
        # ëª¨ë¸ ì„ íƒ
        model_name = st.selectbox(
            "ë¶„ì„ ëª¨ë¸",
            ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"]
        )
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if st.button("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
            if api_key:
                if analysis_system.initialize_system(api_key, model_name):
                    st.success("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
                    st.session_state.system_ready = True
                else:
                    st.error("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨")
            else:
                st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # íŒŒì¼ í†µê³„
        if analysis_system.file_registry:
            st.subheader("ğŸ“Š íŒŒì¼ í†µê³„")
            total_files = len(analysis_system.file_registry)
            total_size = sum(info["size"] for info in analysis_system.file_registry.values())
            
            st.metric("ì´ íŒŒì¼ ìˆ˜", total_files)
            st.metric("ì´ í¬ê¸°", f"{total_size//1024//1024}MB")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    if not getattr(st.session_state, 'system_ready', False):
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        return
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ", 
        "ğŸ” íŒŒì¼ ë¶„ì„", 
        "ğŸ“Š ë¶„ì„ ê²°ê³¼", 
        "ğŸ“ íŒŒì¼ ê´€ë¦¬",
        "ğŸ“ˆ ëŒ€ì‹œë³´ë“œ"
    ])
    
    with tab1:
        st.header("ğŸ“¤ ë°°ì¹˜ íŒŒì¼ ì—…ë¡œë“œ")
        
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_files = st.file_uploader(
            "ë¶„ì„í•  íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'txt', 'docx', 'md', 'html'],
            accept_multiple_files=True,
            help="ìµœëŒ€ 10ê°œ íŒŒì¼ê¹Œì§€ ë™ì‹œ ì—…ë¡œë“œ ê°€ëŠ¥"
        )
        
        if uploaded_files:
            st.write(f"ğŸ“ ì„ íƒëœ íŒŒì¼: {len(uploaded_files)}ê°œ")
            
            # íŒŒì¼ ëª©ë¡ í‘œì‹œ
            for file in uploaded_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                st.write(f"- {file.name} ({file.size//1024}KB)")
            
            if len(uploaded_files) > 5:
                st.write(f"... ì™¸ {len(uploaded_files)-5}ê°œ")
            
            if st.button("ğŸ“¥ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘", type="primary"):
                with st.spinner("íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                    results = analysis_system.process_uploaded_files(uploaded_files)
                
                # ê²°ê³¼ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.success(f"âœ… ì„±ê³µ: {len(results['successful'])}ê°œ")
                
                with col2:
                    if results['failed']:
                        st.error(f"âŒ ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
                
                with col3:
                    if results['duplicates']:
                        st.warning(f"ğŸ”„ ì¤‘ë³µ: {len(results['duplicates'])}ê°œ")
                
                st.info(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {results['processing_time']:.2f}ì´ˆ")
                
                # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ í‘œì‹œ
                if results['failed']:
                    with st.expander("âŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤"):
                        for filename in results['failed']:
                            st.write(f"- {filename}")
    
    with tab2:
        st.header("ğŸ” íŒŒì¼ ë¶„ì„")
        
        if analysis_system.file_registry:
            # ë¶„ì„í•  íŒŒì¼ ì„ íƒ
            available_files = {
                file_id: info["filename"] 
                for file_id, info in analysis_system.file_registry.items()
            }
            
            selected_files = st.multiselect(
                "ë¶„ì„í•  íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì„¸ìš”:",
                options=list(available_files.keys()),
                format_func=lambda x: available_files[x],
                help="ì—¬ëŸ¬ íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ë°°ì¹˜ ë¶„ì„ ê°€ëŠ¥"
            )
            
            if selected_files:
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("ğŸ” ì„ íƒëœ íŒŒì¼ ë¶„ì„", type="primary"):
                        with st.spinner("íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                            analysis_results = analysis_system.analyze_files_batch(selected_files)
                        
                        st.session_state.latest_analysis = analysis_results
                        st.success(f"âœ… {len(analysis_results['analyses'])}ê°œ íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
                        
                        if analysis_results['errors']:
                            st.error(f"âŒ {len(analysis_results['errors'])}ê°œ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨")
                
                with col2:
                    if st.button("ğŸ“‹ ëª¨ë“  íŒŒì¼ ë¶„ì„"):
                        all_file_ids = list(analysis_system.file_registry.keys())
                        with st.spinner("ëª¨ë“  íŒŒì¼ì„ ë¶„ì„í•˜ëŠ” ì¤‘..."):
                            analysis_results = analysis_system.analyze_files_batch(all_file_ids)
                        
                        st.session_state.latest_analysis = analysis_results
                        st.success("âœ… ì „ì²´ íŒŒì¼ ë¶„ì„ ì™„ë£Œ!")
        else:
            st.info("ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    with tab3:
        st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        if hasattr(st.session_state, 'latest_analysis'):
            results = st.session_state.latest_analysis
            
            # ë°°ì¹˜ ìš”ì•½
            if 'summary' in results:
                summary = results['summary']
                
                st.subheader("ğŸ“‹ ë°°ì¹˜ ë¶„ì„ ìš”ì•½")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ë¶„ì„ëœ íŒŒì¼", summary['total_files'])
                
                with col2:
                    avg_chars = summary.get('average_metrics', {}).get('avg_characters', 0)
                    st.metric("í‰ê·  ë¬¸ì ìˆ˜", f"{avg_chars:,}")
                
                with col3:
                    avg_readability = summary.get('average_metrics', {}).get('avg_readability', 0)
                    st.metric("í‰ê·  ê°€ë…ì„± ì ìˆ˜", f"{avg_readability}")
                
                # ë¶„í¬ ì°¨íŠ¸
                if summary.get('document_types'):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        fig = px.pie(
                            values=list(summary['document_types'].values()),
                            names=list(summary['document_types'].keys()),
                            title="ë¬¸ì„œ ìœ í˜• ë¶„í¬"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        if summary.get('complexity_distribution'):
                            fig = px.bar(
                                x=list(summary['complexity_distribution'].keys()),
                                y=list(summary['complexity_distribution'].values()),
                                title="ë³µì¡ë„ ë¶„í¬"
                            )
                            st.plotly_chart(fig, use_container_width=True)
            
            # ê°œë³„ íŒŒì¼ ë¶„ì„ ê²°ê³¼
            st.subheader("ğŸ“„ ê°œë³„ íŒŒì¼ ë¶„ì„")
            
            for file_id, analysis in results.get('analyses', {}).items():
                with st.expander(f"ğŸ“„ {analysis['file_info']['filename']}", expanded=False):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**ğŸ“ ë‚´ìš© ë¶„ì„**")
                        content_analysis = analysis.get('content_analysis', {})
                        
                        if 'summary' in content_analysis:
                            st.write("**ìš”ì•½:**")
                            st.write(content_analysis['summary'])
                        
                        if 'main_topics' in content_analysis:
                            st.write("**ì£¼ìš” ì£¼ì œ:**")
                            st.write(content_analysis['main_topics'])
                    
                    with col2:
                        st.write("**ğŸ“Š ë©”íƒ€ë°ì´í„°**")
                        metadata = analysis.get('metadata_analysis', {})
                        
                        for key, value in metadata.items():
                            st.write(f"**{key}:** {value}")
                        
                        st.write("**í’ˆì§ˆ ë©”íŠ¸ë¦­**")
                        quality = analysis.get('quality_metrics', {})
                        for key, value in quality.items():
                            if isinstance(value, float):
                                st.progress(value, text=f"{key}: {value:.2f}")
        else:
            st.info("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ë¶„ì„ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    with tab4:
        render_file_registry(analysis_system)
    
    with tab5:
        render_analytics_dashboard(analysis_system)

if __name__ == "__main__":
    main()