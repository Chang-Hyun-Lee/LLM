# advanced/practice2_advanced.py
import streamlit as st
import os
import sys
import json
import pickle
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.chat_engine.types import ChatMode
import logging

# ê³µìš© ëª¨ë“ˆ import
sys.path.append(str(Path(__file__).parent.parent / "shared"))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ê¸‰ ë©€í‹°í„´ ì±„íŒ…",
    page_icon="ğŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedChatSystem:
    """ê³ ê¸‰ ë©€í‹°í„´ ì±„íŒ… ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.api_key = None
        self.index = None
        self.chat_engines = {}  # ë‹¤ì¤‘ ì±„íŒ… ì—”ì§„ ì§€ì›
        self.conversation_history = []
        self.system_metrics = {
            "total_questions": 0,
            "average_response_time": 0,
            "session_start": datetime.now()
        }
    
    def initialize_system(self, api_key: str, model_name: str = "gpt-3.5-turbo") -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            self.api_key = api_key
            
            # LLM ì„¤ì •
            Settings.llm = OpenAI(
                model=model_name, 
                api_key=api_key,
                temperature=st.session_state.get('temperature', 0.1),
                max_tokens=st.session_state.get('max_tokens', 1000)
            )
            Settings.embed_model = OpenAIEmbedding(api_key=api_key)
            
            logger.info(f"System initialized with model: {model_name}")
            return True
        except Exception as e:
            logger.error(f"System initialization failed: {e}")
            st.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_and_index_documents(self, docs_folder: str) -> tuple:
        """ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ì‹±"""
        try:
            if not os.path.exists(docs_folder):
                return False, f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_folder}"
            
            # ë¬¸ì„œ ë¡œë“œ
            documents = SimpleDirectoryReader(docs_folder).load_data()
            if not documents:
                return False, "ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê°•í™”
            for i, doc in enumerate(documents):
                doc.metadata.update({
                    'doc_id': i,
                    'load_timestamp': datetime.now().isoformat(),
                    'char_count': len(doc.text),
                    'word_count': len(doc.text.split())
                })
            
            # ì¸ë±ìŠ¤ ìƒì„±
            from llama_index.core.node_parser import SentenceSplitter
            
            chunk_size = st.session_state.get('chunk_size', 1024)
            chunk_overlap = st.session_state.get('chunk_overlap', 50)
            
            splitter = SentenceSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            self.index = VectorStoreIndex.from_documents(
                documents,
                transformations=[splitter]
            )
            
            # ì¸ë±ìŠ¤ ì •ë³´ ì €ì¥
            st.session_state.index_info = {
                'document_count': len(documents),
                'total_chars': sum(len(doc.text) for doc in documents),
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            logger.info(f"Indexed {len(documents)} documents")
            return True, f"ì„±ê³µì ìœ¼ë¡œ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            logger.error(f"Document indexing failed: {e}")
            return False, f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}"
    
    def create_chat_engine(
        self, 
        engine_name: str,
        chat_mode: str = "context",
        memory_limit: int = 1500,
        system_prompt: str = None
    ) -> bool:
        """ì±„íŒ… ì—”ì§„ ìƒì„±"""
        try:
            if not self.index:
                return False
            
            # ë©”ëª¨ë¦¬ ì„¤ì •
            memory = ChatMemoryBuffer.from_defaults(token_limit=memory_limit)
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if not system_prompt:
                system_prompt = st.session_state.get('custom_system_prompt', self._default_system_prompt())
            
            # ì±„íŒ… ëª¨ë“œ ë§¤í•‘
            mode_mapping = {
                "context": ChatMode.CONTEXT,
                "condense_question": ChatMode.CONDENSE_QUESTION,
                "react": ChatMode.REACT
            }
            
            # ì±„íŒ… ì—”ì§„ ìƒì„±
            self.chat_engines[engine_name] = self.index.as_chat_engine(
                chat_mode=mode_mapping.get(chat_mode, ChatMode.CONTEXT),
                memory=memory,
                system_prompt=system_prompt,
                similarity_top_k=st.session_state.get('similarity_top_k', 3)
            )
            
            logger.info(f"Created chat engine: {engine_name}")
            return True
            
        except Exception as e:
            logger.error(f"Chat engine creation failed: {e}")
            st.error(f"ì±„íŒ… ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def _default_system_prompt(self) -> str:
        """ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

1. ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª…í™•íˆ 'ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ'ì„ í‘œì‹œí•˜ì„¸ìš”.
3. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ í•˜ì„¸ìš”.
4. ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì¸ìš©ë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
5. ì‚¬ìš©ìì˜ í›„ì† ì§ˆë¬¸ì— ëŒ€í•´ ë§¥ë½ì„ ìœ ì§€í•˜ë©° ë‹µë³€í•˜ì„¸ìš”."""
    
    def chat(self, message: str, engine_name: str = "default") -> tuple:
        """ì±„íŒ… ì²˜ë¦¬"""
        try:
            start_time = datetime.now()
            
            if engine_name not in self.chat_engines:
                return None, "ì±„íŒ… ì—”ì§„ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
            # ì±„íŒ… ì—”ì§„ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            response = self.chat_engines[engine_name].chat(message)
            
            # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            response_time = (datetime.now() - start_time).total_seconds()
            
            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            conversation_entry = {
                'timestamp': datetime.now().isoformat(),
                'user_message': message,
                'assistant_response': str(response),
                'response_time': response_time,
                'engine_used': engine_name
            }
            
            # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
            if hasattr(response, 'source_nodes') and response.source_nodes:
                conversation_entry['sources'] = [
                    {
                        'text': node.text[:200] + "...",
                        'score': getattr(node, 'score', 0),
                        'metadata': node.metadata
                    }
                    for node in response.source_nodes
                ]
            
            self.conversation_history.append(conversation_entry)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_metrics(response_time)
            
            logger.info(f"Chat processed in {response_time:.2f}s")
            return conversation_entry, "ì„±ê³µ"
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return None, f"ì±„íŒ… ì‹¤íŒ¨: {str(e)}"
    
    def _update_metrics(self, response_time: float):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.system_metrics["total_questions"] += 1
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        current_avg = self.system_metrics["average_response_time"]
        total_questions = self.system_metrics["total_questions"]
        
        new_avg = ((current_avg * (total_questions - 1)) + response_time) / total_questions
        self.system_metrics["average_response_time"] = new_avg
    
    def export_conversation(self) -> str:
        """ëŒ€í™” ë‚´ìš© JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'system_metrics': self.system_metrics,
                'conversation_history': self.conversation_history
            }
            return json.dumps(export_data, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Export failed: {e}")
            return f"ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}"
    
    def reset_conversation(self, engine_name: str = None):
        """ëŒ€í™” ì´ˆê¸°í™”"""
        if engine_name and engine_name in self.chat_engines:
            # íŠ¹ì • ì—”ì§„ë§Œ ì´ˆê¸°í™”
            del self.chat_engines[engine_name]
        else:
            # ì „ì²´ ì´ˆê¸°í™”
            self.chat_engines.clear()
            self.conversation_history.clear()
            self.system_metrics = {
                "total_questions": 0,
                "average_response_time": 0,
                "session_start": datetime.now()
            }

# ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
@st.cache_resource
def get_chat_system():
    return AdvancedChatSystem()

def main():
    st.title("ğŸ’¬ ê³ ê¸‰ ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    chat_system = get_chat_system()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'current_engine' not in st.session_state:
        st.session_state.current_engine = "default"
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", None)
        except:
            api_key = None
        
        if not api_key:
            api_key = st.text_input("OpenAI API Key", type="password")
        
        # ëª¨ë¸ ì„¤ì •
        col1, col2 = st.columns(2)
        with col1:
            model_name = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"],
                key="model_name"
            )
        
        with col2:
            temperature = st.slider("Temperature", 0.0, 1.0, 0.1, 0.1, key="temperature")
        
        max_tokens = st.slider("Max Tokens", 100, 2000, 1000, 100, key="max_tokens")
        
        # ë¬¸ì„œ ì„¤ì •
        st.subheader("ğŸ“ ë¬¸ì„œ ì„¤ì •")
        docs_folder = st.text_input("ë¬¸ì„œ í´ë”", value="../docs")
        
        # ê³ ê¸‰ ì„¤ì •
        with st.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
            chunk_size = st.slider("ì²­í¬ í¬ê¸°", 512, 2048, 1024, key="chunk_size")
            chunk_overlap = st.slider("ì²­í¬ ê²¹ì¹¨", 0, 200, 50, key="chunk_overlap")
            similarity_top_k = st.slider("ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜", 1, 10, 3, key="similarity_top_k")
        
        # ì±„íŒ… ì—”ì§„ ì„¤ì •
        st.subheader("ğŸ¤– ì±„íŒ… ì—”ì§„")
        chat_mode = st.selectbox(
            "ì±„íŒ… ëª¨ë“œ",
            ["context", "condense_question", "react"],
            help="context: ë¬¸ë§¥ ê¸°ë°˜, condense_question: ì§ˆë¬¸ ì••ì¶•, react: ì¶”ë¡  ê¸°ë°˜"
        )
        
        memory_limit = st.slider("ë©”ëª¨ë¦¬ ì œí•œ", 500, 3000, 1500)
        
        # ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        with st.expander("ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •"):
            custom_prompt = st.text_area(
                "ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                value="",
                height=150,
                help="ë¹„ì›Œë‘ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©",
                key="custom_system_prompt"
            )
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”", type="primary"):
            if api_key:
                if chat_system.initialize_system(api_key, model_name):
                    with st.spinner("ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ëŠ” ì¤‘..."):
                        success, message = chat_system.load_and_index_documents(docs_folder)
                        
                        if success:
                            st.success(message)
                            
                            # ì±„íŒ… ì—”ì§„ ìƒì„±
                            if chat_system.create_chat_engine(
                                "default", 
                                chat_mode, 
                                memory_limit,
                                custom_prompt or None
                            ):
                                st.success("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
                                st.session_state.system_ready = True
                        else:
                            st.error(message)
            else:
                st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        if hasattr(st.session_state, 'index_info'):
            st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
            info = st.session_state.index_info
            st.metric("ë¬¸ì„œ ìˆ˜", info['document_count'])
            st.metric("ì´ ë¬¸ì ìˆ˜", f"{info['total_chars']:,}")
            st.caption(f"ì¸ë±ì‹±: {info['created_at']}")
            
            # ëŒ€í™” í†µê³„
            metrics = chat_system.system_metrics
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ì´ ì§ˆë¬¸ ìˆ˜", metrics['total_questions'])
            with col2:
                avg_time = metrics['average_response_time']
                st.metric("í‰ê·  ì‘ë‹µì‹œê°„", f"{avg_time:.2f}ì´ˆ")
        
        # ëŒ€í™” ê´€ë¦¬
        st.subheader("ğŸ”„ ëŒ€í™” ê´€ë¦¬")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
                st.session_state.messages = []
                chat_system.reset_conversation()
                st.rerun()
        
        with col2:
            if st.button("ğŸ’¾ ëŒ€í™” ë‚´ë³´ë‚´ê¸°"):
                if chat_system.conversation_history:
                    exported_data = chat_system.export_conversation()
                    st.download_button(
                        "ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ",
                        data=exported_data,
                        file_name=f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        st.markdown("""
        ### ğŸ”‘ API í‚¤ ì„¤ì • í›„ ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
        - **ë‹¤ì¤‘ ì±„íŒ… ëª¨ë“œ**: Context, Condense Question, ReAct
        - **ê³ ê¸‰ ë©”ëª¨ë¦¬ ê´€ë¦¬**: ëŒ€í™” ë§¥ë½ ìœ ì§€
        - **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­**: ì‘ë‹µ ì‹œê°„, ì§ˆë¬¸ ìˆ˜ ì¶”ì 
        - **ëŒ€í™” ë‚´ë³´ë‚´ê¸°**: JSON í˜•íƒœë¡œ ëŒ€í™” ê¸°ë¡ ì €ì¥
        - **ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸**: AI ì‘ë‹µ ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•
        """)
        return
    
    if not getattr(st.session_state, 'system_ready', False):
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
        
        # ê¸°ëŠ¥ ì†Œê°œ
        tab1, tab2, tab3 = st.tabs(["ğŸš€ ì£¼ìš” ê¸°ëŠ¥", "ğŸ¯ ì±„íŒ… ëª¨ë“œ", "ğŸ“Š ê³ ê¸‰ ê¸°ëŠ¥"])
        
        with tab1:
            st.markdown("""
            ### ğŸŒŸ ê³ ê¸‰ ë©€í‹°í„´ ëŒ€í™” ì‹œìŠ¤í…œì˜ íŠ¹ì§•:
            
            **ğŸ’¬ ìŠ¤ë§ˆíŠ¸ ëŒ€í™” ê´€ë¦¬**
            - ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ë©° ì¼ê´€ì„± ìˆëŠ” ë‹µë³€
            - ë‹¤ì–‘í•œ ì±„íŒ… ëª¨ë“œ ì§€ì›
            - ì‹¤ì‹œê°„ ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
            
            **ğŸ§  ì§€ëŠ¥í˜• ë¬¸ì„œ ì´í•´**  
            - ë¬¸ì„œ ì „ì²´ë¥¼ ì´í•´í•˜ê³  ì—°ê´€ì„± ë¶„ì„
            - ì •í™•í•œ ì†ŒìŠ¤ ì¶”ì  ë° ì¸ìš©
            - ì²­í¬ í¬ê¸° ìµœì í™”ë¡œ ì •í™•ë„ í–¥ìƒ
            
            **ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•**
            - ì‚¬ìš©ì ì •ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            - ëª¨ë¸ ì„ íƒ ë° íŒŒë¼ë¯¸í„° ì¡°ì •
            - ë©”ëª¨ë¦¬ í¬ê¸° ì¡°ì ˆ
            """)
        
        with tab2:
            st.markdown("""
            ### ğŸ¯ ì±„íŒ… ëª¨ë“œ ì„¤ëª…:
            
            **Context ëª¨ë“œ**
            - ë¬¸ì„œ ë§¥ë½ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
            - ì¼ë°˜ì ì¸ ì§ˆë¬¸ë‹µë³€ì— ì í•©
            - ë¹ ë¥¸ ì‘ë‹µ ì†ë„
            
            **Condense Question ëª¨ë“œ**
            - ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•´ ì§ˆë¬¸ì„ ì¬êµ¬ì„±
            - ë³µì¡í•œ ì—°ì† ì§ˆë¬¸ì— íš¨ê³¼ì 
            - ëŒ€í™” íë¦„ ìµœì í™”
            
            **ReAct ëª¨ë“œ**
            - ì¶”ë¡ ê³¼ í–‰ë™ì„ ê²°í•©
            - ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì— ì í•©
            - ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì • í‘œì‹œ
            """)
        
        with tab3:
            st.markdown("""
            ### ğŸ“Š ê³ ê¸‰ ê¸°ëŠ¥ë“¤:
            
            **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­**
            - ì´ ì§ˆë¬¸ ìˆ˜ ì¶”ì 
            - í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            - ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            
            **ëŒ€í™” ê´€ë¦¬**
            - ëŒ€í™” ê¸°ë¡ JSON ë‚´ë³´ë‚´ê¸°
            - ì„ íƒì  ëŒ€í™” ì´ˆê¸°í™”
            - ì‹œê°„ëŒ€ë³„ ëŒ€í™” ë¶„ì„
            
            **ì†ŒìŠ¤ ì¶”ì **
            - ë‹µë³€ ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ
            - ìœ ì‚¬ë„ ì ìˆ˜ ì œê³µ
            - ë©”íƒ€ë°ì´í„° ì •ë³´ í¬í•¨
            """)
        
        return
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("### ğŸ’¬ ë¬¸ì„œ ê¸°ë°˜ ëŒ€í™”")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ (Assistant ë©”ì‹œì§€ì˜ ê²½ìš°)
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("ğŸ“„ ì°¸ì¡° ë¬¸ì„œ", expanded=False):
                    for i, source in enumerate(message["sources"]):
                        st.write(f"**ì†ŒìŠ¤ {i+1}** (ìœ ì‚¬ë„: {source['score']:.3f})")
                        st.write(source["text"])
                        if source["metadata"]:
                            st.caption(f"ë©”íƒ€ë°ì´í„°: {source['metadata']}")
                        st.divider()
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                result, status = chat_system.chat(prompt, st.session_state.current_engine)
            
            if result:
                # ë‹µë³€ í‘œì‹œ
                st.markdown(result["assistant_response"])
                
                # ë©”ì‹œì§€ì— ì†ŒìŠ¤ ì •ë³´ í¬í•¨í•´ì„œ ì €ì¥
                assistant_message = {
                    "role": "assistant", 
                    "content": result["assistant_response"]
                }
                
                if "sources" in result:
                    assistant_message["sources"] = result["sources"]
                    
                    # ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
                    with st.expander("ğŸ“„ ì°¸ì¡° ë¬¸ì„œ", expanded=False):
                        for i, source in enumerate(result["sources"]):
                            st.write(f"**ì†ŒìŠ¤ {i+1}** (ìœ ì‚¬ë„: {source['score']:.3f})")
                            st.write(source["text"])
                            if source["metadata"]:
                                st.caption(f"ë©”íƒ€ë°ì´í„°: {source['metadata']}")
                            st.divider()
                
                st.session_state.messages.append(assistant_message)
                
                # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                st.caption(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ")
                
            else:
                error_msg = f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {status}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()