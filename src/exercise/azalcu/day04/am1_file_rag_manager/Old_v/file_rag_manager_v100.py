import streamlit as st
import os
from datetime import datetime
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.core.schema import Document
from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter
from llama_index.core.ingestion import IngestionPipeline

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—…ë¡œë“œíŒŒì¼ê¸°ë°˜ì˜ ì±—ë´‡", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# âœ… ê°•ì œë¡œ ê¸°ë³¸ ë©”ì‹œì§€ í‘œì‹œ (ë””ë²„ê¹…ìš©)
st.write("âœ… Streamlit ì•±ì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!")

# ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 0.8rem 1rem;
        border-radius: 10px;
        margin: 0.5rem 0 1rem 0;
        text-align: center;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        position: sticky;
        top: 0;
        z-index: 100;
    }
    
    .main-header h1 {
        margin: 0.3rem 0;
        font-size: 1.8rem;
        font-weight: 600;
    }
    
    .main-header p {
        margin: 0.2rem 0;
        font-size: 0.9rem;
        opacity: 0.9;
    }
    
    .file-card {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
    }
    
    .status-success {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 0.75rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
    
    .status-warning {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 0.75rem;
        border-radius: 5px;
        margin: 0.5rem 0;
    }
    
    /* íƒ­ ê°€ì‹œì„± ëŒ€í­ ê°œì„  */
    .stTabs [data-baseweb="tab-list"] {
        gap: 12px;
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        padding: 1rem;
        border-radius: 15px;
        border: 3px solid #dee2e6;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
    }
    
    .stTabs [data-baseweb="tab"] {
        height: 70px;
        padding: 1.2rem 2.5rem;
        background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
        border-radius: 12px;
        font-size: 18px;
        font-weight: 700;
        border: 3px solid #6c757d;
        transition: all 0.3s ease;
        color: #495057 !important;
        box-shadow: 0 3px 6px rgba(0, 0, 0, 0.15);
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
        color: white !important;
        border: 3px solid #667eea !important;
        box-shadow: 0 6px 15px rgba(102, 126, 234, 0.4) !important;
        transform: translateY(-3px);
    }
    
    .stTabs [data-baseweb="tab"]:hover {
        background: linear-gradient(135deg, #e9ecef 0%, #dee2e6 100%);
        border-color: #495057;
        transform: translateY(-2px);
        box-shadow: 0 5px 12px rgba(0, 0, 0, 0.2);
        color: #212529 !important;
    }
    
    /* íƒ­ ë‚´ìš© êµ¬ë¶„ì„  */
    .stTabs [data-baseweb="tab-panel"] {
        background-color: #ffffff;
        border: 2px solid #e9ecef;
        border-radius: 12px;
        padding: 2rem;
        margin-top: 1rem;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.05);
    }
</style>
""", unsafe_allow_html=True)

# ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ ì •ì˜
SUPPORTED_EXTENSIONS = {
    '.txt': 'text/plain',
    '.md': 'text/markdown',
    '.py': 'text/x-python',
    '.js': 'text/javascript',
    '.html': 'text/html',
    '.css': 'text/css',
    '.json': 'application/json',
    '.csv': 'text/csv',
    '.pdf': 'application/pdf',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.doc': 'application/msword'
}

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = "./data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = {}
if 'openai_api_key' not in st.session_state:
    st.session_state.openai_api_key = ""

def get_file_extension(filename):
    """íŒŒì¼ í™•ì¥ì ì¶”ì¶œ"""
    return os.path.splitext(filename)[1].lower()

def is_supported_file(filename):
    """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸"""
    return get_file_extension(filename) in SUPPORTED_EXTENSIONS

def generate_unique_filename(filename):
    """ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±"""
    name, ext = os.path.splitext(filename)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{name}_{timestamp}{ext}"

def save_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ì„ data ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    try:
        # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¸
        if not is_supported_file(uploaded_file.name):
            return None, f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {get_file_extension(uploaded_file.name)}"
        
        # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        unique_filename = generate_unique_filename(uploaded_file.name)
        file_path = os.path.join(DATA_DIR, unique_filename)
        
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        return file_path, f"íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {unique_filename}"
    except Exception as e:
        return None, f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def get_file_info(file_path):
    """íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        stat = os.stat(file_path)
        return {
            'name': os.path.basename(file_path),
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime),
            'path': file_path,
            'extension': get_file_extension(file_path)
        }
    except Exception as e:
        return None

def delete_file(file_path):
    """íŒŒì¼ ì‚­ì œ"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            return True
        return False
    except Exception as e:
        st.error(f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def setup_llama_index():
    """LlamaIndex ì„¤ì • (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)"""
    if not st.session_state.openai_api_key:
        return False, "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        # API í‚¤ê°€ ìˆì„ ë•Œë§Œ import (ì„ íƒì  import)
        from llama_index.llms.openai import OpenAI
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        os.environ["OPENAI_API_KEY"] = st.session_state.openai_api_key
        
        # LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.1)
        Settings.embed_model = OpenAIEmbedding()
        
        # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
        Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
        
        return True, "LlamaIndex ì„¤ì • ì™„ë£Œ"
    except ImportError:
        return False, "OpenAI ê´€ë ¨ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    except Exception as e:
        return False, f"LlamaIndex ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}"

def analyze_documents_basic():
    """ê¸°ë³¸ ë¬¸ì„œ ë¶„ì„ (API í‚¤ ë¶ˆí•„ìš”)"""
    try:
        # ì €ì¥ëœ íŒŒì¼ í™•ì¸
        files = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]
        if not files:
            return "ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # SimpleDirectoryReaderë¡œ ë¬¸ì„œ ë¡œë“œ
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        
        if not documents:
            return "ë¬¸ì„œë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì›ë˜ëŠ” í˜•ì‹(.txt, .md ë“±)ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ë¬¸ì„œ ê¸°ë³¸ ì •ë³´
        result = f"ğŸ“ **ê¸°ë³¸ ë¶„ì„ ê²°ê³¼**\n\nì´ {len(documents)} ê°œì˜ ë¬¸ì„œë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.\n\n"
        
        for i, doc in enumerate(documents):
            result += f"**ğŸ“„ ë¬¸ì„œ {i+1}:**\n"
            result += f"- í…ìŠ¤íŠ¸ ê¸¸ì´: {len(doc.text):,} ë¬¸ì\n"
            if hasattr(doc, 'metadata') and doc.metadata:
                result += f"- ë©”íƒ€ë°ì´í„°: {doc.metadata}\n"
            # ë¯¸ë¦¬ë³´ê¸° (ê¸´ í…ìŠ¤íŠ¸ëŠ” ì¤„ì„)
            preview = doc.text[:200].replace('\n', ' ')
            result += f"- ë¯¸ë¦¬ë³´ê¸°: {preview}...\n\n"
        
        return result
        
    except Exception as e:
        return f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def analyze_with_vector_index():
    """ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ ë¶„ì„ (API í‚¤ í•„ìš”)"""
    try:
        # LlamaIndex ì„¤ì •
        setup_success, setup_message = setup_llama_index()
        if not setup_success:
            return f"âŒ {setup_message}\n\nğŸ’¡ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # ë¬¸ì„œ ë¡œë“œ
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        
        if not documents:
            # ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
            doc = Document(text="ì´ê²ƒì€ ìƒì„±í˜• AI ê³¼ì •ì…ë‹ˆë‹¤. KOSAì—ì„œ ì£¼ê´€í•©ë‹ˆë‹¤.")
            documents = [doc, Document(text="ì˜¤ëŠ˜ì€ llama_indexì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")]
            st.info("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ì–´ ìƒ˜í”Œ ë¬¸ì„œë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
        with st.spinner("ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì¤‘..."):
            index = VectorStoreIndex.from_documents(documents)
        
        # ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
        query_engine = index.as_query_engine()
        
        # ë¶„ì„ ì§ˆë¬¸ë“¤
        questions = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.",
            "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”? í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.",
            "ë¬¸ì„œì— ì–¸ê¸‰ëœ ê¸°ê´€ì´ë‚˜ ì¡°ì§ì´ ìˆë‚˜ìš”? í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”."
        ]
        
        results = []
        for i, question in enumerate(questions):
            try:
                with st.spinner(f"ì§ˆë¬¸ {i+1} ë¶„ì„ ì¤‘..."):
                    response = query_engine.query(question)
                results.append(f"**Q{i+1}: {question}**\n\nA: {response}\n\n")
            except Exception as e:
                results.append(f"**Q{i+1}: {question}**\n\nA: ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n")
        
        return "ğŸ¤– **AI ë²¡í„° ë¶„ì„ ê²°ê³¼:**\n\n" + "".join(results)
        
    except Exception as e:
        return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def analyze_with_ingestion_pipeline():
    """Ingestion Pipelineì„ ì‚¬ìš©í•œ ë¶„ì„ (API í‚¤ ë¶ˆí•„ìš”)"""
    try:
        documents = SimpleDirectoryReader(DATA_DIR).load_data()
        
        if not documents:
            return "ë¶„ì„í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        # Ingestion Pipeline ìƒì„±
        with st.spinner("Pipeline ì²˜ë¦¬ ì¤‘..."):
            pipeline = IngestionPipeline(transformations=[TokenTextSplitter(chunk_size=256)])
            nodes = pipeline.run(documents=documents)
        
        # ê²°ê³¼ ì •ë¦¬
        result = f"ğŸ”§ **Ingestion Pipeline ë¶„ì„ ê²°ê³¼:**\n\n"
        result += f"- ì´ ë…¸ë“œ ìˆ˜: {len(nodes)}\n"
        result += f"- ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ\n\n"
        
        result += "**ë…¸ë“œ ë¯¸ë¦¬ë³´ê¸°:**\n\n"
        for i, node in enumerate(nodes[:3]):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            result += f"**ğŸ“„ ë…¸ë“œ {i+1}:**\n"
            result += f"- ID: {node.id_[:20]}...\n"
            result += f"- í…ìŠ¤íŠ¸ ê¸¸ì´: {len(node.text)} ë¬¸ì\n"
            preview = node.text[:100].replace('\n', ' ')
            result += f"- ë¯¸ë¦¬ë³´ê¸°: {preview}...\n\n"
        
        if len(nodes) > 3:
            result += f"... ì™¸ {len(nodes) - 3}ê°œì˜ ë…¸ë“œê°€ ë” ìˆìŠµë‹ˆë‹¤.\n\n"
        
        return result
        
    except Exception as e:
        return f"Pipeline ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# âœ… ë©”ì¸ í—¤ë” (ì¡°ê±´ ì—†ì´ ë¬´ì¡°ê±´ í‘œì‹œ)
st.markdown("""
<div class="main-header">
    <h1>ğŸ¤– ì—…ë¡œë“œíŒŒì¼ê¸°ë°˜ì˜ ì±—ë´‡</h1>
    <p>llama_indexì™€ Streamlitìœ¼ë¡œ ë§Œë“  ì§€ëŠ¥í˜• ë¬¸ì„œ ê´€ë¦¬ ì‹œìŠ¤í…œ</p>
</div>
""", unsafe_allow_html=True)

# âœ… ì‚¬ì´ë“œë°” (ì¡°ê±´ ì—†ì´ ë¬´ì¡°ê±´ í‘œì‹œ)
with st.sidebar:
    st.markdown("### âš™ï¸ ì„¤ì •")
    
    # OpenAI API í‚¤ ì…ë ¥ (ì„ íƒì‚¬í•­)
    st.markdown("**OpenAI API í‚¤ (AI ë¶„ì„ìš© - ì„ íƒì‚¬í•­):**")
    api_key = st.text_input("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password", 
                           value=st.session_state.openai_api_key,
                           help="AI ë²¡í„° ë¶„ì„ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•„ìš”í•©ë‹ˆë‹¤")
    if api_key:
        st.session_state.openai_api_key = api_key
        st.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")
    else:
        st.info("ğŸ’¡ API í‚¤ ì—†ì´ë„ ê¸°ë³¸ ë¶„ì„ê³¼ íŒŒì´í”„ë¼ì¸ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    st.markdown("---")
    
    # ì§€ì› íŒŒì¼ í˜•ì‹ ì•ˆë‚´
    st.markdown("### ğŸ“„ ì§€ì› íŒŒì¼ í˜•ì‹")
    with st.expander("ì§€ì›ë˜ëŠ” í˜•ì‹ ë³´ê¸°"):
        for ext, desc in SUPPORTED_EXTENSIONS.items():
            st.write(f"â€¢ `{ext}` - {desc}")
    
    st.markdown("---")
    
    # ë¶„ì„ ëª¨ë“œ ì„ íƒ
    st.markdown("### ğŸ“Š ë¶„ì„ ëª¨ë“œ")
    analysis_mode = st.selectbox(
        "ë¶„ì„ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”",
        ["ê¸°ë³¸ ë¶„ì„", "AI ë²¡í„° ë¶„ì„", "Pipeline ë¶„ì„"],
        help="ê¸°ë³¸ ë¶„ì„: íŒŒì¼ ì •ë³´ë§Œ, AI ë²¡í„° ë¶„ì„: OpenAI API í•„ìš”, Pipeline ë¶„ì„: ê³ ê¸‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬"
    )
    
    st.markdown("---")
    
    # í†µê³„ ì •ë³´
    st.markdown("### ğŸ“ˆ í†µê³„")
    files = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]
    st.metric("ì €ì¥ëœ íŒŒì¼", len(files))
    
    if files:
        total_size = sum(os.path.getsize(os.path.join(DATA_DIR, f)) for f in files)
        st.metric("ì´ ìš©ëŸ‰", f"{total_size / 1024:.1f} KB")

# âœ… ë©”ì¸ ì˜ì—­ - íƒ­ êµ¬ì„± (ì¡°ê±´ ì—†ì´ ë¬´ì¡°ê±´ í‘œì‹œ)
st.write("âœ… íƒ­ ìƒì„± í…ŒìŠ¤íŠ¸")
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ“‚ íŒŒì¼ ê´€ë¦¬", "ğŸ” ë¬¸ì„œ ë¶„ì„"])

# íŒŒì¼ ì—…ë¡œë“œ íƒ­
with tab1:
    st.markdown("### ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")
    st.write("âœ… íŒŒì¼ ì—…ë¡œë“œ íƒ­ì´ ì •ìƒ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì§€ì› í˜•ì‹ ì•ˆë‚´
    st.info("ğŸ”¹ **í˜„ì¬ ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹:** .txt, .md, .py, .js, .html, .css, .json, .csv, .pdf, .docx, .doc")
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_files = st.file_uploader(
        "íŒŒì¼ì„ ì„ íƒí•˜ê±°ë‚˜ ë“œë˜ê·¸í•´ì„œ ì—…ë¡œë“œí•˜ì„¸ìš”",
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    
    if uploaded_files:
        st.markdown("#### ğŸ“‹ ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡")
        
        for uploaded_file in uploaded_files:
            col1, col2, col3 = st.columns([3, 1, 1])
            
            with col1:
                st.write(f"ğŸ“„ **{uploaded_file.name}**")
                st.write(f"ğŸ“Š í¬ê¸°: {uploaded_file.size / 1024:.1f} KB")
            
            with col2:
                ext = get_file_extension(uploaded_file.name)
                if is_supported_file(uploaded_file.name):
                    st.success(f"âœ… {ext}")
                else:
                    st.error(f"âŒ {ext}")
            
            with col3:
                if st.button(f"ğŸ’¾ ì €ì¥", key=f"save_{uploaded_file.name}"):
                    file_path, message = save_uploaded_file(uploaded_file)
                    if file_path:
                        st.success("âœ… ì €ì¥ ì™„ë£Œ!")
                        st.balloons()
                        st.rerun()
                    else:
                        st.error(message)

# íŒŒì¼ ê´€ë¦¬ íƒ­
with tab2:
    st.markdown("### ğŸ“‚ íŒŒì¼ ê´€ë¦¬")
    st.write("âœ… íŒŒì¼ ê´€ë¦¬ íƒ­ì´ ì •ìƒ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ì €ì¥ëœ íŒŒì¼ ëª©ë¡
    files = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]
    
    if not files:
        st.info("ğŸ’¡ ì €ì¥ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 'íŒŒì¼ ì—…ë¡œë“œ' íƒ­ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì»¨íŠ¸ë¡¤ ë²„íŠ¼ë“¤
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("ğŸ—‘ï¸ ëª¨ë“  íŒŒì¼ ì‚­ì œ", type="secondary"):
                for file in files:
                    delete_file(os.path.join(DATA_DIR, file))
                st.success("ğŸ‰ ëª¨ë“  íŒŒì¼ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        
        st.markdown("#### ğŸ“‹ ì €ì¥ëœ íŒŒì¼ ëª©ë¡")
        
        # íŒŒì¼ ëª©ë¡ì„ ì¹´ë“œ í˜•íƒœë¡œ í‘œì‹œ
        for i, file in enumerate(files):
            file_path = os.path.join(DATA_DIR, file)
            file_info = get_file_info(file_path)
            
            if file_info:
                with st.container():
                    st.markdown(f"""
                    <div class="file-card">
                        <h4>ğŸ“„ {file_info['name']}</h4>
                        <p><strong>ğŸ“Š í¬ê¸°:</strong> {file_info['size'] / 1024:.1f} KB</p>
                        <p><strong>ğŸ·ï¸ í˜•ì‹:</strong> {file_info['extension']}</p>
                        <p><strong>ğŸ“… ìˆ˜ì •ì¼:</strong> {file_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    col1, col2 = st.columns([1, 4])
                    with col1:
                        if st.button(f"ğŸ—‘ï¸ ì‚­ì œ", key=f"delete_{file}", type="secondary"):
                            if delete_file(file_path):
                                st.success(f"âœ… {file} ì‚­ì œ ì™„ë£Œ!")
                                st.rerun()

# ë¬¸ì„œ ë¶„ì„ íƒ­
with tab3:
    st.markdown("### ğŸ” ë¬¸ì„œ ë¶„ì„")
    st.write("âœ… ë¬¸ì„œ ë¶„ì„ íƒ­ì´ ì •ìƒ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ë¶„ì„ ëª¨ë“œ ì •ë³´ í‘œì‹œ
    if analysis_mode == "ê¸°ë³¸ ë¶„ì„":
        st.info("ğŸ”¹ **ê¸°ë³¸ ë¶„ì„:** ì—…ë¡œë“œëœ íŒŒì¼ì˜ ê¸°ë³¸ ì •ë³´ì™€ ë‚´ìš©ì„ ë¶„ì„í•©ë‹ˆë‹¤. (API í‚¤ ë¶ˆí•„ìš”)")
    elif analysis_mode == "AI ë²¡í„° ë¶„ì„":
        st.info("ğŸ”¹ **AI ë²¡í„° ë¶„ì„:** OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
        if not st.session_state.openai_api_key:
            st.warning("âš ï¸ AI ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:  # Pipeline ë¶„ì„
        st.info("ğŸ”¹ **Pipeline ë¶„ì„:** ê³ ê¸‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤. (API í‚¤ ë¶ˆí•„ìš”)")
    
    # âœ… ë¶„ì„ ë²„íŠ¼ì€ ì¡°ê±´ ì—†ì´ ë¬´ì¡°ê±´ í‘œì‹œ
    col1, col2 = st.columns([1, 2])
    
    with col1:
        if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary"):
            
            # ë¶„ì„ ì‹¤í–‰
            if analysis_mode == "ê¸°ë³¸ ë¶„ì„":
                result = analyze_documents_basic()
            elif analysis_mode == "AI ë²¡í„° ë¶„ì„":
                result = analyze_with_vector_index()
            else:  # Pipeline ë¶„ì„
                result = analyze_with_ingestion_pipeline()
            
            # ê²°ê³¼ ì €ì¥
            st.session_state.analysis_results[analysis_mode] = result
            st.success(f"âœ… {analysis_mode} ì™„ë£Œ!")
    
    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    if analysis_mode in st.session_state.analysis_results:
        st.markdown("#### ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        # ê²°ê³¼ë¥¼ expandable ì»¨í…Œì´ë„ˆì— í‘œì‹œ
        with st.expander(f"ğŸ” {analysis_mode} ê²°ê³¼ ë³´ê¸°", expanded=True):
            st.markdown(st.session_state.analysis_results[analysis_mode])
        
        # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        result_text = st.session_state.analysis_results[analysis_mode]
        st.download_button(
            label="ğŸ“¥ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
            data=result_text,
            file_name=f"analysis_result_{analysis_mode}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain"
        )

# í‘¸í„°
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; margin-top: 2rem; padding: 1rem;">
    <p>ğŸ¤– <strong>ì—…ë¡œë“œíŒŒì¼ê¸°ë°˜ì˜ ì±—ë´‡</strong> v2.1 (PDF ì§€ì› ë²„ì „)</p>
    <p>ğŸ“š llama_index + Streamlitìœ¼ë¡œ êµ¬í˜„</p>
    <p><small>ğŸ’¡ <code>streamlit run app.py</code> ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•˜ì„¸ìš”</small></p>
</div>
""", unsafe_allow_html=True)

st.write("âœ… í˜ì´ì§€ ë§¨ ëê¹Œì§€ ì •ìƒ ë Œë”ë§ ì™„ë£Œ!")