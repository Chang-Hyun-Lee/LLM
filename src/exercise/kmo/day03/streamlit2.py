import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from openai import OpenAI
import tempfile

client = OpenAI()

st.set_page_config(page_title="ğŸ“„ PDF Q&A Chatbot", page_icon="ğŸ“„")
st.title("ğŸ“„ PDF ë¬¸ì„œ ê¸°ë°˜ Q&A")

uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    # âœ… ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name

    with st.spinner("ğŸ“– ë¬¸ì„œë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
        loader = PyPDFLoader(tmp_path)
        documents = loader.load()

        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = splitter.split_documents(documents)

        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(docs, embeddings)
        retriever = vectorstore.as_retriever()

    st.success("âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•˜ì„¸ìš”.")
    query = st.text_input("â“ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if query:
        with st.spinner("ğŸ¤– GPTê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
            context_docs = retriever.get_relevant_documents(query)
            context = "\n\n".join([doc.page_content for doc in context_docs])

            messages = [
                {"role": "system", "content": "ë„ˆëŠ” ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ëŠ” ë„ìš°ë¯¸ì•¼."},
                {"role": "user", "content": f"{context}\n\nì§ˆë¬¸: {query}"}
            ]

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages
            )

            answer = response.choices[0].message.content
            st.markdown("### ğŸ’¬ ë‹µë³€")
            st.write(answer)